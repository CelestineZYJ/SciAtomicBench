[
  {
    "id": "training_217_support",
    "table_caption": "Table: Different choices of optimal loss functions and the predictive performances over three data sets Subset1, Subset2 and FB15K, where fr(h,t)=∥h+r−t∥22, (h,r,t) is a triple in knowledge graph, and (h′,r,t′) is incorrect triple.",
    "table_content": "|| Data sets | Optimal loss function | Mean Rank Raw | Mean Rank Filter ||\n|| Subset1 | [ITALIC] fr( [ITALIC] h, [ITALIC] t)+3− [ITALIC] fr( [ITALIC] h′, [ITALIC] t′) | 339 | 240 ||\n|| Subset2 | [ITALIC] fr( [ITALIC] h, [ITALIC] t)+2− [ITALIC] fr( [ITALIC] h′, [ITALIC] t′) | 500 | 365 ||\n|| FB15K | [ITALIC] fr( [ITALIC] h, [ITALIC] t)+1− [ITALIC] fr( [ITALIC] h′, [ITALIC] t′) | 243 | 125 ||",
    "claim": "Subset1 uses an optimal margin-based loss function with a margin of 3, while Subset2 uses a margin of 2.",
    "label": "support"
  },
  {
    "id": "training_244_support",
    "table_caption": "Table: (d) Tem vs Other",
    "table_content": "|| [BOLD] Model | [BOLD] Acc | [BOLD] P | [BOLD] R | [BOLD] F1 ||\n|| [BOLD] (R & X rutherford-xue:2015:NAACL-HLT) | - | - | - | 33.30 ||\n|| [BOLD] (J & E TACL536) | 87.11 | - | - | 27.63 ||\n|| [BOLD] SVM | 66.25 | 15.10 | 68.24 | 24.73 ||\n|| [BOLD] SCNN | 76.95 | 20.22 | 62.35 | 30.54 ||\n|| [BOLD] VarNDRR | 62.14 | 17.40 | 97.65 | 29.54 ||",
    "claim": "VarNDRR achieves a high recall of 97.65 and an F1 score of 29.54.",
    "label": "support"
  },
  {
    "id": "training_223_support",
    "table_caption": "Table: Mean ρ scores for CD across the alignments. Applies only to RI, SVD and SGNS.",
    "table_content": "|| [BOLD] Dataset | [BOLD] OP | OP− | OP+ | [BOLD] WI | [BOLD] None ||\n|| [BOLD] DURel | 0.618 | 0.557 | [BOLD] 0.621 | 0.468 | 0.254 ||\n|| [BOLD] SURel | [BOLD] 0.590 | 0.514 | 0.401 | 0.492 | 0.285 ||",
    "claim": "OP+ has the best mean performance on DURel but performs poorly on SURel.",
    "label": "support"
  },
  {
    "id": "training_250_support",
    "table_caption": "Table: (c)",
    "table_content": "|| batch size | CTC CPU | CTC GPU | ASG CPU ||\n|| 1 | 40.9 | 97.9 | 16.0 ||\n|| 4 | 41.6 | 99.6 | 17.7 ||\n|| 8 | 41.7 | 100.3 | 19.2 ||",
    "claim": "CTC GPU performance increases with batch size, reaching 100.3 at batch size 8, while ASG CPU performance also increases, reaching 19.2 at batch size 8.",
    "label": "support"
  },
  {
    "id": "training_261_support",
    "table_caption": "Table: Inter-annotator agreement for all the metrics. For all the correlations presented in this table, p<0.01.",
    "table_content": "|| [BOLD] Metric | [BOLD] Spearman | [BOLD] Pearson ||\n|| Topical-Chat | Topical-Chat | Topical-Chat ||\n|| Understandable | 0.5102 | 0.5102 ||\n|| Natural | 0.4871 | 0.4864 ||\n|| Maintains Context | 0.5599 | 0.5575 ||\n|| Interesting | 0.5811 | 0.5754 ||\n|| Uses Knowledge | 0.7090 | 0.7090 ||\n|| Overall Quality | 0.7183 | 0.7096 ||\n|| PersonaChat | PersonaChat | PersonaChat ||\n|| Understandable | 0.2984 | 0.2984 ||\n|| Natural | 0.4842 | 0.4716 ||\n|| Maintains Context | 0.6125 | 0.6130 ||\n|| Interesting | 0.4318 | 0.4288 ||\n|| Uses Knowledge | 0.8115 | 0.8115 ||\n|| Overall Quality | 0.6577 | 0.6603 ||",
    "claim": "The agreement for Overall Quality is 0.7183 for Topical-Chat and 0.6577 for PersonaChat in Spearman correlation.",
    "label": "support"
  },
  {
    "id": "training_249_support",
    "table_caption": "Table: (b)",
    "table_content": "|| batch size | CTC CPU | CTC GPU | ASG CPU ||\n|| 1 | 1.9 | 5.9 | 2.5 ||\n|| 4 | 2.0 | 6.0 | 2.8 ||\n|| 8 | 2.0 | 6.1 | 2.8 ||",
    "claim": "For batch sizes 1, 4, and 8, CTC GPU has higher performance values than CTC CPU and ASG CPU.",
    "label": "support"
  },
  {
    "id": "training_228_support",
    "table_caption": "Table: Model performance in terms of RMS deviation and macro-averaged F1 score, with best results in bold font.",
    "table_content": "|| [BOLD] Model | [BOLD] RMS | [ITALIC] F1 [BOLD]  (macro) ||\n|| LSTM-CRF | 0.154 | [BOLD] 0.60 ||\n|| LSTM-SIG | [BOLD] 0.120 | 0.519 ||",
    "claim": "LSTM-CRF has a higher F1 (macro) score, while LSTM-SIG has a lower RMS score.",
    "label": "support"
  },
  {
    "id": "training_252_support",
    "table_caption": "Table: Evaluating EARL’s Relation Linking performance",
    "table_content": "|| [BOLD] System | [BOLD] Accuracy LC-QuAD | [BOLD] Accuracy - QALD ||\n|| ReMatch  | 0.12 | 0.31 ||\n|| RelMatch  | 0.15 | 0.29 ||\n|| EARL without adaptive learning | 0.32 | 0.45 ||\n|| EARL with adaptive learning | [BOLD] 0.36 | [BOLD] 0.47 ||",
    "claim": "EARL with adaptive learning achieves the highest accuracy on both LC-QuAD and QALD datasets.",
    "label": "support"
  },
  {
    "id": "training_234_support",
    "table_caption": "Table: Comparison of word error rates for CE and sequence trained unfolded RNN and DNN with score fusion and joint modeling on Hub5’00. The WERs for the joint models are after sequence training.",
    "table_content": "|| RNN/CNN combination | WER SWB | WER CH ||\n|| score fusion of CE models | 11.2 | 17.0 ||\n|| score fusion of ST models | 9.4 | 16.1 ||\n|| joint model from CE models (ST) | 9.3 | 15.6 ||\n|| joint model from ST models (ST) | 9.4 | 15.7 ||",
    "claim": "The joint model from CE models (ST) achieves a WER of 9.3 on SWB and 15.6 on CH, while the joint model from ST models (ST) achieves a WER of 9.4 on SWB and 15.7 on CH.",
    "label": "support"
  },
  {
    "id": "training_221_support",
    "table_caption": "Table: ρ for SGNS+OP+CD (L/P, win=2, k=1, t=None) before (ORG) and after time-shuffling (SHF) and downampling them to the same frequency (+DWN).",
    "table_content": "|| [BOLD] Dataset | [BOLD] ORG | [BOLD] SHF | [BOLD] +DWN ||\n|| [BOLD] DURel | [BOLD] 0.816 | 0.180 | 0.372 ||\n|| [BOLD] SURel | [BOLD] 0.767 | 0.763 | 0.576 ||",
    "claim": "The correlation of model predictions with the gold rank decreases from 0.816 to 0.180 for DURel and remains stable for SURel (0.767 vs. 0.763) after shuffling, with a further reduction to 0.576 after downsampling for SURel.",
    "label": "support"
  },
  {
    "id": "training_230_support",
    "table_caption": "Table: Comparison of word error rates for different language models.",
    "table_content": "|| LM | WER SWB | WER CH ||\n|| Baseline 4M 4-gram | 9.3 | 15.6 ||\n|| 37M 4-gram (n-gram) | 8.8 | 15.3 ||\n|| n-gram + model M | 8.4 | 14.3 ||\n|| n-gram + model M + NNLM | 8.0 | 14.1 ||",
    "claim": "The combination of n-gram, model M, and NNLM achieves the lowest WER for both SWB (8.0) and CH (14.1).",
    "label": "support"
  },
  {
    "id": "training_253_support",
    "table_caption": "Table: Empirical comparison of Connection Density and GTSP: n = number of nodes in graph; L = number of clusters in graph; N = number of nodes per cluster; top K results retrieved from ElasticSearch.",
    "table_content": "|| [BOLD] Approach | [BOLD] Accuracy (K=30) | [BOLD] Accuracy (K=10) | [BOLD] Time Complexity ||\n|| Brute Force GTSP | 0.61 | 0.62 | O( [ITALIC] n22 [ITALIC] n) ||\n|| LKH - GTSP | 0.59 | 0.58 | O( [ITALIC] nL2) ||\n|| Connection Density | 0.61 | 0.62 | O( [ITALIC] N2 [ITALIC] L2) ||",
    "claim": "Connection Density and Brute Force GTSP have the same accuracy for K=30 and K=10, but Connection Density has worse time complexity than LKH - GTSP.",
    "label": "support"
  },
  {
    "id": "training_226_support",
    "table_caption": "Table: Results of different architectures with different pre-trained knowledge on CNN/DailyMail, where Enc. and Dec. represent document encoder and decoder respectively.",
    "table_content": "|| [BOLD] Model  [BOLD] Dec. | [BOLD] Model  [BOLD] Enc. | [BOLD] R-1  [BOLD] Baseline | [BOLD] R-2  [BOLD] Baseline | [BOLD] R-L  [BOLD] Baseline | [BOLD] R-1  [BOLD] + GloVe | [BOLD] R-2  [BOLD] + GloVe | [BOLD] R-L  [BOLD] + GloVe | [BOLD] R-1  [BOLD] + BERT | [BOLD] R-2  [BOLD] + BERT | [BOLD] R-L  [BOLD] + BERT | [BOLD] R-1  [BOLD] + Newsroom | [BOLD] R-2  [BOLD] + Newsroom | [BOLD] R-L  [BOLD] + Newsroom ||\n|| SeqLab | LSTM | 41.22 | 18.72 | 37.52 | [BOLD] 41.33 | [BOLD] 18.78 | [BOLD] 37.64 | 42.18 | 19.64 | 38.53 | 41.48 | [BOLD] 18.95 | 37.78 ||\n|| SeqLab | Transformer | 41.31 | [BOLD] 18.85 | 37.63 | 40.19 | 18.67 | 37.51 | 42.28 | [BOLD] 19.73 | 38.59 | 41.32 | 18.83 | 37.63 ||\n|| Pointer | LSTM | [BOLD] 41.56 | 18.77 | [BOLD] 37.83 | 41.15 | 18.38 | 37.43 | [BOLD] 42.39 | 19.51 | [BOLD] 38.69 | 41.35 | 18.59 | 37.61 ||\n|| Pointer | Transformer | 41.36 | 18.59 | 37.67 | 41.10 | 18.38 | 37.41 | 42.09 | 19.31 | 38.41 | [BOLD] 41.54 | 18.73 | [BOLD] 37.83 ||",
    "claim": "All models improve when equipped with BERT, with the highest R-1 score of 42.39 achieved by the Pointer LSTM model.",
    "label": "support"
  },
  {
    "id": "training_235_support",
    "table_caption": "Table: Comparison of word error rates on Hub5’00 (SWB and CH) for existing systems (∗ note that the 19.1% CallHome WER is not reported in [13]).",
    "table_content": "|| System | AM training data | SWB | CH ||\n|| Vesely et al.  | SWB | 12.6 | 24.1 ||\n|| Seide et al.  | SWB+Fisher+other | 13.1 | – ||\n|| Hannun et al.  | SWB+Fisher | 12.6 | 19.3 ||\n|| Zhou et al.  | SWB | 14.2 | – ||\n|| Maas et al.  | SWB | 14.3 | 26.0 ||\n|| Maas et al.  | SWB+Fisher | 15.0 | 23.0 ||\n|| Soltau et al.  | SWB | 10.4 | 19.1∗ ||\n|| This system | SWB+Fisher+CH | 8.0 | 14.1 ||",
    "claim": "\"This system\" achieves the lowest WER on both the SWB and CH datasets.",
    "label": "support"
  },
  {
    "id": "training_247_support",
    "table_caption": "Table: SER (%), WER (%), and SA-WER (%) for baseline systems and proposed method. The number of profiles per test audio was 8. Each profile was extracted by using 2 utterances (15 sec on average). For random speaker assignment experiment (3rd row), averages of 10 trials were computed. No LM was used in the evaluation.",
    "table_content": "|| ModelEval Set | 1-speaker SER | 1-speaker WER | 1-speaker  [BOLD] SA-WER | 2-speaker-mixed SER | 2-speaker-mixed WER | 2-speaker-mixed  [BOLD] SA-WER | 3-speaker-mixed SER | 3-speaker-mixed WER | 3-speaker-mixed  [BOLD] SA-WER | Total SER | Total WER | Total  [BOLD] SA-WER ||\n|| Single-speaker ASR | - | 4.7 | - | - | 66.9 | - | - | 90.7 | - | - | 68.4 | - ||\n|| SOT-ASR | - | 4.5 | - | - | 10.3 | - | - | 19.5 | - | - | 13.9 | - ||\n|| SOT-ASR + random speaker assignment | 87.4 | 4.5 | [BOLD] 175.2 | 82.8 | 23.4 | [BOLD] 169.7 | 76.1 | 39.1 | [BOLD] 165.1 | 80.2 | 28.1 | [BOLD] 168.3 ||\n|| SOT-ASR + d-vec speaker identification | 0.4 | 4.5 | [BOLD] 4.8 | 6.4 | 10.3 | [BOLD] 16.5 | 13.1 | 19.5 | [BOLD] 31.7 | 8.7 | 13.9 | [BOLD] 22.2 ||\n|| Proposed Model | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| SOT-ASR + Spk-Enc + Inv-Attn | 0.3 | 4.3 | [BOLD] 4.7 | 5.5 | 10.4 | [BOLD] 12.2 | 14.8 | 23.4 | [BOLD] 26.7 | 9.3 | 15.9 | [BOLD] 18.2 ||\n|| ↪ + SpeakerQueryRNN | 0.4 | 4.2 | [BOLD] 4.6 | 3.0 | 9.1 | [BOLD] 10.9 | 11.6 | 21.5 | [BOLD] 24.7 | 6.9 | 14.5 | [BOLD] 16.7 ||\n|| ↪ + Weighted Profile (¯ [ITALIC] dn) | 0.2 | 4.2 | [BOLD] 4.5 | 2.5 | 8.7 | [BOLD] 9.9 | 10.2 | 20.2 | [BOLD] 23.1 | 6.0 | 13.7 | [BOLD] 15.6 ||",
    "claim": "SOT-ASR improved the WER for all evaluation settings compared to the single-speaker ASR.",
    "label": "support"
  },
  {
    "id": "training_204_support",
    "table_caption": "Table: Quantitative evaluation on the Reddit dataset. (∗ is implemented based on [5].)",
    "table_content": "|| Models | Relevance BLEU | Relevance ROUGE | Relevance Greedy | Relevance Average | Relevance Extreme | Diversity Dist-1 | Diversity Dist-2 | Diversity Ent-4 ||\n|| seq2seq | 1.85 | 0.9 | 1.845 | 0.591 | 0.342 | 0.040 | 0.153 | 6.807 ||\n|| cGAN | 1.83 | 0.9 | 1.872 | 0.604 | 0.357 | 0.052 | 0.199 | 7.864 ||\n|| AIM | [BOLD] 2.04 | [BOLD] 1.2 | [BOLD] 1.989 | [BOLD] 0.645 | 0.362 | 0.050 | 0.205 | 8.014 ||\n|| DAIM | 1.93 | 1.1 | 1.945 | 0.632 | [BOLD] 0.366 | [BOLD] 0.054 | [BOLD] 0.220 | [BOLD] 8.128 ||\n|| MMI∗ | 1.87 | 1.1 | 1.864 | 0.596 | 0.353 | 0.046 | 0.127 | 7.142 ||\n|| Human | - | - | - | - | - | 0.129 | 0.616 | 9.566 ||",
    "claim": "The diversity of generated responses is improved in cGAN compared to seq2seq.",
    "label": "support"
  },
  {
    "id": "training_302_support",
    "table_caption": "Table: Per-grid-point FLOPs of the finite-difference TTI wave-equation stencil with different spatial discretization orders.",
    "table_content": "|| spatial order | w/o optimizations | w/ optimizations ||\n|| 4 | 501 | 95 ||\n|| 8 | 539 | 102 ||\n|| 12 | 1613 | 160 ||\n|| 16 | 5489 | 276 ||",
    "claim": "The implementation with optimizations significantly reduces the metric values compared to the implementation without optimizations across all spatial orders.",
    "label": "support"
  },
  {
    "id": "training_256_support",
    "table_caption": "Table: Games played by DM with MaxQ=10, and the baseline with 5 fixed questions. Percentages of games (among all games and only decided games) where the DM models ask either fewer or more questions than the baseline. For the decided games, percentages of games where asking fewer/more questions helps (+ Change), hurts (– Change) or does not have an impact on task success w.r.t. the baseline result (No Change).",
    "table_content": "|| DM | Decided games + Change | Decided games + Change | Decided games – Change | Decided games – Change | Decided games No Change | Decided games No Change | Decided games Total | Decided games Total | All games Total | All games Total ||\n|| DM | Fewer | More | Fewer | More | Fewer | More | Fewer | More | Fewer | More ||\n|| DM1 | 1.77 | 3.46 | 2.64 | 3.79 | 22.58 | 50.35 | 26.99 | 57.6 | 22.63 | 64.43 ||\n|| DM2 | 25.01 | 0.16 | 13.98 | 0.81 | 56.18 | 3.67 | 95.17 | 4.64 | 14.83 | 85.14 ||",
    "claim": "In 95.17% of decided games, DM2 asks fewer questions than the baseline.",
    "label": "support"
  },
  {
    "id": "training_255_support",
    "table_caption": "Table: Evaluating EARL’s Entity Linking performance",
    "table_content": "|| [BOLD] System | [BOLD] Accuracy LC-QuAD | [BOLD] Accuracy - QALD ||\n|| FOX  | 0.36 | 0.30 ||\n|| DBpediaSpotlight  | 0.40 | 0.42 ||\n|| TextRazor | 0.52 | 0.53 ||\n|| Babelfy  | 0.56 | 0.56 ||\n|| EARL without adaptive learning | 0.61 | 0.55 ||\n|| EARL with adaptive learning | [BOLD] 0.65 | [BOLD] 0.57 ||",
    "claim": "EARL with adaptive learning achieves the highest accuracy on both LC-QuAD and QALD datasets.",
    "label": "support"
  },
  {
    "id": "training_263_support",
    "table_caption": "Table: Turn-level correlations between all automatic metrics and the Overall Quality ratings for the Topical-Chat corpus. All values with p>0.05 are italicized.",
    "table_content": "|| Metric | Spearman | Pearson ||\n|| Word-Overlap Metrics | Word-Overlap Metrics | Word-Overlap Metrics ||\n|| F-1 | 0.1645 | 0.1690 ||\n|| BLEU-1 | 0.2728 | 0.2876 ||\n|| BLEU-2 | 0.2862 | 0.3012 ||\n|| BLEU-3 | 0.2569 | 0.3006 ||\n|| BLEU-4 | 0.2160 | 0.2956 ||\n|| METEOR | 0.3365 | 0.3908 ||\n|| ROUGE-L | 0.2745 | 0.2870 ||\n|| Embedding Based Metrics | Embedding Based Metrics | Embedding Based Metrics ||\n|| Greedy Matching | 0.1712 | 0.1943 ||\n|| Embedding Average | 0.1803 | 0.2038 ||\n|| Vector Extrema | 0.2032 | 0.2091 ||\n|| Skip-Thought | [ITALIC] 0.1040 | [ITALIC] 0.1181 ||\n|| BERTScore (base) | 0.3229 | 0.3540 ||\n|| BERTScore (large) | 0.2982 | 0.3252 ||\n|| Reference Free Metrics | Reference Free Metrics | Reference Free Metrics ||\n|| USR - MLM | 0.3086 | 0.3345 ||\n|| USR - DR (x = c) | 0.3245 | 0.4068 ||\n|| USR - DR (x = f) | 0.1419 | 0.3221 ||\n|| USR | [BOLD] 0.4192 | [BOLD] 0.4220 ||",
    "claim": "USR shows a strong improvement over all other methods.",
    "label": "support"
  },
  {
    "id": "training_201_support",
    "table_caption": "Table: WER for adaptation of the MGB model to episodes in the longitudinal eval data.",
    "table_content": "|| [EMPTY] | eval ||\n|| [BOLD] baseline | 19.9 ||\n|| [BOLD] LHUC-LAT | 19.4 ||\n|| [BOLD] LHUC-BP | 19.5 ||\n|| [BOLD] ALL-LAT | 19.2 ||\n|| [BOLD] ALL-BP | 19.7 ||",
    "claim": "ALL-LAT achieves the lowest evaluation score of 19.2 among the methods listed.",
    "label": "support"
  },
  {
    "id": "training_271_support",
    "table_caption": "Table: Effect of including elements in the model conditioning contexts. Results are given on the YM development set.",
    "table_content": "|| Context elements | UAS | LAS ||\n|| [ITALIC] σ1. [ITALIC] t, [ITALIC] σ2. [ITALIC] t | 73.25 | 70.14 ||\n|| + [ITALIC] rc1( [ITALIC] σ1). [ITALIC] t | 80.21 | 76.64 ||\n|| + [ITALIC] lc1( [ITALIC] σ1). [ITALIC] t | 85.18 | 82.03 ||\n|| + [ITALIC] σ3. [ITALIC] t | 87.23 | 84.26 ||\n|| + [ITALIC] rc1( [ITALIC] σ2). [ITALIC] t | 87.95 | 85.04 ||\n|| + [ITALIC] σ1. [ITALIC] w | 88.53 | 86.11 ||\n|| + [ITALIC] σ2. [ITALIC] w | 88.93 | 86.57 ||",
    "claim": "Adding context elements progressively improves UAS and LAS, with the highest scores achieved when all elements are included.",
    "label": "support"
  },
  {
    "id": "training_272_support",
    "table_caption": "Table: Parsing accuracies on the YM test set. compared against previous published results. TitovH07 was retrained to enable direct comparison.",
    "table_content": "|| Model | UAS | LAS ||\n|| Eisner96 | 80.7 | - ||\n|| WallachSM08 | 85.7 | - ||\n|| TitovH07 | 89.36 | 87.65 ||\n|| [BOLD] HPYP-DP | [BOLD] 88.47 | [BOLD] 86.13 ||\n|| MaltParser | 88.88 | 87.41 ||\n|| ZhangN11 | 92.9 | 91.8 ||\n|| ChoiM13 | 92.96 | 91.93 ||",
    "claim": "The HPYP model performs better than Eisner96 and WallachSM08 in terms of UAS.",
    "label": "support"
  },
  {
    "id": "training_231_support",
    "table_caption": "Table: Word error rates of sigmoid vs. Maxout networks trained with annealed dropout (Maxout-AD) for ST CNNs, DNNs and score fusion on Hub5’00 SWB. Note that all networks are trained only on the SWB-1 data (262 hours).",
    "table_content": "|| Model | WER SWB (ST) sigmoid | WER SWB (ST) Maxout-AD ||\n|| DNN | 11.9 | 11.0 ||\n|| CNN | 11.8 | 11.6 ||\n|| DNN+CNN | 10.5 | 10.2 ||",
    "claim": "Maxout-AD models have lower WERs compared to sigmoid-based models across DNN, CNN, and DNN+CNN architectures.",
    "label": "support"
  },
  {
    "id": "training_220_support",
    "table_caption": "Table: Correlations between the ground-truth system ranking and the rankings by automatic evaluation.",
    "table_content": "|| Metrics | Spearman | p-value ||\n|| BLEU-1 | −0.36 | 0.30 ||\n|| BLEU-2 | 0.085 | 0.82 ||\n|| METEOR | 0.073 | 0.84 ||\n|| ROUGE-L | 0.35 | 0.33 ||\n|| RANDOM | 0.43 | - ||\n|| [BOLD] CHOSEN | [BOLD] 0.48 | [BOLD] 0.19 ||\n|| HUMAN | 0.87 | 0.0038 ||",
    "claim": "The CHOSEN metric has a higher correlation coefficient (0.48) with human evaluation than other automatic metrics, except for HUMAN, which has the highest correlation (0.87).",
    "label": "support"
  },
  {
    "id": "training_309_support",
    "table_caption": "Table: Evaluation results on the English-to-Japanese translation task.",
    "table_content": "|| System | BLEU | RIBES ||\n|| RNNsearch | 34.83 | 80.92 ||\n|| Eriguchi et al. (2016) | 34.91 | 81.66 ||\n|| Transformer | 36.24 | 81.90 ||\n|| +CSH | 36.83 | 82.15 ||\n|| +PSH | 36.75 | 82.09 ||\n|| +CSH+PSH | [BOLD] 37.22 | [BOLD] 82.37 ||",
    "claim": "Transformer+CSH+PSH achieves the highest BLEU and RIBES scores among the systems listed.",
    "label": "support"
  },
  {
    "id": "training_219_support",
    "table_caption": "Table: Evaluation results of triple classification. (%)",
    "table_content": "|| Data sets | WN11 | FB13 | FB15K ||\n|| SE | 53.0 | 75.2 | - ||\n|| SME(linear) | 70.0 | 63.7 | - ||\n|| SLM | 69.9 | 85.3 | - ||\n|| LFM | 73.8 | 84.3 | - ||\n|| NTN | 70.4 | 87.1 | 68.5 ||\n|| TransH(unif) | 77.7 | 76.5 | 79.0 ||\n|| TransH(bern) | 78.8 | 83.3 | 80.2 ||\n|| TransA | 93.2 | 82.8 | 87.7 ||",
    "claim": "On WN11 and FB15K, TransA outperforms other methods, while on FB13, NTN performs the best.",
    "label": "support"
  },
  {
    "id": "training_237_support",
    "table_caption": "Table: Evaluation results (ordered by decreasing precision)",
    "table_content": "|| [ITALIC] λ1 | [ITALIC] λ2 | [ITALIC] λ3 | [ITALIC] λ4 | [ITALIC] λ5 | C | # SVs | Precision ||\n|| 0 | 1 | 0.5 | 0.25 | 0.125 | 3.0 | 710 | 74.716% ||\n|| 2 | 1 | 0.5 | 0.25 | 0.125 | 3.0 | 899 | 74.705% ||\n|| 2 | 0 | 0.5 | 0.25 | 0.125 | 3.0 | 852 | 74.675% ||\n|| 0.5 | 0.25 | 0.125 | 0.0625 | 0.0312 | 3.0 | 653 | 74.67% ||\n|| 2 | 0.5 | 0.5 | 0.25 | 0.125 | 3.0 | 899 | 74.641% ||\n|| 0.25 | 0.125 | 0.0625 | 0.0312 | 0.015 | 3.0 | 615 | 74.613% ||\n|| 1 | 1 | 1 | 0.5 | 0.25 | 3.0 | 796 | 74.61% ||\n|| 0 | 1.5 | 1 | 0.5 | 0.25 | 3.0 | 792 | 74.548% ||\n|| 1.5 | 1.5 | 1 | 0.75 | 0.25 | 3.0 | 900 | 74.471% ||\n|| 2 | 1.5 | 1 | 0.5 | 0.25 | 3.0 | [BOLD] 995 | 74.36% ||\n|| 0 | 0 | 0 | 0 | 0 | 3.0 | 324 | 65.58% ||",
    "claim": "Precision drops to 65.58% when λ1 and λ2 are both zero, and the number of support vectors is highest (995) when both λ1 and λ2 take their highest values.",
    "label": "support"
  },
  {
    "id": "training_311_support",
    "table_caption": "Table: Results of various models on FewRel and TACRED (%).",
    "table_content": "|| Model | FewRel P | FewRel R | FewRel F1 | TACRED P | TACRED R | TACRED F1 ||\n|| CNN | 69.51 | 69.64 | 69.35 | 70.30 | 54.20 | 61.20 ||\n|| PA-LSTM | - | - | - | 65.70 | 64.50 | 65.10 ||\n|| C-GCN | - | - | - | 69.90 | 63.30 | 66.40 ||\n|| BERT | 85.05 | 85.11 | 84.89 | 67.23 | 64.81 | 66.00 ||\n|| ERNIE | 88.49 | 88.44 | [BOLD] 88.32 | 69.97 | 66.08 | [BOLD] 67.97 ||",
    "claim": "ERNIE achieves an F1 score increase of 3.4% over BERT on FewRel and increases the F1 of BERT by nearly 2.0% on TACRED.",
    "label": "support"
  },
  {
    "id": "training_214_support",
    "table_caption": "Table: The same BiLSTM-CRF approach was evaluated twice under Evaluation 1. The threshold column depicts the average difference in percentage points F1-score for statistical significance with 0.04",
    "table_content": "|| [BOLD] Task | [BOLD] Threshold  [ITALIC] τ | [BOLD] % significant | Δ( [ITALIC] test)95 | Δ( [ITALIC] test) [ITALIC] Max ||\n|| ACE 2005 - Entities | 0.65 | 28.96% | 1.21 | 2.53 ||\n|| ACE 2005 - Events | 1.97 | 34.48% | 4.32 | 9.04 ||\n|| CoNLL 2000 - Chunking | 0.20 | 18.36% | 0.30 | 0.56 ||\n|| CoNLL 2003 - NER-En | 0.42 | 31.02% | 0.83 | 1.69 ||\n|| CoNLL 2003 - NER-De | 0.78 | 33.20% | 1.61 | 3.36 ||\n|| GermEval 2014 - NER-De | 0.60 | 26.80% | 1.12 | 2.38 ||\n|| TempEval 3 - Events | 1.19 | 10.72% | 1.48 | 2.99 ||",
    "claim": "For the ACE 2005 - Events task, 34.48% of the cases show a significant difference, while for other tasks, the percentage of significant cases ranges from 10.72% to 33.20%.",
    "label": "support"
  },
  {
    "id": "training_210_support",
    "table_caption": "Table: Comparison of normalizing query and key in N-SAN.",
    "table_content": "|| Query | Key | B@4 | M | R | C | S ||\n|| ✗ | ✗ | 38.4 | 28.6 | 58.4 | 128.6 | 22.6 ||\n|| ✓ | ✗ | 39.3 | [BOLD] 29.1 | [BOLD] 58.9 | [BOLD] 130.8 | 23.0 ||\n|| ✗ | ✓ | 39.2 | 29.0 | 58.8 | 130.1 | 22.8 ||\n|| ✓ | ✓ | [BOLD] 39.4 | [BOLD] 29.1 | 58.8 | 130.7 | [BOLD] 23.1 ||",
    "claim": "Normalizing either Q or K increases performance, with normalizing both Q and K and normalizing Q alone yielding similar results. Normalizing K alone is inferior to normalizing Q alone.",
    "label": "support"
  },
  {
    "id": "training_297_support",
    "table_caption": "Table: Human-evaluation results for how reasonable hypotheses are (CSQA development set). Each rater determined whether a hypothesis is reasonable (1 point), somewhat reasonable (0.5 point) or not reasonable (0 points). The score is the average rating across raters and examples.",
    "table_content": "|| Model | Score ||\n|| | [ITALIC] c|=3+KLD+REP | 0.72 ||\n|| Top- [ITALIC] K=5 ST | [BOLD] 0.74 ||\n|| SupGen | [ITALIC] c|=3 | 0.60 ||\n|| SupGen | [ITALIC] c|=30 | 0.55 ||",
    "claim": "Top-K=5 ST achieved the highest score of 0.74.",
    "label": "support"
  },
  {
    "id": "training_216_support",
    "table_caption": "Table: 95% percentile of Δ(test) after averaging.",
    "table_content": "|| [BOLD] Task | Δ( [ITALIC] test)95 [BOLD]  for  [ITALIC] n scores  [BOLD] 1 | Δ( [ITALIC] test)95 [BOLD]  for  [ITALIC] n scores  [BOLD] 3 | Δ( [ITALIC] test)95 [BOLD]  for  [ITALIC] n scores  [BOLD] 5 | Δ( [ITALIC] test)95 [BOLD]  for  [ITALIC] n scores  [BOLD] 10 | Δ( [ITALIC] test)95 [BOLD]  for  [ITALIC] n scores  [BOLD] 20 ||\n|| ACE-Ent. | 1.21 | 0.72 | 0.51 | 0.38 | 0.26 ||\n|| ACE-Ev. | 4.32 | 2.41 | 1.93 | 1.39 | 0.97 ||\n|| Chk. | 0.30 | 0.16 | 0.14 | 0.09 | 0.06 ||\n|| NER-En | 0.83 | 0.45 | 0.35 | 0.26 | 0.18 ||\n|| NER-De | 1.61 | 0.94 | 0.72 | 0.51 | 0.37 ||\n|| GE 14 | 1.12 | 0.64 | 0.48 | 0.34 | 0.25 ||\n|| TE 3 | 1.48 | 0.81 | 0.63 | 0.48 | 0.32 ||",
    "claim": "For increasing n, the value Δ(test)95 decreases, indicating more stable mean scores. For n=10, the Δ(test)95 is 0.26 for the NER-En task and 1.39 for the ACE-Ev. task.",
    "label": "support"
  },
  {
    "id": "training_329_support",
    "table_caption": "Table: BLEU assessable sentences",
    "table_content": "|| [EMPTY] | People | Mathematics | Food & drink ||\n|| [BOLD] #sentences | 15 | 24 | 23 ||\n|| [BOLD] #sentences recognized | 15 | 22 | 23 ||\n|| [BOLD] BLEU assessable | 10 | 15 | 11 ||",
    "claim": "The system failed to recognize the structure of 2 sentences in the Mathematics category.",
    "label": "support"
  },
  {
    "id": "training_323_support",
    "table_caption": "Table: Ablation studies on PTB dev set (wsj 22). Forward and backward context, and part-of-speech input were all critical to strong performace.",
    "table_content": "|| Parser | UAS | LAS ||\n|| Bi-LSTM Hierarchical† | 93.31 | 91.01 ||\n|| † - Hierarchical Actions | 92.94 | 90.96 ||\n|| † - Backward-LSTM | 91.12 | 88.72 ||\n|| † - Forward-LSTM | 91.85 | 88.39 ||\n|| † - tag embeddings | 92.46 | 89.81 ||",
    "claim": "Bi-LSTM Hierarchical achieves the highest UAS and LAS scores among the configurations listed.",
    "label": "support"
  },
  {
    "id": "training_212_support",
    "table_caption": "Table: Comparison of various variants of GSA.",
    "table_content": "|| Approach | #params | B@4 | M | R | C | S ||\n|| SAN | 40.2M | 38.4 | 28.6 | 58.4 | 128.6 | 22.6 ||\n|| absolute | 40.2M | 38.3 | 28.5 | 58.4 | 128.4 | 22.6 ||\n|| content-independent | 40.2M | 39.2 | 29.1 | 58.9 | 131.0 | 22.9 ||\n|| key-dependent | 41.5M | 38.9 | 29.0 | 58.8 | 129.5 | 22.8 ||\n|| query-dependent | 41.5M | [BOLD] 39.3 | [BOLD] 29.2 | [BOLD] 59.0 | [BOLD] 131.4 | [BOLD] 23.0 ||",
    "claim": "The \"query-dependent\" approach achieves the highest performance across all metrics compared to other approaches.",
    "label": "support"
  },
  {
    "id": "training_241_support",
    "table_caption": "Table: (b) Con vs Other",
    "table_content": "|| [BOLD] Model | [BOLD] Acc | [BOLD] P | [BOLD] R | [BOLD] F1 ||\n|| [BOLD] (R & X rutherford-xue:2015:NAACL-HLT) | - | - | - | 53.80 ||\n|| [BOLD] (J & E TACL536) | 76.95 | - | - | 52.78 ||\n|| [BOLD] SVM | 62.62 | 39.14 | 72.40 | 50.82 ||\n|| [BOLD] SCNN | 63.00 | 39.80 | 75.29 | 52.04 ||\n|| [BOLD] VarNDRR | 53.82 | 35.39 | 88.53 | 50.56 ||",
    "claim": "VarNDRR achieves the highest recall among the models, contributing to its F1 score improvements, but does not show substantial improvements in accuracy compared to the baselines.",
    "label": "support"
  },
  {
    "id": "training_222_support",
    "table_caption": "Table: Best and mean ρ scores across similarity measures (CD, LND, JSD) on semantic representations.",
    "table_content": "|| [BOLD] Dataset | [BOLD] Representation | [BOLD] best | [BOLD] mean ||\n|| [BOLD] DURel | raw count | 0.639 | 0.395 ||\n|| [BOLD] DURel | PPMI | 0.670 | 0.489 ||\n|| [BOLD] DURel | SVD | 0.728 | 0.498 ||\n|| [BOLD] DURel | RI | 0.601 | 0.374 ||\n|| [BOLD] DURel | SGNS | [BOLD] 0.866 | [BOLD] 0.502 ||\n|| [BOLD] DURel | SCAN | 0.327 | 0.156 ||\n|| [BOLD] SURel | raw count | 0.599 | 0.120 ||\n|| [BOLD] SURel | PPMI | 0.791 | 0.500 ||\n|| [BOLD] SURel | SVD | 0.639 | 0.300 ||\n|| [BOLD] SURel | RI | 0.622 | 0.299 ||\n|| [BOLD] SURel | SGNS | [BOLD] 0.851 | [BOLD] 0.520 ||\n|| [BOLD] SURel | SCAN | 0.082 | -0.244 ||",
    "claim": "SGNS has the highest 'best' performance scores for both DURel and SURel datasets, while PPMI and SVD show strong results among count models.",
    "label": "support"
  },
  {
    "id": "training_315_support",
    "table_caption": "Table: Ablation study on FewRel (%).",
    "table_content": "|| Model | P | R | F1 ||\n|| BERT | 85.05 | 85.11 | 84.89 ||\n|| ERNIE | 88.49 | 88.44 | [BOLD] 88.32 ||\n|| w/o entities | 85.89 | 85.89 | 85.79 ||\n|| w/o dEA | 85.85 | 85.75 | 85.62 ||",
    "claim": "ERNIE achieves the highest F1 score of 88.32 among the models.",
    "label": "support"
  },
  {
    "id": "training_290_support",
    "table_caption": "Table: Ablative study layers and heads of Transformers.",
    "table_content": "|| SPAT | Acc | VAcc | Cons | SAcc ||\n|| ImgGrnd | 17.03 | 9.71 | 50.41 | 7.14 ||\n|| +OTx (1L, 3H) | 19.8 | 10.91 | 48.34 | 8.45 ||\n|| +OTx (2L, 3H) | 20.8 | 11.38 | 49.45 | 9.17 ||\n|| +OTx (2L, 6H) | [BOLD] 21.16 | [BOLD] 12.2 | 48.86 | [BOLD] 9.58 ||\n|| +OTx (3L, 3H) | 20.68 | 11.34 | 48.66 | 9.19 ||\n|| +OTx (3L, 6H) | 21.14 | 12.1 | [BOLD] 49.66 | 9.52 ||\n|| VOGNet | 23.53 | 14.22 | 56.5 | 11.58 ||\n|| +MTx (2L,3H) | 23.38 | 14.78 | 55.5 | 11.9 ||\n|| +MTx (2L,6H) | 23.96 | 14.44 | 55.5 | 11.59 ||\n|| +MTx (3L,3H) | 24.53 | 14.84 | 56.19 | 12.37 ||\n|| +MTx (3L,6H) | 24.24 | 15.36 | 57.37 | 12.52 ||\n|| +OTx(3L,6H) | [BOLD] 24.99 | [BOLD] 17.33 | [BOLD] 66.29 | [BOLD] 14.47 ||",
    "claim": "A multi-modal transformer with 3 layers and 6 heads outperforms an object transformer with 3 layers and 6 heads across all metrics.",
    "label": "support"
  },
  {
    "id": "training_213_support",
    "table_caption": "Table: Video captioning results on VATEX dataset.",
    "table_content": "|| Model | B@4 | M | R | C ||\n|| VATEX  | 28.2 | 21.7 | 46.9 | 45.7 ||\n|| Transformer (Ours) | 30.6 | 22.3 | 48.4 | 53.4 ||\n|| +NSA | [BOLD] 31.0 | [BOLD] 22.7 | [BOLD] 49.0 | [BOLD] 57.1 ||",
    "claim": "Transformer outperforms VATEX on all metrics, and Transformer+NSA further improves upon Transformer on all metrics, including a 3.7-point increase in the CIDEr score compared to Transformer and an 11.4-point increase compared to VATEX.",
    "label": "support"
  },
  {
    "id": "training_119_support",
    "table_caption": "Table: Ablation study on the effectiveness of the hierarchical graph on the dev set in the Distractor setting. RoBERTa-large is used for context encoding.",
    "table_content": "|| Model | Ans F1 | Sup F1 | Joint F1 ||\n|| w/o Graph | 80.58 | 85.83 | 71.02 ||\n|| PS Graph | 81.68 | 88.44 | 73.83 ||\n|| PSE Graph | 82.10 | 88.40 | 74.13 ||\n|| Hier. Graph | [BOLD] 82.22 | [BOLD] 88.58 | [BOLD] 74.37 ||",
    "claim": "The Hier. Graph model achieves the highest Ans F1, Sup F1, and Joint F1 scores among the models listed.",
    "label": "support"
  },
  {
    "id": "training_218_support",
    "table_caption": "Table: Evaluation results on link prediction.",
    "table_content": "|| Data sets Metric | WN18 Mean Rank | WN18 Mean Rank | FB15K Mean Rank | FB15K Mean Rank ||\n|| Metric | Raw | Filter | Raw | Filter ||\n|| Unstructured | 315 | 304 | 1,074 | 979 ||\n|| RESCAL | 1,180 | 1,163 | 828 | 683 ||\n|| SE | 1,011 | 985 | 273 | 162 ||\n|| SME(linear) | 545 | 533 | 274 | 154 ||\n|| SME(bilinear) | 526 | 509 | 284 | 158 ||\n|| LFM | 469 | 456 | 283 | 164 ||\n|| TransE | 263 | 251 | 243 | 125 ||\n|| TransH(bern) | 401 | 388 | 212 | 87 ||\n|| TransH(unif) | 318 | 303 | 211 | 84 ||\n|| TransA | 165 | 153 | 164 | 58 ||",
    "claim": "TransA obtains the lowest mean rank on both WN18 and FB15K datasets.",
    "label": "support"
  },
  {
    "id": "training_246_support",
    "table_caption": "Table: The comparison of different models based on over 0.999 on-topic recall on seen and unseen benchmarks. AOR means Average Off-topic Recall (%) and PRR3 means Prompt Ratio over off-topic Recall 0.3 (%).",
    "table_content": "|| Systems | Model | Seen PPR3 | Seen AOR | Unseen PPR3 | Unseen AOR ||\n|| Malinin et al., 2017 | Att-RNN | 84.6 | 72.2 | 32.0 | 21.0 ||\n|| Our baseline model | G-Att-RNN | 87.8 | 76.8 | 54.0 | 38.1 ||\n|| This work | + Bi-Attention | 90.4 | 78.3 | 56.0 | 39.7 ||\n|| This work | + RNN→CNN | 89.7 | 76.6 | 66.0 | 43.7 ||\n|| This work | +  [ITALIC] maxpooling | 92.3 | 79.1 | 68.0 | 42.2 ||\n|| This work | + Res-conn in gated unit (GCBiA) | [BOLD] 93.6 | [BOLD] 79.2 | [BOLD] 68.0 | [BOLD] 45.0 ||",
    "claim": "The model with Res-conn in gated unit (GCBiA) achieves the highest performance across all metrics, with 93.6 on Seen PPR3, 79.2 on Seen AOR, 68.0 on Unseen PPR3, and 45.0 on Unseen AOR.",
    "label": "support"
  },
  {
    "id": "training_324_support",
    "table_caption": "Table: Test F-scores for constituency parsing on Penn Treebank and CTB-5.",
    "table_content": "|| Parser | [ITALIC] b | English greedy | English beam | Chinese greedy | Chinese beam ||\n|| zhu+:2013 | 16 | 86.08 | 90.4 | 75.99 | 85.6 ||\n|| Mi & Huang (05) | 32 | 84.95 | 90.8 | 75.61 | 83.9 ||\n|| Vinyals et al. (05) | 10 | - | 90.5 | - | - ||\n|| Bi-LSTM | - | 89.75 | - | 79.44 | - ||\n|| 2-Layer Bi-LSTM | - | [BOLD] 89.95 | - | [BOLD] 80.13 | - ||",
    "claim": "The work achieves the highest accuracy among greedy parsers for both English and Chinese.",
    "label": "support"
  },
  {
    "id": "training_279_support",
    "table_caption": "Table: The Effect of Hierarchical Feature Generation",
    "table_content": "|| Source | Target | LIFG – w/o  [ITALIC] CMeta | LIFG – w/  [ITALIC] CMeta ||\n|| E | F | 52.63 | 62.03 ||\n|| E | G | 55.19 | 63.34 ||\n|| F | E | 50.87 | 60.49 ||\n|| F | G | 49.32 | 60.88 ||\n|| G | E | 51.06 | 59.61 ||\n|| G | F | 50.01 | 61.84 ||",
    "claim": "LIFG with CMeta shows higher performance than LIFG without CMeta across all source-target pairs.",
    "label": "support"
  },
  {
    "id": "training_344_support",
    "table_caption": "Table: Results with Different Pooling Types",
    "table_content": "|| Method | WER ||\n|| Max Pooling | 18.9 ||\n|| Stochastic Pooling | 18.8 ||\n|| [ITALIC] lp pooing | 18.9 ||",
    "claim": "Stochastic pooling has a slightly lower WER than max pooling and lp pooling.",
    "label": "support"
  },
  {
    "id": "training_254_support",
    "table_caption": "Table: Evaluation of joint linking performance",
    "table_content": "|| [BOLD] Value of k | R [ITALIC] f [BOLD]  based on R [ITALIC] i | R [ITALIC] f [BOLD]  based on C,H | R [ITALIC] f [BOLD]  based on R [ITALIC] i,C,H ||\n|| [ITALIC] k = 10 | 0.543 | 0.689 | 0.708 ||\n|| [ITALIC] k = 30 | 0.544 | 0.666 | 0.735 ||\n|| [ITALIC] k = 50 | 0.543 | 0.617 | [BOLD] 0.737 ||\n|| [ITALIC] k = 100 | 0.540 | 0.534 | 0.733 ||\n|| [ITALIC] k∗ = 10 | 0.568 | 0.864 | [BOLD] 0.905 ||\n|| [ITALIC] k∗ = 30 | 0.554 | 0.779 | 0.864 ||\n|| [ITALIC] k∗ = 50 | 0.549 | 0.708 | 0.852 ||\n|| [ITALIC] k∗ = 50 | 0.545 | 0.603 | 0.817 ||",
    "claim": "The MRR increases from 0.568 to 0.905 when correct URIs are artificially inserted into the candidate list for k∗ = 10.",
    "label": "support"
  },
  {
    "id": "training_334_support",
    "table_caption": "Table: Error analysis: Core word error types for CA",
    "table_content": "|| Error | Freq. | % | Explanation | Examples ||\n|| Invalid diacritized form | 195 | 38.8 | invalid form | “>aqosaAm” (أِقْسَام¿ – portions) vs. “>aqasaAm” (أَقَسَام¿) ||\n|| Wrong selection | 157 | 31.4 | Homographs with different diacritized forms | “raAfoE” (رَقْع¿ – lifting) vs. “rafaE” (رَفَع¿ – he lifted) ||\n|| Affix diacritization error | 66 | 13.2 | Some affixes are erroneously diacritized | “baladhu” (بَلَدهُ¿ – his country, where country is subject of verb) vs. “baladhi” (بَلَدهِ¿ – his country, where country is subject or object of preposition) ||\n|| Named entities | 44 | 8.8 | Named entities | “Alr~ayob” (الرَّيْب¿ – Arrayb) vs. “Alr~iyab” (الرِّيَب¿)) ||\n|| Problems with reference | 22 | 4.4 | Some words in the reference were partially diacritized | “nuEoTaY” (نُعْطَى¿ – we are given) vs. “nETY” (نعطى¿)) ||\n|| Guess has no diacritics | 9 | 1.8 | system did not produce any diacritics | “mhnd” (مهند¿ – sword) vs. “muhan~ad” (مُهَنَّد¿)) ||\n|| Different valid forms | 7 | 1.4 | Some words have multiple valid diacritized forms | “maA}op” (مَائْة¿ – hundred) and “miA}op” (مِائَة¿) ||\n|| Misspelled word | 1 | 0.2 | [EMPTY] | “lbAlmsjd” (لبالمسجد¿) vs. “lbAlmsjd” (بالمسجد¿ – in the mosque)) ||",
    "claim": "The two most common errors are invalid diacritized forms (38.8%) and wrong selection (31.4%).",
    "label": "support"
  },
  {
    "id": "training_232_support",
    "table_caption": "Table: Comparison of word error rates for CE-trained DNNs with different number of outputs and phonetic context size on Hub5’00 SWB.",
    "table_content": "|| Nb. outputs | Phonetic ctx. | WER SWB (CE) ||\n|| 16000 | ±2 | 12.0 ||\n|| 16000 | ±3 | 11.8 ||\n|| 32000 | ±2 | 11.7 ||\n|| 64000 | ±2 | 11.9 ||",
    "claim": "The lowest WER of 11.7 is achieved with 32000 outputs and a phonetic context of ±2.",
    "label": "support"
  },
  {
    "id": "training_209_support",
    "table_caption": "Table: Ablation study on dev sets of the corresponding datasets.",
    "table_content": "|| [EMPTY] | [BOLD] SNLI | [BOLD] Quora | [BOLD] Scitail | [BOLD] WikiQA ||\n|| original | 88.9 | 89.4 | 88.9 | 0.7740 ||\n|| w/o enc-in | 87.2 | 85.7 | 78.1 | 0.7146 ||\n|| residual conn. | 88.9 | 89.2 | 87.4 | 0.7640 ||\n|| simple fusion | 88.8 | 88.3 | 87.5 | 0.7345 ||\n|| alignment alt. | 88.7 | 89.3 | 88.2 | 0.7702 ||\n|| prediction alt. | 88.9 | 89.2 | 88.8 | 0.7558 ||\n|| parallel blocks | 88.8 | 88.6 | 87.6 | 0.7607 ||",
    "claim": "Without richer features as the alignment input, performance degrades on all datasets. The simpler implementation of the fusion layer leads to worse performance.",
    "label": "support"
  },
  {
    "id": "training_284_support",
    "table_caption": "Table: bAbI QA dataset [Weston et al., 2016] error rates (%) of QRN and previous work: LSTM [Weston et al., 2016], End-to-end Memory Networks (N2N) [Sukhbaatar et al., 2015], Dynamic Memory Networks (DMN+) [Xiong et al., 2016], Gated End-to-end Memory Networks(GMemN2N) [Perez and Liu, 2016]. Results within each task of Differentiable Neural Computer(DNC) were not provided in its paper Graves et al. [2016]). For QRN, a number in the front (1, 2, 3, 6) indicates the number of layers. A number in the back (200) indicates the dimension of hidden vector, while the default value is 50. ‘r’ indicates that the reset gate is used, and ‘v’ indicates that the gates were vectorized. ‘*’ indicates joint training.",
    "table_content": "|| Task | 1k Previous works | 1k Previous works | 1k Previous works | 1k Previous works | 1k QRN | 1k QRN | 1k QRN | 1k QRN | 1k QRN | 1k QRN | 10k Previous works | 10k Previous works | 10k Previous works | 10k QRN | 10k QRN | 10k QRN | 10k QRN ||\n|| Task | LSTM | N2N | DMN+ | GMemN2N | 1r | 2 | 2r | 3r | 6r | 6r200* | N2N | DMN+ | GMemN2N | 2r | 2rv | 3r | 6r200 ||\n|| 1: Single supporting fact | 50.0 | 0.1 | 1.3 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 13.1 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 ||\n|| 2: Two supporting facts | 80.0 | 18.8 | 72.3 | 8.1 | 65.7 | 1.2 | 0.7 | 0.5 | 1.5 | 15.3 | 0.3 | 0.3 | 0.0 | 0.4 | 0.8 | 0.4 | 0.0 ||\n|| 3: Three supporting facts | 80.0 | 31.7 | 73.3 | 38.7 | 68.2 | 17.5 | 5.7 | 1.2 | 15.3 | 13.8 | 2.1 | 1.1 | 4.5 | 0.4 | 1.4 | 0.0 | 0.0 ||\n|| 4: Two arg relations | 39.0 | 17.5 | 26.9 | 0.4 | 0.0 | 0.0 | 0.0 | 0.7 | 9.0 | 13.6 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 ||\n|| 5: Three arg relations | 30.0 | 12.9 | 25.6 | 1.0 | 1.0 | 1.1 | 1.1 | 1.2 | 1.3 | 12.5 | 0.8 | 0.5 | 0.2 | 0.5 | 0.2 | 0.3 | 0.0 ||\n|| 6: Yes/no questions | 52.0 | 2.0 | 28.5 | 8.4 | 0.1 | 0.0 | 0.9 | 1.2 | 50.6 | 15.5 | 0.1 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 ||\n|| 7: Counting | 51.0 | 10.1 | 21.9 | 17.8 | 10.9 | 11.1 | 9.6 | 9.4 | 13.1 | 15.3 | 2.0 | 2.4 | 1.8 | 1.0 | 0.7 | 0.7 | 0.0 ||\n|| 8: Lists/sets | 55.0 | 6.1 | 21.9 | 12.5 | 6.8 | 5.7 | 5.6 | 3.7 | 7.8 | 15.1 | 0.9 | 0.0 | 0.3 | 1.4 | 0.6 | 0.8 | 0.4 ||\n|| 9 : Simple negation | 36.0 | 1.5 | 42.9 | 10.7 | 0.0 | 0.6 | 0.0 | 0.0 | 32.7 | 13.0 | 0.3 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 ||\n|| 10: Indefinite knowledge | 56.0 | 2.6 | 23.1 | 16.5 | 0.8 | 0.6 | 0.0 | 0.0 | 3.5 | 12.9 | 0.0 | 0.0 | 0.2 | 0.0 | 0.0 | 0.0 | 0.0 ||\n|| 11: Basic coreference | 38.0 | 3.3 | 4.3 | 0.0 | 11.3 | 0.5 | 0.0 | 0.0 | 0.9 | 14.7 | 0.1 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 ||\n|| 12: Conjunction | 26.0 | 0.0 | 3.5 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 15.1 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 ||\n|| 13: Compound coreference | 6.0 | 0.5 | 7.8 | 0.0 | 5.3 | 5.5 | 0.0 | 0.3 | 8.9 | 13.7 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 ||\n|| 14: Time reasoning | 73.0 | 2.0 | 61.9 | 1.2 | 20.2 | 1.3 | 0.8 | 3.8 | 18.2 | 14.5 | 0.1 | 0.0 | 0.0 | 0.2 | 0.0 | 0.0 | 0.1 ||\n|| 15: Basic deduction | 79.0 | 1.8 | 47.6 | 0.0 | 39.4 | 0.0 | 0.0 | 0.0 | 0.1 | 14.7 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 ||\n|| 16: Basic induction | 77.0 | 51.0 | 54.4 | 0.1 | 50.6 | 54.8 | 53.0 | 53.4 | 53.5 | 15.5 | 51.8 | 45.3 | 0.0 | 49.4 | 50.4 | 49.1 | 0.0 ||\n|| 17: Positional reasoning | 49.0 | 42.6 | 44.1 | 41.7 | 40.6 | 36.5 | 34.4 | 51.8 | 52.0 | 13.0 | 18.6 | 4.2 | 27.8 | 0.9 | 0.0 | 5.8 | 4.1 ||\n|| 18: Size reasoning | 48.0 | 9.2 | 9.1 | 9.2 | 8.2 | 8.6 | 7.9 | 8.8 | 47.5 | 14.9 | 5.3 | 2.1 | 8.5 | 1.6 | 8.4 | 1.8 | 0.7 ||\n|| 19: Path finding | 92.0 | 90.6 | 90.8 | 88.5 | 88.8 | 89.8 | 78.7 | 90.7 | 88.6 | 13.6 | 2.3 | 0.0 | 31.0 | 36.1 | 1.0 | 27.9 | 0.1 ||\n|| 20: Agents motivations | 9.0 | 0.2 | 2.2 | 0.0 | 0.0 | 0.0 | 0.2 | 0.3 | 5.5 | 14.6 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 ||\n|| # Failed | 20 | 10 | 16 | 10 | 12 | 8 | 7 | [BOLD] 5 | 13 | 20 | 3 | 1 | 3 | 2 | 2 | 3 | [BOLD] 0 ||\n|| Average error rates (%) | 51.3 | 15.2 | 33.2 | 12.7 | 20.1 | 11.7 | [BOLD] 9.9 | 11.3 | 20.5 | 14.2 | 4.2 | 2.8 | 3.7 | 4.6 | 3.2 | 4.3 | [BOLD] 0.3 ||",
    "claim": "In the 1k dataset, QRN's '2r' model has the lowest average error rate of 9.9%, outperforming all other models. In the 10k dataset, QRN's '6r200' model achieves the lowest average error rate of 0.3%, outperforming all previous models.",
    "label": "support"
  },
  {
    "id": "training_343_support",
    "table_caption": "Table: WER as a function of # of hidden units",
    "table_content": "|| Number of Hidden Units | WER ||\n|| 64 | 24.1 ||\n|| 128 | 23.0 ||\n|| 220 | 22.1 ||\n|| 128/256 | 21.9 ||",
    "claim": "As the number of hidden units increases up to 220, the WER decreases, with the lowest WER achieved using 128 hidden units for the first layer and 256 for the second layer.",
    "label": "support"
  },
  {
    "id": "training_285_support",
    "table_caption": "Table: Evaluation of VOGNet in GT5 setting by training (first column) and testing (top row) on SVSQ, TEMP, SPAT respectively",
    "table_content": "|| [EMPTY] | SVSQ Acc | SVSQ SAcc | TEMP Acc | TEMP SAcc | SPAT Acc | SPAT SAcc ||\n|| SVSQ | 76.38 | 59.58 | 1.7 | 0.42 | 2.27 | 0.6 ||\n|| TEMP | 75.4 | 57.38 | 23.07 | 12.06 | 18.03 | 8.16 ||\n|| SPAT | 75.15 | 57.02 | 22.6 | 11.04 | 23.53 | 11.58 ||",
    "claim": "Models trained on SVSQ have an accuracy of less than 3% when tested on TEMP and SPAT.",
    "label": "support"
  },
  {
    "id": "training_117_support",
    "table_caption": "Table: Results on the test set of HotpotQA in the Distractor setting. HGN achieves state-of-the-art results at the time of submission (Dec. 1, 2019). (†) indicates unpublished work. RoBERTa-large is used for context encoding.",
    "table_content": "|| Model | Ans EM | Ans F1 | Sup EM | Sup F1 | Joint EM | Joint F1 ||\n|| DecompRC Min et al. ( 2019b ) | 55.20 | 69.63 | - | - | - | - ||\n|| ChainEx Chen et al. ( 2019 ) | 61.20 | 74.11 | - | - | - | - ||\n|| Baseline Model Yang et al. ( 2018 ) | 45.60 | 59.02 | 20.32 | 64.49 | 10.83 | 40.16 ||\n|| QFE Nishida et al. ( 2019 ) | 53.86 | 68.06 | 57.75 | 84.49 | 34.63 | 59.61 ||\n|| DFGN Xiao et al. ( 2019 ) | 56.31 | 69.69 | 51.50 | 81.62 | 33.62 | 59.82 ||\n|| LQR-Net Grail et al. ( 2020 ) | 60.20 | 73.78 | 56.21 | 84.09 | 36.56 | 63.68 ||\n|| P-BERT† | 61.18 | 74.16 | 51.38 | 82.76 | 35.42 | 63.79 ||\n|| TAP2† | 64.99 | 78.59 | 55.47 | 85.57 | 39.77 | 69.12 ||\n|| EPS+BERT† | 65.79 | 79.05 | 58.50 | 86.26 | 42.47 | 70.48 ||\n|| SAE-large Tu et al. ( 2020 ) | 66.92 | 79.62 | 61.53 | 86.86 | 45.36 | 71.45 ||\n|| C2F ReaderShao et al. ( 2020 ) | 67.98 | 81.24 | 60.81 | 87.63 | 44.67 | 72.73 ||\n|| HGN (ours) | [BOLD] 69.22 | [BOLD] 82.19 | [BOLD] 62.76 | [BOLD] 88.47 | [BOLD] 47.11 | [BOLD] 74.21 ||",
    "claim": "HGN achieves the highest scores on Ans EM, Ans F1, Sup EM, Sup F1, Joint EM, and Joint F1 metrics compared to other models.",
    "label": "support"
  },
  {
    "id": "training_319_support",
    "table_caption": "Table: Average precision scores on the same-different task (dev sets), showing the effects of applying vtln to the input features for the utd and/or cae systems. cae input is either MFCC or MFCC+VTLN. Topline results (rows 5-6) train cAE on gold standard pairs, rather than UTD output. Baseline results (final rows) directly evaluate acoustic features without UTD/cAE training. Best unsupervised result in bold.",
    "table_content": "|| [BOLD] UTD input | [BOLD] cAE input | [BOLD] ES | [BOLD] HA | [BOLD] HR | [BOLD] SV | [BOLD] TR | [BOLD] ZH ||\n|| PLP | [EMPTY] | 28.6 | 39.9 | 26.9 | 22.2 | 25.2 | 20.4 ||\n|| PLP | +VTLN | 46.2 | 48.2 | 36.3 | 37.9 | 31.4 | 35.7 ||\n|| PLP+VTLN | [EMPTY] | 40.4 | 45.7 | 35.8 | 25.8 | 25.9 | 26.9 ||\n|| PLP+VTLN | +VTLN | [BOLD] 51.5 | [BOLD] 52.9 | [BOLD] 39.6 | [BOLD] 42.9 | [BOLD] 33.4 | [BOLD] 44.4 ||\n|| [ITALIC] Gold pairs | [EMPTY] | 65.3 | 65.2 | 55.6 | 52.9 | 50.6 | 60.5 ||\n|| [ITALIC] Gold pairs | +VTLN | 68.9 | 70.1 | 57.8 | 56.9 | 56.3 | 69.5 ||\n|| [ITALIC] Baseline: MFCC | [ITALIC] Baseline: MFCC | 18.3 | 19.6 | 17.6 | 12.3 | 16.8 | 18.3 ||\n|| [ITALIC] Baseline: MFCC+VTLN | [ITALIC] Baseline: MFCC+VTLN | 27.4 | 28.4 | 23.2 | 20.4 | 21.3 | 27.7 ||",
    "claim": "Applying VTLN to both UTD and cAE inputs improves performance across all languages, and cAE with VTLN outperforms MFCC+VTLN.",
    "label": "support"
  },
  {
    "id": "training_341_support",
    "table_caption": "Table: WER on Broadcast News, 400 hrs",
    "table_content": "|| model | dev04f | rt04 ||\n|| Hybrid DNN | 15.1 | 13.4 ||\n|| DNN-based Features | 15.3 | 13.5 ||\n|| Old CNN-based Features  | 13.4 | 12.2 ||\n|| Proposed CNN-based Features | 13.6 | 12.5 ||\n|| Proposed Hybrid CNN | [BOLD] 12.7 | [BOLD] 11.7 ||",
    "claim": "The Proposed Hybrid CNN achieves the lowest WER on both dev04f and rt04 datasets.",
    "label": "support"
  },
  {
    "id": "training_312_support",
    "table_caption": "Table: Results of various models on FIGER (%).",
    "table_content": "|| Model | Acc. | Macro | Micro ||\n|| NFGEC (Attentive) | 54.53 | 74.76 | 71.58 ||\n|| NFGEC (LSTM) | 55.60 | 75.15 | 71.73 ||\n|| BERT | 52.04 | 75.16 | 71.63 ||\n|| ERNIE | [BOLD] 57.19 | [BOLD] 76.51 | [BOLD] 73.39 ||",
    "claim": "ERNIE achieves the highest accuracy, macro, and micro scores among the models.",
    "label": "support"
  },
  {
    "id": "training_350_support",
    "table_caption": "Table: Mean and variance of the average BLEU score for the Diverse group. The models trained with MultiDDS-S perform better and have less variance.",
    "table_content": "|| [BOLD] Method | [BOLD] M2O  [BOLD] Mean | [BOLD] M2O  [BOLD] Var. | [BOLD] O2M  [BOLD] Mean | [BOLD] O2M  [BOLD] Var. ||\n|| MultiDDS | 26.85 | 0.04 | 18.20 | 0.05 ||\n|| MultiDDS-S | 26.94 | 0.02 | 18.24 | 0.02 ||",
    "claim": "MultiDDS-S results in smaller variance in the final model performance compared to MultiDDS.",
    "label": "support"
  },
  {
    "id": "training_301_support",
    "table_caption": "Table: Phone Accuracy (%) for the noisy TIMIT database .",
    "table_content": "|| noise model | mix uni. | mix bi. | babble uni. | babble bi. | music uni. | music bi. | factory uni. | factory bi. ||\n|| ASR | [BOLD] 22.7 | [BOLD] 37.4 | [BOLD] 22.4 | [BOLD] 37.0 | [BOLD] 22.2 | [BOLD] 36.3 | [BOLD] 21.6 | [BOLD] 36.5 ||\n|| POLQA | 21.7 | 35.7 | 21.6 | 35.6 | 21.1 | 35.3 | 21.4 | 35.5 ||",
    "claim": "ASR scores are higher than POLQA scores across all noise models and conditions.",
    "label": "support"
  },
  {
    "id": "training_240_support",
    "table_caption": "Table: DSTC2 and WOZ 2.0 test set accuracies for: a) joint goals; and b) turn-level requests. The asterisk indicates statistically significant improvement over the baseline trackers (paired t-test; p<0.05).",
    "table_content": "|| [BOLD] DST Model | [BOLD] DSTC2  [BOLD] Goals | [BOLD] DSTC2  [BOLD] Requests | [BOLD] WOZ 2.0  [BOLD] Goals | [BOLD] WOZ 2.0  [BOLD] Requests ||\n|| [BOLD] Delexicalisation-Based Model | 69.1 | 95.7 | 70.8 | 87.1 ||\n|| [BOLD] Delexicalisation-Based Model + Semantic Dictionary | 72.9* | 95.7 | 83.7* | 87.6 ||\n|| Neural Belief Tracker: NBT-DNN | 72.6* | 96.4 | [BOLD] 84.4* | 91.2* ||\n|| Neural Belief Tracker: NBT-CNN | [BOLD] 73.4* | [BOLD] 96.5 | 84.2* | [BOLD] 91.6* ||",
    "claim": "The NBT models outperform the baseline models in both joint goal and request accuracies.",
    "label": "support"
  },
  {
    "id": "training_308_support",
    "table_caption": "Table: Case-insensitive BLEU scores (%) for Chinese-to-English translation on NIST datasets. “+CSH” denotes model only trained under the supervision of child attentional adjacency matrix (β = 0). “+PSH” denotes model only trained under the supervision of parent attentional adjacency matrix (α = 0). “+CSH+PSH” is trained under the supervision of both.",
    "table_content": "|| System | NIST2005 | NIST2008 | NIST2012 | Average ||\n|| RNNsearch | 38.41 | 30.01 | 28.48 | 32.30 ||\n|| Tree2Seq  | 39.44 | 31.03 | 29.22 | 33.23 ||\n|| SE-NMT (Wu et al. 2017) | 40.01 | 31.44 | 29.45 | 33.63 ||\n|| Transformer | 43.89 | 34.83 | 32.59 | 37.10 ||\n|| +CSH | 44.21 | 36.63 | 33.57 | 38.14 ||\n|| +PSH | 44.24 | 36.17 | 33.86 | 38.09 ||\n|| +CSH+PSH | [BOLD] 44.87 | [BOLD] 36.73 | [BOLD] 34.28 | [BOLD] 38.63 ||",
    "claim": "Transformer+CSH+PSH achieves the highest BLEU scores across all test sets and on average.",
    "label": "support"
  },
  {
    "id": "training_362_support",
    "table_caption": "Table: Comparison of MultiFiT results with different pretraining corpora and ULMFiT, fine-tuned with 1k labels on MLDoc.",
    "table_content": "|| [EMPTY] | de | es | zh ||\n|| ULMFiT | 94.19 | 95.23 | 66.82 ||\n|| MultiFiT, no wiki | 95.23 | 95.07 | 90.03 ||\n|| MultiFiT, small Wiki | 95.37 | 95.30 | 89.80 ||\n|| MultiFiT | [BOLD] 95.90 | [BOLD] 96.07 | [BOLD] 92.52 ||",
    "claim": "MultiFiT outperforms ULMFiT in all languages, with a particularly pronounced improvement in Chinese.",
    "label": "support"
  },
  {
    "id": "training_310_support",
    "table_caption": "Table: BLEU scores (%) for Chinese-to-English (Zh-En), English-to-Chinese (En-Zh) translation on WMT2017 datasets and English-to-German (En-De) task. Both char-level BLEU (CBLEU) and word-level BLEU (WBLEU) are used as metrics for the En-Zh task.",
    "table_content": "|| System | Zh-En | En-Zh CBLEU | En-Zh WBLEU | En-De ||\n|| Transformer | 21.29 | 32.12 | 19.14 | 25.71 ||\n|| +CSH | 21.60 | 32.46 | 19.54 | 26.01 ||\n|| +PSH | 21.67 | 32.37 | 19.53 | 25.87 ||\n|| +CSH+PSH | [BOLD] 22.15 | [BOLD] 33.03 | [BOLD] 20.19 | [BOLD] 26.31 ||",
    "claim": "For the Chinese-to-English task, the Transformer+CSH+PSH system outperforms the baseline by 0.86 BLEU score. For the English-to-Chinese task, it gains 0.91 and 1.05 improvements on char-level BLEU and word-level BLEU, respectively. For the English-to-German task, the improvement is 0.6 BLEU score.",
    "label": "support"
  },
  {
    "id": "training_379_support",
    "table_caption": "Table: Ablation results of HMNet on AMI’s test set. “+role text” means the role vector is not used, but the role name is prepended to each turn’s transcript.",
    "table_content": "|| Model | ROUGE-1 | R-2 | R-SU4 ||\n|| HMNet | [BOLD] 52.1 | [BOLD] 19.7 | [BOLD] 24.1 ||\n|| −POS&ENT | 49.3 | 18.8 | 23.5 ||\n|| −role vector | 47.8 | 17.2 | 21.7 ||\n|| +role text | 47.4 | 18.8 | 23.7 ||\n|| −hierarchy | 45.1 | 15.9 | 20.5 ||",
    "claim": "When the role vector is removed, the ROUGE-1 score drops by 4.3 points, and when HMNet is without the hierarchy structure, the ROUGE-1 score drops by 7.0 points.",
    "label": "support"
  },
  {
    "id": "training_277_support",
    "table_caption": "Table: CLTC Results on the Webis-CLS-10C Dataset",
    "table_content": "|| Baseline | Source | Target | Baseline Results | LIFG ||\n|| SHFR-ECOC | E | F | 62.09 | 90.00 ||\n|| SHFR-ECOC | E | G | 65.22 | 91.29 ||\n|| Inverted | E | G | 49.00 | 91.00 ||\n|| DCI | E | F | 83.80 | 90.38 ||\n|| DCI | E | G | 83.80 | 92.07 ||",
    "claim": "DCI achieves higher accuracy than SHFR-ECOC and Inverted for both E to F and E to G source-target pairs.",
    "label": "support"
  },
  {
    "id": "training_346_support",
    "table_caption": "Table: WER With Improved fMLLR Features",
    "table_content": "|| Feature | WER ||\n|| VTLN-warped log-mel+d+dd | 18.8 ||\n|| proposed fMLLR + VTLN-warped log-mel+d+dd | [BOLD] 18.3 ||",
    "claim": "Applying fMLLR in a decorrelated space achieves a 0.5% improvement over the baseline VTLN-warped log-mel system.",
    "label": "support"
  },
  {
    "id": "training_320_support",
    "table_caption": "Table: Word error rates of monolingual sgmm and 10-lingual TDNN ASR system evaluated on the development sets.",
    "table_content": "|| [EMPTY] | [BOLD] Mono | [BOLD] Multi ||\n|| PL | 16.5 | 15.1 ||\n|| PT | 20.5 | 19.9 ||\n|| RU | 27.5 | 26.9 ||\n|| TH | 34.3 | 33.3 ||\n|| VI | 11.3 | 11.6 ||",
    "claim": "The multilingual model shows small improvements over the monolingual model for all languages except Vietnamese.",
    "label": "support"
  },
  {
    "id": "training_378_support",
    "table_caption": "Table: Accuracy of user dialog act prediction on the development and test sets.",
    "table_content": "|| Model | Dev | Test ||\n|| Majority | 63.3 | 62.8 ||\n|| TBCNN-pair | 84.2 | 83.9 ||",
    "claim": "TBCNN-pair achieves higher accuracy than Majority on both the development and test sets.",
    "label": "support"
  },
  {
    "id": "training_352_support",
    "table_caption": "Table: Average BLEU of the best baseline and three MultiDDS-S settings for the Diverse group. MultiDDS-S always outperform the baseline.",
    "table_content": "|| [BOLD] Setting | [BOLD] Baseline | [BOLD] MultiDDS-S  [BOLD] Regular | [BOLD] MultiDDS-S  [BOLD] Low | [BOLD] MultiDDS-S  [BOLD] High ||\n|| M2O | 26.68 | 27.00 | 26.97 | 27.08 ||\n|| O2M | 17.94 | 18.24 | 17.95 | 18.55 ||",
    "claim": "In the O2M setting, MultiDDS-S High achieves the highest performance with a value of 18.55.",
    "label": "support"
  },
  {
    "id": "training_330_support",
    "table_caption": "Table: Comparison to other systems for full diacritization",
    "table_content": "|| Setup | WER% ||\n|| MSA | MSA ||\n|| [BOLD] Our System | [BOLD] 6.0 ||\n|| Microsoft ATKS | 12.2 ||\n|| Farasa | 12.8 ||\n|| RDI (rashwan2015deep) | 16.0 ||\n|| MADAMIRA (pasha2014madamira) | 19.0 ||\n|| MIT (belinkov2015arabic) | 30.5 ||\n|| CA | CA ||\n|| Our system | [BOLD] 4.3 ||\n|| Our best MSA system on CA | 14.7 ||",
    "claim": "Our system achieves a WER of 6.0% for MSA and 4.3% for CA, which is better than using our best MSA system to diacritize CA.",
    "label": "support"
  },
  {
    "id": "training_335_support",
    "table_caption": "Table: Comparing our system to state-of-the-art systems – Core word diacritics",
    "table_content": "|| System | Error Rate WER | Error Rate DER ||\n|| MSA | MSA | MSA ||\n|| [BOLD] Our system | [BOLD] 2.9 | [BOLD] 0.9 ||\n|| (rashwan2015deep) | 3.0 | 1.0 ||\n|| Farasa | 3.3 | 1.1 ||\n|| Microsoft ATKS | 5.7 | 2.0 ||\n|| MADAMIRA | 6.7 | 1.9 ||\n|| (belinkov2015arabic) | 14.9 | 3.9 ||\n|| CA | CA | CA ||\n|| Our system | 2.2 | 0.9 ||\n|| Our best MSA system on CA | 8.5 | 3.7 ||",
    "claim": "Our system achieves the lowest error rates (WER and DER) compared to the other systems listed.",
    "label": "support"
  },
  {
    "id": "training_245_support",
    "table_caption": "Table: The performance of GCBiA with negative sampling augmentation method conditioned on over 0.999 on-topic recall.",
    "table_content": "|| Model | Seen PPR3 | Seen AOR | Unseen PPR3 | Unseen AOR ||\n|| GCBiA | 93.6 | 79.2 | 68.0 | 45.0 ||\n|| + neg sampling | [BOLD] 94.2 | [BOLD] 88.2 | [BOLD] 79.4 | [BOLD] 69.1 ||",
    "claim": "The GCBiA model with negative sampling achieves higher performance across all conditions compared to the GCBiA model without negative sampling.",
    "label": "support"
  },
  {
    "id": "training_306_support",
    "table_caption": "Table: Performance of discriminative models on VisDial v0.9. Higher is better for MRR and recall@k, while lower is better for mean rank. (*) denotes use of external knowledge.",
    "table_content": "|| Model | MRR | R@1 | R@5 | R@10 | Mean ||\n|| LF  | 0.5807 | 43.82 | 74.68 | 84.07 | 5.78 ||\n|| HRE  | 0.5846 | 44.67 | 74.50 | 84.22 | 5.72 ||\n|| HREA  | 0.5868 | 44.82 | 74.81 | 84.36 | 5.66 ||\n|| MN  | 0.5965 | 45.55 | 76.22 | 85.37 | 5.46 ||\n|| HieCoAtt-QI  | 0.5788 | 43.51 | 74.49 | 83.96 | 5.84 ||\n|| AMEM  | 0.6160 | 47.74 | 78.04 | 86.84 | 4.99 ||\n|| HCIAE-NP-ATT  | 0.6222 | 48.48 | 78.75 | 87.59 | 4.81 ||\n|| SF-QIH-se-2  | 0.6242 | 48.55 | 78.96 | 87.75 | 4.70 ||\n|| CorefNMN * | 0.636 | 50.24 | 79.81 | 88.51 | 4.53 ||\n|| CoAtt-GAN-w/  [BOLD] R [ITALIC] inte-TF  | 0.6398 | 50.29 | 80.71 | 88.81 | 4.47 ||\n|| CorefNMN (ResNet-152) * | 0.641 | 50.92 | 80.18 | 88.81 | 4.45 ||\n|| FGA (VGG) | 0.6525 | 51.43 | 82.08 | 89.56 | 4.35 ||\n|| FGA (F-RCNNx101) | 0.6712 | 54.02 | 83.21 | 90.47 | 4.08 ||\n|| 9×FGA (VGG) | [BOLD] 0.6892 | [BOLD] 55.16 | [BOLD] 86.26 | [BOLD] 92.95 | [BOLD] 3.39 ||",
    "claim": "9×FGA (VGG) achieves the highest performance across all metrics (MRR, R@1, R@5, R@10, Mean) in the table.",
    "label": "support"
  },
  {
    "id": "training_316_support",
    "table_caption": "Table: Fluency of reviews (in MOS). Bold font indicates highest score.",
    "table_content": "|| Model | Native | Amazon Non-native | Overall | Native | Yelp Non-native | Overall ||\n|| Original review | 2.85 | 3.09 | 2.95 | [BOLD] 3.43 | [BOLD] 3.56 | [BOLD] 3.49 ||\n|| Pretrained GPT-2 | 2.93 | 3.16 | 3.06 | 2.68 | 2.72 | 2.70 ||\n|| Fine-tuned GPT-2 | 3.24 | 3.22 | 3.23 | 3.35 | 3.25 | 3.30 ||\n|| mLSTM | 3.06 | [BOLD] 3.37 | 3.21 | 3.12 | 2.96 | 3.04 ||\n|| Sentiment modeling | [BOLD] 3.61 | 3.35 | [BOLD] 3.47 | 2.90 | 2.86 | 2.88 ||",
    "claim": "For the Amazon dataset, sentiment modeling had the highest overall score, and all fake reviews scored higher than the original reviews. For the Yelp dataset, the original reviews scored higher than the fake ones, with the fine-tuned GPT-2 model having the highest score among fake reviews.",
    "label": "support"
  },
  {
    "id": "training_386_support",
    "table_caption": "Table: Speech translation results. BLEU scores (↑) are reported. We also include the results of the cascaded system (ASR + MT) reported in [28] and the S-Transformer model reported in [29]. Only the results on the test set are available for these two approaches.",
    "table_content": "|| Methods | dev | test ||\n|| Cascaded | - | 14.6 ||\n|| S-Transformer | - | 13.8 ||\n|| log Mel | 12.5 | 12.9 ||\n|| CPC | 12.1 | 12.5 ||\n|| R-APC | 13.5 | 13.8 ||\n|| T-APC | 13.7 | 14.3 ||\n|| PASE10 | 12.0 | 12.4 ||\n|| CPC10 | 11.8 | 12.3 ||\n|| R-APC10 | 13.2 | 13.7 ||\n|| T-APC10 | 12.8 | 13.4 ||",
    "claim": "The RNN-based model with T-APC features (14.3) outperforms S-Transformer (13.8) on the test set.",
    "label": "support"
  },
  {
    "id": "training_391_support",
    "table_caption": "Table: Comparing our models with recent results on the 2002 CoNLL Dutch and Spanish NER datasets.",
    "table_content": "|| Model | Dutch | Spanish ||\n|| Carreras et al.  Carreras et al. ( 2002 ) | 77.05 | 81.39 ||\n|| Nothman et al.  Nothman et al. ( 2013 ) | 78.60 | [EMPTY] ||\n|| dos Santos and Guimarães  dos Santos and Guimarães ( 2015 ) | [EMPTY] | 82.21 ||\n|| Gillick et al.  Gillick et al. ( 2015 ) | 82.84 | 82.95 ||\n|| Lample et al.  Lample et al. ( 2016 ) | 81.74 | 85.75 ||\n|| LSTM-CRF-T | 83.91 | 84.89 ||\n|| LSTM-CRF-TI | 84.12 | 85.28 ||\n|| LSTM-CRF-TI(g) | [BOLD] 84.51 | [BOLD] 85.92 ||",
    "claim": "The LSTM-CRF-TI(g) model achieves the highest performance on both Dutch and Spanish NER tasks.",
    "label": "support"
  },
  {
    "id": "training_307_support",
    "table_caption": "Table: Performance on the question generation task. Higher is better for MRR and recall@k, while lower is better for mean rank.",
    "table_content": "|| Model | MRR | R@1 | R@5 | R@10 | Mean ||\n|| SF-QIH-se-2  | 0.4060 | 26.76 | 55.17 | 70.39 | 9.32 ||\n|| FGA | [BOLD] 0.4138 | [BOLD] 27.42 | [BOLD] 56.33 | [BOLD] 71.32 | [BOLD] 9.1 ||",
    "claim": "FGA outperforms SF-QIH-se-2 across all performance metrics (MRR, R@1, R@5, R@10, and Mean).",
    "label": "support"
  },
  {
    "id": "training_375_support",
    "table_caption": "Table: Results on development (average over random splits) and test set. Middle: results on all examples. Bottom: results on the subset where candidate extraction succeeded.",
    "table_content": "|| [BOLD] System | [BOLD] Dev  [BOLD] F1 | [BOLD] Dev  [BOLD] p@1 | [BOLD] Test  [BOLD] F1 | [BOLD] Test  [BOLD] p@1 | [BOLD] Test  [BOLD] MRR ||\n|| STAGG | - | - | 37.7 | - | - ||\n|| CompQ | - | - | [BOLD] 40.9 | - | - ||\n|| WebQA | 35.3 | 36.4 | 32.6 | 33.5 | 42.4 ||\n|| WebQA-extrapol | - | - | 34.4 | - | - ||\n|| CompQ-Subset | - | - | 48.5 | - | - ||\n|| WebQA-Subset | 53.6 | 55.1 | 51.9 | 53.4 | 67.5 ||",
    "claim": "WebQA obtained a 32.6 F1 score compared to a 40.9 F1 score for CompQ, and the WebQA-Subset achieved a 51.9 F1 score on test examples where candidate extraction succeeded.",
    "label": "support"
  },
  {
    "id": "training_332_support",
    "table_caption": "Table: MSA case errors accounting from more than 1% of errors",
    "table_content": "|| Error | Count | % | Most Common Causes ||\n|| a ⇔ u | 133 | 19.3 | [ITALIC] POS error: ex. “ka$afa” (كَشَفَ¿ – he exposed) vs. “ka$ofu” (كَشْفُ¿ – exposure) &  [ITALIC] Subject vs. object: ex. “tuwHy  [BOLD] mivolu” (تُوحِي مِثْلُ¿ –  [BOLD] such indicates) vs. “tuwHy  [BOLD] mivola” (تُوحِي مِثْلَ¿ – she indicates  [BOLD] such) ||\n|| i ⇔ a | 130 | 18.9 | [ITALIC] Incorrect attachment (due to coordinating conjunction or distant attachment): ex. “Alogaza Alomusay~ili lilidumuEi –  [BOLD] wa+AlraSaSi vs.  [BOLD] wa+AlraSaSa (الغَازَ الْمُسَيِّلَ لِلدُمُوعِ والرَصَاص¿ – tear gas and bullets) where bullets were attached incorrectly to tear instead of gas &  [ITALIC] indeclinability such as foreign words and feminine names: ex. “kaAnuwni” (كَانُونِ¿ – Cyrillic month name) vs. “kaAuwna” (كَانُونَ¿) ||\n|| i ⇔ u | 95 | 13.8 | [ITALIC] POS error of previous word: ex. “tadahowuru  [BOLD] waDoEihi” (تَدَهْؤُرُ وَضْعِهِ¿ – deterioration of his situation – situtation is part of idafa construct) vs. “tadahowara  [BOLD] waDoEihu” (تَذَهْوَرَ وَضْعُهُ¿ – his situation deteriorated – situation is subject) &  [ITALIC] Incorrect attachment (due to coordinating conjunction or distant attachment): (as example for i ⇔ a) ||\n|| a ⇔ o | 60 | 8.7 | [ITALIC] Foreign named entities: ex. “siyraAloyuna” (سِيرَالْيُونَ¿ – Siera Leon) vs. “siyraAloyuno” (سِيرَالْيُونْ¿) ||\n|| i ⇔ K | 27 | 4.0 | [ITALIC] Incorrect Idafa: “ [BOLD] liAt~ifaqi ha⁢aA Alo>usobuwE” (لِاتِفَاقِ هَذَا الأُسْبُوع¿ – this week’s agreement) vs. “ [BOLD] liAt~ifaqK ha⁢aA Alo>usobuwE” (لِاتِّفَاقٍ هَذَا الْأُسْبُوع¿ – to an agreement this week) ||\n|| K ⇔ N | 29 | 4.2 | [ITALIC] Subject vs. object (as in a ⇔ u) and Incorrect attachment (as in i ⇔ a) ||\n|| F ⇔ N | 25 | 3.7 | [ITALIC] Words ending with feminine marker “p” or “At”: ex. “muHaADarap” (مُحَاضَرَة¿ – lecture) ||\n|| i ⇔ o | 22 | 3.2 | [ITALIC] Foreign named entities (as in a ⇔ o) ||\n|| F ⇔ a | 16 | 2.3 | [ITALIC] Incorrect Idafa (as in i ⇔ K) ||\n|| u ⇔ o | 14 | 2.0 | [ITALIC] Foreign named entities (as in a ⇔ o) ||\n|| F ⇔ K | 9 | 1.3 | [ITALIC] Words ending with feminine marker (as in F ⇔ N) ||\n|| K ⇔ a | 8 | 1.2 | [ITALIC] Incorrect Idafa (as in i ⇔ K) ||",
    "claim": "The most common error type involves guessing a fatHa (a) instead of damma (u) or vice versa, accounting for 19.3% of errors.",
    "label": "support"
  },
  {
    "id": "training_373_support",
    "table_caption": "Table: Task 2: Results for our three official submissions, baselines, and top three teams. Evaluation measures for Task 2 are micro-averaged P, R, and F1-score for class 1 (intake) and class 2 (possible intake).",
    "table_content": "|| [BOLD] Submission | [ITALIC] Pclass1+ [ITALIC] class2 | [ITALIC] Rclass1+ [ITALIC] class2 | [ITALIC] Fclass1+ [ITALIC] class2 ||\n|| [ITALIC] a. Baselines | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| a.1. Assigning class 2 to all instances | 0.359 | 0.609 | 0.452 ||\n|| a.2. SVM-unigrams | 0.680 | 0.616 | 0.646 ||\n|| [ITALIC] b. Top 3 teams in the shared task | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| b.1. InfyNLP | 0.725 | 0.664 | 0.693 ||\n|| b.2. UKNLP | 0.701 | 0.677 | 0.689 ||\n|| b.3. NRC-Canada | 0.708 | 0.642 | 0.673 ||\n|| [ITALIC] c. NRC-Canada official submissions | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| c.1. submission 1 | 0.708 | 0.642 | 0.673 ||\n|| c.2. submission 2 | 0.705 | 0.639 | 0.671 ||\n|| c.3. submission 3 | 0.704 | 0.635 | 0.668 ||",
    "claim": "Submission 1 achieves the highest Fclass1+class2 score among the NRC-Canada submissions, and the SVM-unigrams baseline achieves an Fclass1+class2 score of 0.646.",
    "label": "support"
  },
  {
    "id": "training_374_support",
    "table_caption": "Table: Feature ablation results. The five features that lead to largest drop in performance are displayed.",
    "table_content": "|| [BOLD] Feature Template | [BOLD] F1 | Δ ||\n|| WebQA | 53.6 | [EMPTY] ||\n|| - Max-NE | 51.8 | -1.8 ||\n|| - Ne+Common | 51.8 | -1.8 ||\n|| - Google Rank | 51.4 | -2.2 ||\n|| - In Quest | 50.1 | -3.5 ||\n|| - TF-IDF | 41.5 | -12 ||",
    "claim": "Removing the TF-IDF feature results in a 12-point drop in F1 score.",
    "label": "support"
  },
  {
    "id": "training_257_support",
    "table_caption": "Table: Perplexities on PTB for various LMs.",
    "table_content": "|| Model | Test PPL ||\n|| KN 5-gram  | 141 ||\n|| FNNLM  | 140 ||\n|| RNNLM  | 123 ||\n|| LSTM  | 117 ||\n|| bigram FNNLM | 176 ||\n|| trigram FNNLM | 131 ||\n|| 4-gram FNNLM | 118 ||\n|| 5-gram FNNLM | 114 ||\n|| 6-gram FNNLM | 113 ||\n|| 1st-order FOFE-FNNLM | 116 ||\n|| 2nd-order FOFE-FNNLM | [BOLD] 108 ||",
    "claim": "The 2nd-order FOFE-FNNLM achieves the lowest perplexity of 108, outperforming all other models listed in the table.",
    "label": "support"
  },
  {
    "id": "training_305_support",
    "table_caption": "Table: Performance of discriminative models on VisDial v1.0 test-std. Higher is better for MRR and recall@k, while lower is better for mean rank and NDCG. (*) denotes use of external knowledge.",
    "table_content": "|| Model | MRR | R@1 | R@5 | R@10 | Mean | NDCG ||\n|| LF  | 0.554 | 40.95 | 72.45 | 82.83 | 5.95 | 0.453 ||\n|| HRE  | 0.542 | 39.93 | 70.45 | 81.50 | 6.41 | 0.455 ||\n|| MN  | 0.555 | 40.98 | 72.30 | 83.30 | 5.92 | 0.475 ||\n|| CorefNMN (ResNet-152) * | 0.615 | 47.55 | 78.10 | 88.80 | 4.40 | 0.547 ||\n|| NMN (ResNet-152) * | 0.588 | 44.15 | 76.88 | 86.88 | 4.81 | [BOLD] 0.581 ||\n|| FGA (VGG) | 0.637 | 49.58 | 80.97 | 88.55 | 4.51 | 0.521 ||\n|| FGA (F-RCNNx101) | 0.662 | 52.75 | 82.92 | 91.07 | 3.8 | 0.569 ||\n|| 5×FGA (VGG) | 0.673 | 53.40 | 85.28 | 92.70 | 3.54 | 0.545 ||\n|| 5×FGA (F-RCNNx101) | [BOLD] 0.693 | [BOLD] 55.65 | [BOLD] 86.73 | [BOLD] 94.05 | [BOLD] 3.14 | 0.572 ||",
    "claim": "The 5×FGA (F-RCNNx101) model achieves the highest scores in MRR, R@1, R@5, R@10, and Mean among the models listed.",
    "label": "support"
  },
  {
    "id": "training_296_support",
    "table_caption": "Table: Results of End2End compared to our model (with GS and ST variants) on hypernym extraction.",
    "table_content": "|| Model | Accuracy ||\n|| +GS +ST | 84.0 ||\n|| +GS -ST | 61.0 ||\n|| -GS +ST | 84.7 ||\n|| -GS -ST | 54.7 ||\n|| End2End | 86.5 ||",
    "claim": "The ST estimator is crucial for achieving higher accuracy, as models with ST (+ST) have higher accuracy than those without ST (-ST), regardless of the use of GS.",
    "label": "support"
  },
  {
    "id": "training_340_support",
    "table_caption": "Table: Effect of self-attention (1-head) in each encoder. We denote the image encoder using selective spatial pooling and the text encoder using the last hidden state which are used in Engilberg [3] as W/O attention. We report only R@1 for effective comparison.",
    "table_content": "|| Image Encoder | Text Encoder | Sentence Retrieval(R@1) | Image Retrieval(R@1) ||\n|| W/O attention  | W/O attention  | 69.8 | 55.9 ||\n|| Our image encoder(1-head) | W/O attention  | [BOLD] 70.1 | 55.8 ||\n|| W/O attention  | Our text encoder(1-head) | 69.9 | [BOLD] 56.2 ||",
    "claim": "The image encoder with single-head attention achieves the highest sentence retrieval score, while the text encoder with single-head attention achieves the highest image retrieval score.",
    "label": "support"
  },
  {
    "id": "training_366_support",
    "table_caption": "Table: F1 scores calculated using conlleval.pl script for NER taggers. The table shows that SpeedRead F1 score is 10% below the sate-of-art achieved by SENNA.",
    "table_content": "|| [80pt][l]PhaseDataset | Training | Dev | Test ||\n|| SR+Gold Chunks | 90.80 | 91.98 | 87.87 ||\n|| SpeeRead | 82.05 | 83.35 | 78.28 ||\n|| Stanford | 99.28 | 92.98 | 89.03 ||\n|| SENNA | 96.75 | 97.24 | 89.58 ||",
    "claim": "The scores drop around 9.5% in F1 scores when using chunks generated by SpeeRead compared to SR+Gold Chunks.",
    "label": "support"
  },
  {
    "id": "training_268_support",
    "table_caption": "Table: Cross-Topic",
    "table_content": "|| [EMPTY] | Method | Topics Abortion | Topics Cloning | Topics Death penalty | Topics Gun control | Topics Marij. legal. | Topics Min. wage | Topics Nucl. energy | Topics School unif. | \\diameter ||\n|| two-classes | BiLSTM | 0.61 | 0.72 | 0.70 | 0.75 | 0.64 | 0.62 | 0.67 | 0.54 | 0.66 ||\n|| two-classes | BiCLSTM | 0.67 | 0.71 | 0.71 | 0.73 | 0.69 | 0.75 | 0.71 | 0.58 | 0.70 ||\n|| two-classes | TACAM-WE | 0.64 | 0.71 | 0.70 | 0.74 | 0.64 | 0.63 | 0.68 | 0.55 | 0.66 ||\n|| two-classes | TACAM-KG | 0.62 | 0.69 | 0.70 | 0.75 | 0.64 | 0.76 | 0.71 | 0.56 | 0.68 ||\n|| two-classes | CAM-BERT Base | 0.61 | 0.77 | 0.74 | 0.76 | 0.74 | 0.61 | 0.76 | 0.73 | 0.72 ||\n|| two-classes | CAM-BERT Large | 0.62 | 0.79 | 0.75 | 0.77 | 0.77 | 0.65 | 0.75 | 0.73 | 0.73 ||\n|| [EMPTY] | TACAM-BERT Base | 0.78 | 0.77 | [BOLD] 0.78 | 0.80 | [BOLD] 0.79 | 0.83 | 0.80 | [BOLD] 0.83 | 0.80 ||\n|| [EMPTY] | TACAM-BERT Large | [BOLD] 0.79 | [BOLD] 0.78 | [BOLD] 0.78 | [BOLD] 0.81 | [BOLD] 0.79 | [BOLD] 0.84 | [BOLD] 0.83 | 0.82 | [BOLD] 0.80 ||\n|| three-classes | BiLSTM | 0.47 | 0.52 | 0.48 | 0.48 | 0.44 | 0.42 | 0.48 | 0.42 | 0.46 ||\n|| three-classes | BiCLSTM | 0.49 | 0.52 | 0.46 | 0.51 | 0.46 | 0.44 | 0.47 | 0.42 | 0.47 ||\n|| three-classes | TACAM-WE | 0.47 | 0.52 | 0.47 | 0.48 | 0.46 | 0.46 | 0.48 | 0.41 | 0,47 ||\n|| three-classes | TACAM-KG | 0.46 | 0.51 | 0.47 | 0.47 | 0.46 | 0.48 | 0.47 | 0.41 | 0.47 ||\n|| three-classes | CAM-BERT Base | 0.38 | 0.63 | 0.53 | 0.49 | 0.54 | 0.54 | 0.61 | 0,50 | 0.53 ||\n|| three-classes | TACAM-BERT Base | 0.42 | 0.68 | 0.54 | 0.50 | 0.60 | 0.49 | 0.64 | [BOLD] 0.69 | 0.57 ||\n|| [EMPTY] | CAM-BERT Large | 0.53 | 0.67 | 0.56 | 0.53 | 0.59 | 0.66 | 0.67 | 0.66 | 0.61 ||\n|| [EMPTY] | TACAM-BERT Large | [BOLD] 0.54 | [BOLD] 0.69 | [BOLD] 0.59 | [BOLD] 0.55 | [BOLD] 0.63 | [BOLD] 0.69 | [BOLD] 0.71 | [BOLD] 0.69 | [BOLD] 0.64 ||",
    "claim": "In the two-classes scenario, TACAM-BERT Large achieves the highest average score of 0.80, and in the three-classes scenario, TACAM-BERT Large achieves the highest average score of 0.64.",
    "label": "support"
  },
  {
    "id": "training_300_support",
    "table_caption": "Table: Command Accuracy (%) for different commands at different SERs.",
    "table_content": "|| SER (dB) params. | −35∼−30 ASR | −35∼−30 POLQA | −30∼−25 ASR | −30∼−25 POLQA | −25∼−20 ASR | −25∼−20 POLQA ||\n|| BACK | 73 | 47 | 83 | 50 | 90 | 53 ||\n|| NEXT | 70 | 50 | 90 | 57 | 90 | 63 ||\n|| PLAY | 80 | 67 | 94 | 80 | 96 | 83 ||\n|| PAUSE | 76 | 50 | 87 | 57 | 87 | 60 ||",
    "claim": "ASR scores are higher than POLQA scores across all SER conditions for each command.",
    "label": "support"
  },
  {
    "id": "training_356_support",
    "table_caption": "Table: Ubuntu evaluation using F1 metrics w.r.t. activities and entities. G-VHRED, P-VHRED and H-VHRED all outperform the baseline HRED. G-VHRED performs best w.r.t. activities and H-VHRED performs best w.r.t. entities.",
    "table_content": "|| [BOLD] Model | [BOLD] Activity | [BOLD] Entity ||\n|| [ITALIC] HRED | 4.77 | 2.43 ||\n|| [ITALIC] G-VHRED | [BOLD] 9.24 | 2.49 ||\n|| [ITALIC] P-VHRED | 5 | 2.49 ||\n|| [ITALIC] H-VHRED | 8.41 | [BOLD] 3.72 ||",
    "claim": "G-VHRED performs best with respect to activities, while H-VHRED performs best with respect to entities.",
    "label": "support"
  },
  {
    "id": "training_361_support",
    "table_caption": "Table: Comparison of zero-shot, translation-based and supervised methods (with 2k training examples) on all domains of CLS. MT-BOW and CL-SCL results are from Zhou et al. (2016).",
    "table_content": "|| [EMPTY] | [EMPTY] | de Books | de DVD | de Music | fr Books | fr DVD | fr Music | ja Books | ja DVD | ja Music ||\n|| [ITALIC] Zero-shot | LASER, code | 84.15 | 78.00 | 79.15 | 83.90 | 83.40 | 80.75 | 74.99 | 74.55 | 76.30 ||\n|| [ITALIC] Zero-shot | MultiBERT | 72.15 | 70.05 | 73.80 | 75.50 | 74.70 | 76.05 | 65.41 | 64.90 | 70.33 ||\n|| [ITALIC] Zero-shot | MultiFiT, pseudo | [BOLD] 89.60 | [BOLD] 81.80 | [BOLD] 84.40 | [BOLD] 87.84 | [BOLD] 83.50 | [BOLD] 85.60 | [BOLD] 80.45 | [BOLD] 77.65 | [BOLD] 81.50 ||\n|| [ITALIC] Translat. | MT-BOW | 79.68 | 77.92 | 77.22 | 80.76 | 78.83 | 75.78 | 70.22 | 71.30 | 72.02 ||\n|| [ITALIC] Translat. | CL-SCL | 79.50 | 76.92 | 77.79 | 78.49 | 78.80 | 77.92 | 73.09 | 71.07 | 75.11 ||\n|| [ITALIC] Translat. | BiDRL | 84.14 | 84.05 | 84.67 | 84.39 | 83.60 | 82.52 | 73.15 | 76.78 | 78.77 ||\n|| [ITALIC] Super. | MultiBERT | 86.05 | 84.90 | 82.00 | 86.15 | 86.90 | 86.65 | 80.87 | 82.83 | 79.95 ||\n|| [ITALIC] Super. | MultiFiT | [BOLD] 93.19 | [BOLD] 90.54 | [BOLD] 93.00 | [BOLD] 91.25 | [BOLD] 89.55 | [BOLD] 93.40 | [BOLD] 86.29 | [BOLD] 85.75 | [BOLD] 86.59 ||",
    "claim": "MultiFiT outperforms LASER in the zero-shot setting and outperforms MultiBERT in the supervised setting across all domains.",
    "label": "support"
  },
  {
    "id": "training_405_support",
    "table_caption": "Table: Performance of NLProlog when extracted facts are used as input. Average accuracy over 3 runs is reported. NLProlog empowered by 21 natural language explanations and 5 hand-written rules achieves 1% gain in accuracy.",
    "table_content": "|| [EMPTY] | |S [ITALIC] a| | |S [ITALIC] u| | Accuracy ||\n|| NLProlog (published code) | 0 | 0 | 74.57 ||\n|| + S [ITALIC] a | 103 | 0 | 74.40 ||\n|| + S [ITALIC] u (confidence >0.3) | 103 | 340 | 74.74 ||\n|| + S [ITALIC] u (confidence >0.2) | 103 | 577 | 75.26 ||\n|| + S [ITALIC] u (confidence >0.1) | 103 | 832 | [BOLD] 75.60 ||",
    "claim": "Adding structured facts with a confidence threshold >0.1 results in the highest accuracy of 75.60.",
    "label": "support"
  },
  {
    "id": "training_355_support",
    "table_caption": "Table: Test perplexities on three document modeling tasks: 20-NewGroup (20-NG), Reuters corpus (RCV1) and CADE12 (CADE). Perplexities were calculated using 10 samples to estimate the variational lower-bound. The H-NVDM models perform best across all three datasets.",
    "table_content": "|| [BOLD] Model | [BOLD] 20-NG | [BOLD] RCV1 | [BOLD] CADE ||\n|| [ITALIC] LDA | 1058 | −− | −− ||\n|| [ITALIC] docNADE | 896 | −− | −− ||\n|| [ITALIC] NVDM | 836 | −− | −− ||\n|| [ITALIC] G-NVDM | 651 | 905 | 339 ||\n|| [ITALIC] H-NVDM-3 | 607 | 865 | [BOLD] 258 ||\n|| [ITALIC] H-NVDM-5 | [BOLD] 566 | [BOLD] 833 | 294 ||",
    "claim": "H-NVDM-5 achieves the lowest perplexity on the 20-NG and RCV1 datasets, while H-NVDM-3 achieves the lowest perplexity on the CADE dataset.",
    "label": "support"
  },
  {
    "id": "training_401_support",
    "table_caption": "Table: Entity Discovery Performance of our method on the KBP2015 EDL evaluation data, with comparison to the best system in KBP2015 official evaluation.",
    "table_content": "|| [EMPTY] | 2015 track best  [ITALIC] P | 2015 track best  [ITALIC] R | 2015 track best  [ITALIC] F1 | ours  [ITALIC] P | ours  [ITALIC] R | ours  [ITALIC] F1 ||\n|| Trilingual | 75.9 | 69.3 | 72.4 | 78.3 | 69.9 | [BOLD] 73.9 ||\n|| English | 79.2 | 66.7 | [BOLD] 72.4 | 77.1 | 67.8 | 72.2 ||\n|| Chinese | 79.2 | 74.8 | [BOLD] 76.9 | 79.3 | 71.7 | 75.3 ||\n|| Spanish | 78.4 | 72.2 | 75.2 | 79.9 | 71.8 | [BOLD] 75.6 ||",
    "claim": "The overall trilingual entity discovery performance is better than the best system from the 2015 track, with an F1 score of 73.9 compared to 72.4.",
    "label": "support"
  },
  {
    "id": "training_385_support",
    "table_caption": "Table: ASR WER results using different numbers of GRU layers for the encoder in the ASR seq2seq model.",
    "table_content": "|| Features | Number of encoder layers 1 | Number of encoder layers 2 | Number of encoder layers 3 | Number of encoder layers 4 ||\n|| log Mel | 28.8 | 23.5 | 20.8 | 18.3 ||\n|| CPC | 34.3 | 29.8 | 25.2 | 23.7 ||\n|| R-APC | 26.2 | 20.3 | 17.6 | 15.2 ||\n|| T-APC | 25.2 | 18.6 | 15.8 | 13.7 ||\n|| PASE10 | 29.4 | 25.7 | 22.5 | 20.8 ||\n|| CPC10 | 35.8 | 31.3 | 26.0 | 24.4 ||\n|| R-APC10 | 27.6 | 22.3 | 19.6 | 17.6 ||\n|| T-APC10 | 28.1 | 23.2 | 20.6 | 18.0 ||",
    "claim": "T-APC with 2 layers performs similarly to log Mel with 4 layers (18.6 vs. 18.3).",
    "label": "support"
  },
  {
    "id": "training_348_support",
    "table_caption": "Table: HF Seq. Training WER Per CE Iteration",
    "table_content": "|| CE Iter | # Times Annealed | CE WER | HF WER ||\n|| 4 | 1 | 20.8 | 15.3 ||\n|| 6 | 2 | 19.8 | 15.0 ||\n|| 8 | 3 | 19.4 | 15.0 ||\n|| 13 | 7 | 18.8 | 15.0 ||",
    "claim": "After two annealing times, the HF WER remains constant at 15.0, indicating that further CE iterations do not improve HF WER.",
    "label": "support"
  },
  {
    "id": "training_318_support",
    "table_caption": "Table: Word error rates of monolingual sgmm and 10-lingual TDNN ASR system evaluated on the development sets.",
    "table_content": "|| [BOLD] Language | [BOLD] Mono | [BOLD] Multi ||\n|| BG | 17.5 | 16.9 ||\n|| CS | 17.1 | 15.7 ||\n|| DE | 9.6 | 9.3 ||\n|| FR | 24.5 | 24.0 ||\n|| KO | 20.3 | 19.3 ||",
    "claim": "The multilingual model shows small improvements over the monolingual model for BG, CS, DE, FR, and KO.",
    "label": "support"
  },
  {
    "id": "training_283_support",
    "table_caption": "Table: Figure 3: (top) bAbI QA dataset (Weston et al., 2016) visualization of update and reset gates in QRN ‘2r’ model (bottom two) bAbI dialog and DSTC2 dialog dataset (Bordes and Weston, 2016) visualization of update and reset gates in QRN ‘2r’ model. Note that the stories can have as many as 800+ sentences; we only show part of them here. More visualizations are shown in Figure 4 (bAbI QA) and Figure 5 (dialog datasets).",
    "table_content": "|| Task 6: DSTC2 dialog | Layer 1  [ITALIC] z1 | Layer 1 → [ITALIC] r1 | Layer 1 ← [ITALIC] r1 | Layer 2  [ITALIC] z2 ||\n|| Spanish food. | 0.84 | 0.07 | 0.00 | 0.82 ||\n|| You are lookng for a spanish restaurant right? | 0.98 | 0.02 | 0.49 | 0.75 ||\n|| Yes. | 0.01 | 1.00 | 0.33 | 0.13 ||\n|| What part of town do you have in mind? | 0.20 | 0.73 | 0.41 | 0.11 ||\n|| I don’t care. | 0.00 | 1.00 | 0.02 | 0.00 ||\n|| What price range would you like? | 0.72 | 0.46 | 0.52 | 0.72 ||\n|| I don’t care. API CALL spanish R-location R-price | I don’t care. API CALL spanish R-location R-price | I don’t care. API CALL spanish R-location R-price | I don’t care. API CALL spanish R-location R-price | I don’t care. API CALL spanish R-location R-price ||",
    "claim": "In dialog Task 6, the model focuses on sentences containing \"Spanish\" and does not concentrate much on other facts such as \"I don’t care.\"",
    "label": "support"
  },
  {
    "id": "training_399_support",
    "table_caption": "Table: (Left) Comparison against the best prior work for NMT on the IWSLT 2014 German-English test set. (Upper Right) Comparison of inference alternatives of variational attention on IWSLT 2014. (Lower Right) Comparison of different models in terms of implied discrete entropy (lower = more certain alignment).",
    "table_content": "|| Model | Entropy NMT | Entropy VQA ||\n|| Soft Attention | 1.24 | 2.70 ||\n|| Marginal Likelihood | 0.82 | 2.66 ||\n|| Hard Attention + Enum | 0.05 | 0.73 ||\n|| Hard Attention + Sample | 0.07 | 0.58 ||\n|| Variational Relaxed Attention | 2.02 | - ||\n|| Variational Attention + Enum | 0.54 | 2.07 ||\n|| Variational Attention + Sample | 0.52 | 2.44 ||",
    "claim": "Hard Attention + Enum and Hard Attention + Sample have the lowest entropy values for both NMT and VQA tasks.",
    "label": "support"
  },
  {
    "id": "training_313_support",
    "table_caption": "Table: Results of various models on Open Entity (%).",
    "table_content": "|| Model | P | R | F1 ||\n|| NFGEC (LSTM) | 68.80 | 53.30 | 60.10 ||\n|| UFET | 77.40 | 60.60 | 68.00 ||\n|| BERT | 76.37 | 70.96 | 73.56 ||\n|| ERNIE | [BOLD] 78.42 | [BOLD] 72.90 | [BOLD] 75.56 ||",
    "claim": "BERT and ERNIE achieve higher recall scores than NFGEC and UFET. ERNIE improves the precision by 2% and the recall by 2% compared to BERT.",
    "label": "support"
  },
  {
    "id": "training_370_support",
    "table_caption": "Table: Task 2: Results of our best system (submission 1) on the test set when one of the feature groups is removed.",
    "table_content": "|| [BOLD] Submission | [ITALIC] Pclass1+ [ITALIC] class2 | [ITALIC] Rclass1+ [ITALIC] class2 | [ITALIC] Fclass1+ [ITALIC] class2 ||\n|| a. submission 1 (all features) | 0.708 | 0.642 | 0.673 ||\n|| b. all − general textual features | 0.697 | 0.603 | 0.647 ||\n|| b.1. all − general  [ITALIC] n-grams | 0.676 | 0.673 | 0.674 ||\n|| b.2. all − general embeddings | 0.709 | 0.638 | 0.671 ||\n|| b.3. all − general clusters | 0.685 | 0.671 | 0.678 ||\n|| b.4. all − negation − Twitter-specific − punctuation | 0.683 | 0.670 | 0.676 ||\n|| c. all − domain-specific features | 0.679 | 0.653 | 0.666 ||\n|| c.1. all − domain generalized  [ITALIC] n-grams | 0.680 | 0.652 | 0.665 ||\n|| c.2. all − domain embeddings | 0.682 | 0.671 | 0.676 ||\n|| d. all − sentiment lexicon features | 0.685 | 0.673 | 0.679 ||\n|| e. all − class weights | 0.718 | 0.645 | 0.680 ||",
    "claim": "Removing general textual features results in a drop in the F-measure, while removing individual subgroups within this feature set results in only slight changes in performance.",
    "label": "support"
  },
  {
    "id": "training_211_support",
    "table_caption": "Table: Comparison of using various normalization methods in NSA.",
    "table_content": "|| Approach | B@4 | M | R | C | S ||\n|| SAN | 38.4 | 28.6 | 58.4 | 128.6 | 22.6 ||\n|| LN | 38.5 | 28.6 | 58.3 | 128.2 | 22.5 ||\n|| BN | 38.8 | 28.9 | 58.7 | 129.4 | 22.8 ||\n|| IN | [BOLD] 39.4 | [BOLD] 29.2 | [BOLD] 59.0 | 130.7 | [BOLD] 23.0 ||\n|| IN w/o  [ITALIC] γ, [ITALIC] β | 39.3 | 29.1 | 58.9 | [BOLD] 130.8 | [BOLD] 23.0 ||",
    "claim": "IN and IN w/o γ, β outperform SAN and all other normalization methods, while BN outperforms SAN but is inferior to IN.",
    "label": "support"
  },
  {
    "id": "training_347_support",
    "table_caption": "Table: WER of HF Sequence Training + Dropout",
    "table_content": "|| Non-Linearity | WER ||\n|| Sigmoid | 15.7 ||\n|| ReLU, No Dropout | 15.6 ||\n|| ReLU, Dropout Fixed for CG Iterations | [BOLD] 15.0 ||\n|| ReLU, Dropout Per CG Iteration | 15.3 ||",
    "claim": "Using a fixed dropout mask per utterance across all CG iterations achieves a lower WER (15.0) compared to varying the dropout mask per CG iteration (15.3).",
    "label": "support"
  },
  {
    "id": "training_303_support",
    "table_caption": "Table: Accuracy scores of the German–English models on the ContraWSD and MuCoW test suites.",
    "table_content": "|| Encoder heads | ContraWSD 6+1 | ContraWSD 6+6 | MuCoW 6+1 | MuCoW 6+6 ||\n|| 8L | [BOLD] 0.804 | 0.831 | [BOLD] 0.741 | 0.761 ||\n|| 7Ftoken+1L | 0.793 | [BOLD] 0.834 | 0.734 | [BOLD] 0.772  ||\n|| 7Ftoken (H8 disabled) | 0.761 | 0.816 | 0.721 | 0.757 ||",
    "claim": "The 7Ftoken+1L model achieves higher accuracy than the 8L model in the 6+6 configuration, while the 8L model performs better in the 6+1 configuration. Disabling the learnable attention head results in consistently lower performance.",
    "label": "support"
  },
  {
    "id": "training_419_support",
    "table_caption": "Table: Sentiment Polarity Classification",
    "table_content": "|| Model | Acc | F1-W ||\n|| araujo2016@sac | - | 0.71 ||\n|| BertPT | 0.77 | 0.76 ||\n|| AlbertPT | [BOLD] 0.79 | [BOLD] 0.78 ||\n|| Multilingual | 0.71 | 0.70 ||",
    "claim": "AlbertPT achieves the highest accuracy and F1 score among the models.",
    "label": "support"
  },
  {
    "id": "training_424_support",
    "table_caption": "Table: Weakly Supervised Model Results on STS-Benchmark Dataset.",
    "table_content": "|| Weakly Supervised Model | Dev | Test ||\n|| InferSent (bi-LSTM trained on SNLI)  | 80.1 | 75.8 ||\n|| Sent2vec  | 78.7 | 75.5 ||\n|| Conversation response prediction + SNLI  | 81.4 | 78.2 ||\n|| SIF on Glove vectors  | 80.1 | 72.0 ||\n|| GRAN (uses SimpWiki)  | 81.8 | 76.4 ||\n|| Unsupervised SIF + ParaNMT vectors  | 84.2 | 79.5 ||\n|| GEM  | 83.5 | 78.4 ||\n|| ROTS+ binary tree (ours) | 84.4 | 80.0 ||\n|| ROTS+ dependency tree (ours) | [BOLD] 84.6 | [BOLD] 80.6 ||",
    "claim": "ROTS+ dependency tree achieves the highest performance on both Dev and Test datasets.",
    "label": "support"
  },
  {
    "id": "training_404_support",
    "table_caption": "Table: Experiment results on Relation Extraction and Sentiment Analysis. Average and standard deviation of F1 scores (%) over multiple runs are reported (5 runs for RE and 10 runs for SA). LF(E) denotes directly applying logical forms onto explanations. Bracket behind each method illustrates corresponding data used in the method. S denotes training data without labels, E denotes explanations, R denotes surface pattern rules transformed from explanations; Sa denotes labeled data annotated with explanations, Su denotes the remaining unlabeled data. Sl denotes labeled data annotated using same time as creating explanations E, Slu denotes remaining unlabeled data corresponding to Sl.",
    "table_content": "|| [EMPTY] | Restaurant | Laptop ||\n|| LF (E) | 7.7 | 13.1 ||\n|| CBOW-GloVe (R+S) | 68.5±2.9 | 61.5±1.3 ||\n|| PCNN (S [ITALIC] a) | 72.6±1.2 | 60.9±1.1 ||\n|| ATAE-LSTM (S [ITALIC] a) | 71.1±0.4 | 56.2±3.6 ||\n|| ATAE-LSTM (S [ITALIC] l) | 71.4±0.5 | 52.0±1.4 ||\n|| Self Training (S [ITALIC] a+S [ITALIC] u) | 71.2±0.5 | 57.6±2.1 ||\n|| Pseudo Labeling (S [ITALIC] a+ [ITALIC] Su) | 70.9±0.4 | 58.0±1.9 ||\n|| Mean Teacher (S [ITALIC] a+S [ITALIC] u) | 72.0±1.5 | 62.1±2.3 ||\n|| Mean Teacher (S [ITALIC] l+S [ITALIC] lu) | 74.1±0.4 | 61.7±3.7 ||\n|| Data Programming (E+S) | 71.2±0.0 | 61.5±0.1 ||\n|| NExT (E+S) | [BOLD] 75.8±0.8 | [BOLD] 62.8±1.9 ||",
    "claim": "NExT consistently outperforms all baseline models on both Restaurant and Laptop datasets.",
    "label": "support"
  },
  {
    "id": "training_331_support",
    "table_caption": "Table: Error analysis: Core word error types for MSA",
    "table_content": "|| Error | Freq. | % | Explanation | Examples ||\n|| Wrong selection | 215 | 40.8 | Homographs with different diacritized forms | “qaSor” (قَصْر¿ – palace) vs. “qaSar” (قَصَر¿ – he limited) ||\n|| Foreign word | 124 | 23.5 | transliterated words including 96 foreign named entities | wiykiymaAnoyaA (وِيكِيمَانْيَا¿ – Wikimania) ||\n|| Invalid diacritized form | 57 | 10.8 | invalid form | ya*okur (يّذْكُر¿ – he mentions) vs. ya*okar (يّذْكَر¿) ||\n|| Named entity | 56 | 10.6 | Arabic named entities | “EabÃdiy” (عَبَّادِي¿ – name) vs. “EibAdiy” (عِبَادِي¿ – my servants) ||\n|| both correct | 48 | 9.1 | Some words have multiple valid diacritized forms | “wikAlap” (وِكَالَة¿) and “wakAlap” (وَكَالَة¿ – agency) ||\n|| Affix diacritization error | 16 | 3.0 | Some sufixes are erroneously diacritized | b [ITALIC]  [BOLD] aAkt$Afihim (بَاكتشافِهِم¿ – with their discovery) ||\n|| Reference is wrong | 10 | 1.9 | the truth diacritics were incorrect | AlofiyfaA (الْفِيفَا¿ – FIFA) vs. AlofayofaA (الْفَيْفَا¿) ||\n|| dialectal word | 1 | 0.2 | dialectal word | mawaAyiliy (مَوَايِلِي¿ – my chant) ||",
    "claim": "The most prominent error type is wrong selection at 40.8%, followed by foreign words at 23.5%, invalid diacritized forms at 10.8%, and named entities at 10.6%.",
    "label": "support"
  },
  {
    "id": "training_360_support",
    "table_caption": "Table: Comparison of zero-shot and supervised methods on MLDoc.",
    "table_content": "|| [EMPTY] | de | es | fr | it | ja | ru | zh ||\n|| [ITALIC] Zero-shot (1,000 source language examples) | [ITALIC] Zero-shot (1,000 source language examples) | [ITALIC] Zero-shot (1,000 source language examples) | [ITALIC] Zero-shot (1,000 source language examples) | [ITALIC] Zero-shot (1,000 source language examples) | [ITALIC] Zero-shot (1,000 source language examples) | [ITALIC] Zero-shot (1,000 source language examples) | [ITALIC] Zero-shot (1,000 source language examples) ||\n|| MultiCCA | 81.20 | 72.50 | 72.38 | 69.38 | 67.63 | 60.80 | 74.73 ||\n|| LASER, paper | 86.25 | [BOLD] 79.30 | 78.30 | 70.20 | 60.95 | 67.25 | 70.98 ||\n|| LASER, code | 87.65 | 75.48 | 84.00 | 71.18 | 64.58 | 66.58 | 76.65 ||\n|| MultiBERT | 82.35 | 74.98 | 83.03 | 68.27 | 64.58 | [BOLD] 71.58 | 66.17 ||\n|| MultiFiT, pseudo | [BOLD] 91.62 | 79.10 | [BOLD] 89.42 | [BOLD] 76.02 | [BOLD] 69.57 | 67.83 | [BOLD] 82.48 ||\n|| [ITALIC] Supervised (100 target language examples) | [ITALIC] Supervised (100 target language examples) | [ITALIC] Supervised (100 target language examples) | [ITALIC] Supervised (100 target language examples) | [ITALIC] Supervised (100 target language examples) | [ITALIC] Supervised (100 target language examples) | [ITALIC] Supervised (100 target language examples) | [ITALIC] Supervised (100 target language examples) ||\n|| MultiFit | 90.90 | 89.00 | 85.03 | 80.12 | 80.55 | 73.55 | 88.02 ||\n|| [ITALIC] Supervised (1,000 target language examples) | [ITALIC] Supervised (1,000 target language examples) | [ITALIC] Supervised (1,000 target language examples) | [ITALIC] Supervised (1,000 target language examples) | [ITALIC] Supervised (1,000 target language examples) | [ITALIC] Supervised (1,000 target language examples) | [ITALIC] Supervised (1,000 target language examples) | [ITALIC] Supervised (1,000 target language examples) ||\n|| MultiCCA | 93.70 | 94.45 | 92.05 | 85.55 | 85.35 | 85.65 | 87.30 ||\n|| LASER, paper | 92.70 | 88.75 | 90.80 | 85.93 | 85.15 | 84.65 | 88.98 ||\n|| MultiBERT | 94.00 | 95.15 | 93.20 | 85.82 | 87.48 | 86.85 | 90.72 ||\n|| Monolingual BERT | 94.93 | - | - | - | - | - | 92.17 ||\n|| MultiFiT, no wiki | 95.23 | 95.07 | 94.65 | 89.30 | 88.63 | 87.52 | 90.03 ||\n|| MultiFiT | [BOLD] 95.90 | [BOLD] 96.07 | [BOLD] 94.75 | [BOLD] 90.25 | [BOLD] 90.03 | [BOLD] 87.65 | [BOLD] 92.52 ||",
    "claim": "In the zero-shot setting, MultiBERT underperforms compared to other methods, and when fine-tuned with 1,000 target examples, MultiFiT outperforms all comparison methods.",
    "label": "support"
  },
  {
    "id": "training_388_support",
    "table_caption": "Table: Comparing our models with several state-of-the-art systems on the CoNLL 2003 English NER dataset.",
    "table_content": "|| Model | English ||\n|| LSTM-CRF Lample et al. ( 2016 ) | 90.94 ||\n|| LSTM-CNN-CRF Ma and Hovy ( 2016 ) | 91.21 ||\n|| LM-LSTM-CRF Liu et al. ( 2018 ) | 91.06 ||\n|| LSTM-CRF-T | 90.8 ||\n|| LSTM-CRF-TI | 91.16 ||\n|| LSTM-CRF-TI(g) | [BOLD] 91.68 ||",
    "claim": "LSTM-CRF-TI(g) achieves the highest performance on English NER tasks among the models listed.",
    "label": "support"
  },
  {
    "id": "training_266_support",
    "table_caption": "Table: Speech translation model performance in BLEU score.",
    "table_content": "|| Model | Fisher dev | Fisher dev2 | Fisher test | Callhome devtest | Callhome evltest ||\n|| End-to-end ST 3 | 46.5 | 47.3 | 47.3 | 16.4 | 16.6 ||\n|| Multi-task ST / ASR 3 | 48.3 | 49.1 | 48.7 | 16.8 | 17.4 ||\n|| ASR→NMT cascade 3 | 45.1 | 46.1 | 45.5 | 16.2 | 16.6 ||\n|| Post et al.  | – | 35.4 | – | – | 11.7 ||\n|| Kumar et al.  | – | 40.1 | 40.4 | – | – ||",
    "claim": "The end-to-end model outperforms the ASR→NMT cascade by about 1.8 BLEU points on the Fisher test set, and the multi-task configuration achieves an additional improvement of 1.4 BLEU points or more on all Fisher datasets.",
    "label": "support"
  },
  {
    "id": "training_208_support",
    "table_caption": "Table: Robustness checks on dev sets of the corresponding datasets.",
    "table_content": "|| [EMPTY] | [BOLD] SNLI | [BOLD] Quora | [BOLD] Scitail ||\n|| 1 block | 88.1±0.1 | 88.7±0.1 | 88.3±0.8 ||\n|| 2 blocks | 88.9±0.2 | 89.2±0.2 | [BOLD] 88.9±0.3 ||\n|| 3 blocks | 88.9±0.1 | 89.4±0.1 | 88.8±0.5 ||\n|| 4 blocks | [BOLD] 89.0±0.1 | [BOLD] 89.5±0.1 | 88.7±0.5 ||\n|| 5 blocks | 89.0±0.2 | 89.2±0.2 | 88.5±0.5 ||\n|| 1 enc. layer | 88.6±0.2 | 88.9±0.2 | 88.1±0.4 ||\n|| 2 enc. layers | 88.9±0.2 | 89.2±0.2 | 88.9±0.3 ||\n|| 3 enc. layers | [BOLD] 89.2±0.1 | [BOLD] 89.2±0.1 | 88.7±0.6 ||\n|| 4 enc. layers | 89.1±0.0 | 89.1±0.1 | 88.7±0.5 ||\n|| 5 enc. layers | 89.0±0.1 | 89.0±0.2 | [BOLD] 89.1±0.3 ||",
    "claim": "Adding more blocks or encoder layers generally improves performance on SNLI, Quora, and Scitail datasets, but adding more than necessary does not significantly harm performance.",
    "label": "support"
  },
  {
    "id": "training_400_support",
    "table_caption": "Table: Official entity discovery performance of our methods on KBP2016 trilingual EDL track.",
    "table_content": "|| LANG | NAME P | NAME R | NAME F1 | NOMINAL P | NOMINAL R | NOMINAL F1 | OVERALL P | OVERALL R | OVERALL F1 ||\n|| [EMPTY] | RUN1 (our official ED result in KBP2016 EDL2) | RUN1 (our official ED result in KBP2016 EDL2) | RUN1 (our official ED result in KBP2016 EDL2) | RUN1 (our official ED result in KBP2016 EDL2) | RUN1 (our official ED result in KBP2016 EDL2) | RUN1 (our official ED result in KBP2016 EDL2) | RUN1 (our official ED result in KBP2016 EDL2) | RUN1 (our official ED result in KBP2016 EDL2) | RUN1 (our official ED result in KBP2016 EDL2) ||\n|| ENG | 0.898 | 0.789 | 0.840 | 0.554 | 0.336 | 0.418 | 0.836 | 0.680 | 0.750 ||\n|| CMN | 0.848 | 0.702 | 0.768 | 0.414 | 0.258 | 0.318 | 0.789 | 0.625 | 0.698 ||\n|| SPA | 0.835 | 0.778 | 0.806 | 0.000 | 0.000 | 0.000 | 0.835 | 0.602 | 0.700 ||\n|| ALL | 0.893 | 0.759 | 0.821 | 0.541 | 0.315 | 0.398 | 0.819 | 0.639 | [BOLD] 0.718 ||\n|| [EMPTY] | RUN3 (system fusion of RUN1 with the best system in ) | RUN3 (system fusion of RUN1 with the best system in ) | RUN3 (system fusion of RUN1 with the best system in ) | RUN3 (system fusion of RUN1 with the best system in ) | RUN3 (system fusion of RUN1 with the best system in ) | RUN3 (system fusion of RUN1 with the best system in ) | RUN3 (system fusion of RUN1 with the best system in ) | RUN3 (system fusion of RUN1 with the best system in ) | RUN3 (system fusion of RUN1 with the best system in ) ||\n|| ENG | 0.857 | 0.876 | 0.866 | 0.551 | 0.373 | 0.444 | 0.804 | 0.755 | 0.779 ||\n|| CMN | 0.790 | 0.839 | 0.814 | 0.425 | 0.380 | 0.401 | 0.735 | 0.760 | 0.747 ||\n|| SPA | 0.790 | 0.877 | 0.831 | 0.000 | 0.000 | 0.000 | 0.790 | 0.678 | 0.730 ||\n|| ALL | 0.893 | 0.759 | 0.821 | 0.541 | 0.315 | 0.398 | 0.774 | [BOLD] 0.735 | [BOLD] 0.754 ||",
    "claim": "The overall trilingual F1 score improved from 0.718 in RUN1 to 0.754 in RUN3, and the recall rate increased to 0.735 after system combination.",
    "label": "support"
  },
  {
    "id": "training_353_support",
    "table_caption": "Table: Super Sense Tagging (SST) accuracy for Simple Projection, RNN and their combination.",
    "table_content": "|| [BOLD] Model Baseline | [BOLD] Model | [BOLD] Italian  [BOLD] MSC-IT-1 | [BOLD] Italian  [BOLD] MSC-IT-2 | [BOLD] French  [BOLD] MSC-FR-1 | [BOLD] French  [BOLD] MSC-FR-2 ||\n|| Baseline | [EMPTY] | [BOLD] trans man. | [BOLD] trans. auto | [BOLD] trans. auto | [BOLD] trans auto. ||\n|| Baseline | Simple Projection | 61.3 | 45.6 | 42.6 | 44.5 ||\n|| SST Based RNN | SRNN | 59.4 | 46.2 | 46.2 | 47.0 ||\n|| SST Based RNN | BRNN | 59.7 | 46.2 | 46.0 | 47.2 ||\n|| SST Based RNN | SRNN-POS-In | 61.0 | 47.0 | 46.5 | 47.3 ||\n|| SST Based RNN | SRNN-POS-H1 | 59.8 | 46.5 | 46.8 | 47.4 ||\n|| SST Based RNN | SRNN-POS-H2 | 63.1 | 48.7 | 47.7 | 49.8 ||\n|| SST Based RNN | BRNN-POS-In | 61.2 | 47.0 | 46.4 | 47.3 ||\n|| SST Based RNN | BRNN-POS-H1 | 60.1 | 46.5 | 46.8 | 47.5 ||\n|| SST Based RNN | BRNN-POS-H2 | 63.2 | 48.8 | 47.7 | 50 ||\n|| SST Based RNN | BRNN-POS-H2 - OOV | 64.6 | 49.5 | 48.4 | 50.7 ||\n|| Combination | Projection + SRNN | 62.0 | 46.7 | 46.5 | 47.4 ||\n|| Combination | Projection + BRNN | 62.2 | 46.8 | 46.4 | 47.5 ||\n|| Combination | Projection + SRNN-POS-In | 62.9 | 47.4 | 46.9 | 47.7 ||\n|| Combination | Projection + SRNN-POS-H1 | 62.5 | 47.0 | 47.1 | 48.0 ||\n|| Combination | Projection + SRNN-POS-H2 | 63.5 | 49.2 | 48.0 | 50.1 ||\n|| Combination | Projection + BRNN-POS-In | 62.9 | 47.5 | 46.9 | 47.8 ||\n|| Combination | Projection + BRNN-POS-H1 | 62.7 | 47.0 | 47.0 | 48.0 ||\n|| Combination | Projection + BRNN-POS-H2 | 63.6 | 49.3 | 48.0 | 50.3 ||\n|| Combination | Projection + BRNN-POS-H2 - OOV | [BOLD] 64.7 | 49.8 | 48.6 | 51.0 ||\n|| S-E | MFS Semeval 2013 | 60.7 | 60.7 | [BOLD] 52.4 | [BOLD] 52.4 ||\n|| S-E | GETALP  | 40.2 | 40.2 | 34.6 | 34.6 ||",
    "claim": "The best performance on Italian MSC-IT-1 is achieved by the combination of Projection + BRNN-POS-H2 - OOV with a score of 64.7.",
    "label": "support"
  },
  {
    "id": "training_376_support",
    "table_caption": "Table: Performance of various methods over all questions (question match) and all interactions (interaction match).",
    "table_content": "|| Model | Question Match | Question Match | Interaction Match | Interaction Match ||\n|| [EMPTY] | Dev | Test | Dev | Test ||\n|| CD-Seq2Seq | 13.8 | 13.9 | 2.1 | 2.6 ||\n|| SyntaxSQL-con | 15.1 | 14.1 | 2.7 | 2.2 ||",
    "claim": "Both models achieve less than 16% question-level accuracy and less than 3% interaction-level accuracy.",
    "label": "support"
  },
  {
    "id": "training_317_support",
    "table_caption": "Table: Rate (in %) and standard error of fake reviews preserving sentiment of original review.",
    "table_content": "|| LM | Amazon | Yelp ||\n|| Pretrained GPT-2 | 62.1±0.9 | 64.3±1.4 ||\n|| Fine-tuned GPT-2 | 67.0±1.4 | 67.7±1.2 ||\n|| mLSTM | 63.2±0.7 | 71.0±1.3 ||\n|| Sentiment modeling | 70.7±1.3 | 70.1±1.2 ||",
    "claim": "Sentiment modeling has the highest rate for the Amazon database, and mLSTM has the highest rate for Yelp reviews. Fine-tuned GPT-2 outperforms Pretrained GPT-2 for Yelp reviews.",
    "label": "support"
  },
  {
    "id": "training_430_support",
    "table_caption": "Table: Evaluation of different features for SentiCite (SC).",
    "table_content": "|| [BOLD] Label | [BOLD] SC-SVM | [BOLD] SC-Paum ||\n|| Only POS | 0.7241 | 0.7336 ||\n|| Combination | [BOLD] 0.7260 | [BOLD] 0.8154 ||",
    "claim": "SC-Paum achieves higher performance with the Combination feature set compared to SC-SVM.",
    "label": "support"
  },
  {
    "id": "training_203_support",
    "table_caption": "Table: Quantitative evaluation on the Twitter dataset.",
    "table_content": "|| Models | Relevance BLEU | Relevance ROUGE | Relevance Greedy | Relevance Average | Relevance Extreme | Diversity Dist-1 | Diversity Dist-2 | Diversity Ent-4 ||\n|| seq2seq | 0.64 | 0.62 | 1.669 | 0.54 | 0.34 | 0.020 | 0.084 | 6.427 ||\n|| cGAN | 0.62 | 0.61 | 1.68 | 0.536 | 0.329 | 0.028 | 0.102 | 6.631 ||\n|| AIM | [BOLD] 0.85 | [BOLD] 0.82 | [BOLD] 1.960 | [BOLD] 0.645 | [BOLD] 0.370 | 0.030 | 0.092 | 7.245 ||\n|| DAIM | 0.81 | 0.77 | 1.845 | 0.588 | 0.344 | [BOLD] 0.032 | [BOLD] 0.137 | [BOLD] 7.907 ||\n|| MMI | 0.80 | 0.75 | 1.876 | 0.591 | 0.348 | 0.028 | 0.105 | 7.156 ||",
    "claim": "AIM achieves the highest scores in relevance metrics, while DAIM achieves the highest scores in diversity metrics.",
    "label": "support"
  },
  {
    "id": "training_383_support",
    "table_caption": "Table: ASR WER results with varying amounts of training data randomly sampled from si284. Feature extractors pre-trained with just 10 hours of LibriSpeech audio are denoted with a subscript 10.",
    "table_content": "|| Features | Proportion of si284 1 | Proportion of si284 1/2 | Proportion of si284 1/4 | Proportion of si284 1/8 | Proportion of si284 1/16 | Proportion of si284 1/32 ||\n|| log Mel | 18.3 | 24.1 | 33.4 | 44.6 | 66.4 | 87.7 ||\n|| CPC | 20.7 | 28.3 | 38.8 | 50.9 | 69.7 | 88.1 ||\n|| R-APC | 15.2 | 18.3 | 24.6 | 35.8 | 49.0 | 66.8 ||\n|| T-APC | 13.7 | 16.4 | 21.3 | 31.4 | 43.0 | 63.2 ||\n|| PASE10 | 20.8 | 26.6 | 32.8 | 42.1 | 58.8 | 78.6 ||\n|| CPC10 | 23.4 | 30.0 | 40.1 | 53.5 | 71.3 | 89.3 ||\n|| R-APC10 | 17.6 | 22.7 | 28.9 | 38.6 | 55.3 | 73.7 ||\n|| T-APC10 | 18.0 | 23.8 | 31.6 | 43.4 | 61.2 | 80.4 ||",
    "claim": "R-APC and T-APC outperform log Mel across all proportions, with T-APC using half of the training data outperforming log Mel trained on the full set. Most of the time, APC outperforms CPC and PASE.",
    "label": "support"
  },
  {
    "id": "training_387_support",
    "table_caption": "Table: Results of all proposed causal models, using stratified 10-fold cross-validation. The combined system is a sieve-based architecture that applies the models in decreasing order of their precision. The combined system significantly outperforms the best single model, SVM with L1 regularization, according to a bootstrap resampling test (p = 0.022).",
    "table_content": "|| [ITALIC] Model | [ITALIC] p | [ITALIC] r | [ITALIC] f1 ||\n|| Intra-sentence | 0.5 | 0.01 | 0.01 ||\n|| Inter-sentence | 0.5 | 0.01 | 0.01 ||\n|| Reichenbach | 0 | 0 | 0 ||\n|| LR+L1 | 0.58 | 0.32 | 0.41 ||\n|| LR+L2 | 0.65 | 0.26 | 0.37 ||\n|| SVM+L1 | 0.54 | 0.35 | [BOLD] 0.43 ||\n|| SVM+L2 | 0.54 | 0.29 | 0.38 ||\n|| RF | 0.62 | 0.25 | 0.36 ||\n|| LSTM | 0.40 | 0.25 | 0.31 ||\n|| LSTM+P | 0.39 | 0.20 | 0.26 ||\n|| FLSTM | 0.43 | 0.15 | 0.22 ||\n|| FLSTM+P | 0.38 | 0.22 | 0.28 ||\n|| Combined | 0.38 | 0.58 | [BOLD] 0.46* ||",
    "claim": "The \"Combined\" model achieves the highest F1 score among the models.",
    "label": "support"
  },
  {
    "id": "training_394_support",
    "table_caption": "Table: Class distribution of the gold data Toulmin corpus approximated to the sentence level boundaries.",
    "table_content": "|| [BOLD] Class | [BOLD] Sentences in data  [BOLD] Relative (%) | [BOLD] Sentences in data  [BOLD] Absolute | [BOLD] Class | [BOLD] Sentences in data  [BOLD] Relative (%) | [BOLD] Sentences in data  [BOLD] Absolute ||\n|| Backing-B | 5.6 | 220 | Premise-I | 8.6 | 336 ||\n|| Backing-I | 7.2 | 281 | Rebuttal-B | 1.6 | 61 ||\n|| Claim-B | 4.4 | 171 | Rebuttal-I | 0.9 | 37 ||\n|| Claim-I | 0.4 | 16 | Refutation-B | 0.5 | 18 ||\n|| O | 56.8 | 2214 | Refutation-I | 0.4 | 15 ||\n|| Premise-B | 13.6 | 530 | [BOLD] Total | [EMPTY] | 3899 ||",
    "claim": "Rebuttal and refutation classes together account for only 3.4% of the data.",
    "label": "support"
  },
  {
    "id": "training_422_support",
    "table_caption": "Table: Classifying a pair of complementary scenes. All accuracies are percentage of test pairs that have been predicted correctly.",
    "table_content": "|| [EMPTY] | Training set Unbalanced | Training set Balanced ||\n|| Blind-Q+Tuple | 0 | 0 ||\n|| Q+Tuple+H-IMG | 03.20 | 23.13 ||\n|| Q+Tuple+A-IMG | [BOLD] 09.84 | [BOLD] 34.73 ||",
    "claim": "Q+Tuple+A-IMG achieves the highest performance on both the unbalanced and balanced training sets.",
    "label": "support"
  },
  {
    "id": "training_395_support",
    "table_caption": "Table: (Left) Comparison against the best prior work for NMT on the IWSLT 2014 German-English test set. (Upper Right) Comparison of inference alternatives of variational attention on IWSLT 2014. (Lower Right) Comparison of different models in terms of implied discrete entropy (lower = more certain alignment).",
    "table_content": "|| Inference Method | #Samples | PPL | BLEU ||\n|| REINFORCE | 1 | 6.17 | 33.30 ||\n|| RWS | 5 | 6.41 | 32.96 ||\n|| Gumbel-Softmax | 1 | 6.51 | 33.08 ||",
    "claim": "RWS reaches a comparable performance to REINFORCE but requires more samples, while Gumbel-Softmax achieves nearly the same performance as REINFORCE.",
    "label": "support"
  },
  {
    "id": "training_372_support",
    "table_caption": "Table: Task 1: Results of our best system (submission 1) on the test set when one of the feature groups is removed.",
    "table_content": "|| [BOLD] Submission | [ITALIC] Pclass1 | [ITALIC] Rclass1 | [ITALIC] Fclass1 ||\n|| a. submission 1 (all features) | 0.392 | 0.488 | 0.435 ||\n|| b. all − general textual features | 0.390 | 0.444 | 0.415 ||\n|| b.1. all − general  [ITALIC] n-grams | 0.397 | 0.484 | 0.436 ||\n|| b.2. all − general embeddings | 0.365 | 0.480 | 0.414 ||\n|| b.3. all − general clusters | 0.383 | 0.498 | 0.433 ||\n|| b.4. all − Twitter-specific − punctuation | 0.382 | 0.494 | 0.431 ||\n|| c. all − domain-specific features | 0.341 | 0.523 | 0.413 ||\n|| c.1. all − domain generalized  [ITALIC] n-grams | 0.366 | 0.514 | 0.427 ||\n|| c.2. all − Pronoun lexicon | 0.385 | 0.496 | 0.433 ||\n|| c.3. all − domain embeddings | 0.365 | 0.515 | 0.427 ||\n|| c.4. all − domain clusters | 0.386 | 0.492 | 0.432 ||\n|| d. all − under-sampling | 0.628 | 0.217 | 0.322 ||",
    "claim": "Removing general textual features results in a drop in recall, while removing domain-specific features impacts precision.",
    "label": "support"
  },
  {
    "id": "training_364_support",
    "table_caption": "Table: Confusion Matrix of the POS tags assigned by SpeedRead over the words of sections 22-24 of PTB. O represents all the other not mentioned tags.",
    "table_content": "|| [0pt][l]RefTest | DT | IN | JJ | NN | NNP | NNPS | NNS | RB | VBD | VBG | O ||\n|| DT | 11094 | 62 | 3 | 7 | 3 | 0 | 0 | 1 | 0 | 0 | 13 ||\n|| IN | 15 | 13329 | 9 | 1 | 0 | 0 | 0 | 88 | 0 | 0 | 50 ||\n|| JJ | 1 | 11 | 7461 | [BOLD] 257 | 130 | 2 | 10 | 65 | 38 | 81 | 159 ||\n|| NN | 1 | 5 | [BOLD] 288 | 17196 | 111 | 0 | 18 | 11 | 2 | 109 | 93 ||\n|| NNP | 8 | 13 | 118 | 109 | 12585 | 264 | 31 | 8 | 0 | 2 | 39 ||\n|| NNPS | 0 | 0 | 0 | 0 | 70 | 81 | 16 | 0 | 0 | 0 | 0 ||\n|| NNS | 0 | 0 | 1 | 23 | 20 | 42 | 7922 | 0 | 0 | 0 | 53 ||\n|| RB | 17 | [BOLD] 281 | 103 | 23 | 8 | 0 | 0 | 3892 | 0 | 1 | 80 ||\n|| VBD | 0 | 0 | 8 | 5 | 4 | 0 | 0 | 0 | 4311 | 1 | 232 ||\n|| VBG | 0 | 0 | 25 | 104 | 5 | 0 | 0 | 0 | 0 | 1799 | 0 ||\n|| O | 26 | 163 | 154 | 172 | 47 | 4 | 107 | 67 | 174 | 2 | 45707 ||",
    "claim": "Proper nouns are often incorrectly tagged as adjectives or nouns, and adverbs (RB) and prepositions (IN) are also common sources of tagging errors.",
    "label": "support"
  },
  {
    "id": "training_342_support",
    "table_caption": "Table: WER as a Function of # of Convolutional Layers",
    "table_content": "|| # of Convolutional vs. | WER ||\n|| Fully Connected Layers | [EMPTY] ||\n|| No conv, 6 full (DNN) | 24.8 ||\n|| 1 conv, 5 full | 23.5 ||\n|| 2 conv, 4 full | 22.1 ||\n|| 3 conv, 3 full | 22.4 ||",
    "claim": "Increasing the number of convolutional layers up to 2 reduces the WER, but further increases lead to higher WER. CNNs offer improvements over DNNs.",
    "label": "support"
  },
  {
    "id": "training_239_support",
    "table_caption": "Table: DSTC2 and WOZ 2.0 test set performance (joint goals and requests) of the NBT-CNN model making use of three different word vector collections. The asterisk indicates statistically significant improvement over the baseline xavier (random) word vectors (paired t-test; p<0.05).",
    "table_content": "|| [BOLD] Word Vectors | [BOLD] DSTC2  [BOLD] Goals | [BOLD] DSTC2  [BOLD] Requests | [BOLD] WOZ 2.0  [BOLD] Goals | [BOLD] WOZ 2.0  [BOLD] Requests ||\n|| xavier [BOLD]  (No Info.) | 64.2 | 81.2 | 81.2 | 90.7 ||\n|| [BOLD] GloVe | 69.0* | 96.4* | 80.1 | 91.4 ||\n|| [BOLD] Paragram-SL999 | [BOLD] 73.4* | [BOLD] 96.5* | [BOLD] 84.2* | [BOLD] 91.6 ||",
    "claim": "Paragram-SL999 vectors outperform GloVe and xavier vectors for goal tracking on both DSTC2 and WOZ 2.0 datasets.",
    "label": "support"
  },
  {
    "id": "training_434_support",
    "table_caption": "Table: Performance improvement of the two-stage APR-based UV.",
    "table_content": "|| Test Set | APR | APR2-stage | Δ ||\n|| BNS-1 | 0.9675 | 0.9677 | +0.0002 ||\n|| BNS-2 | 0.9592 | 0.9598 | +0.0006 ||",
    "claim": "APR2-stage shows a slight improvement over APR on both BNS-1 and BNS-2 test sets.",
    "label": "support"
  },
  {
    "id": "training_398_support",
    "table_caption": "Table: (Left) Comparison against the best prior work for NMT on the IWSLT 2014 German-English test set. (Upper Right) Comparison of inference alternatives of variational attention on IWSLT 2014. (Lower Right) Comparison of different models in terms of implied discrete entropy (lower = more certain alignment).",
    "table_content": "|| [EMPTY] | IWSLT ||\n|| Model | BLEU ||\n|| Beam Search Optimization  Wiseman2016  | 26.36 ||\n|| Actor-Critic  Bahdanau2017  | 28.53 ||\n|| Neural PBMT + LM  Huang2018  | 30.08 ||\n|| Minimum Risk Training  Edunov2017  | 32.84 ||\n|| Soft Attention | 32.77 ||\n|| Marginal Likelihood | 33.29 ||\n|| Hard Attention + Enum | 31.40 ||\n|| Hard Attention + Sample | 30.42 ||\n|| Variational Relaxed Attention | 30.05 ||\n|| Variational Attention + Enum | 33.69 ||\n|| Variational Attention + Sample | 33.30 ||",
    "claim": "Variational attention with enumeration achieves a BLEU score of 33.69, which is comparable to the marginal likelihood model's score of 33.29 on the IWSLT 2014 German-English task.",
    "label": "support"
  },
  {
    "id": "training_451_support",
    "table_caption": "Table: Fisher speaker recognition task results",
    "table_content": "|| [BOLD] System | [BOLD] EER | [BOLD] minDCF08 | [BOLD] minDCF10 ||\n|| i-vector  | 2.10 | 0.093 | 0.3347 ||\n|| x-vector + stat. pooling  | 1.73 | 0.086 | 0.3627 ||\n|| phn. vec. + finetune  | 1.60 | 0.076 | 0.3413 ||\n|| + multi-tasking  | 1.39 | 0.073 | 0.3087 ||\n|| x-vector + SAP | 1.50 | 0.074 | 0.2973 ||\n|| pretrain + CNN + SAP | [BOLD] 1.07 | [BOLD] 0.052 | [BOLD] 0.2247 ||",
    "claim": "The pretrain + CNN + SAP system achieves the lowest EER, minDCF08, and minDCF10 values among the systems listed.",
    "label": "support"
  },
  {
    "id": "training_436_support",
    "table_caption": "Table: Comparison between the proposed APR-based UV and the conventional LRT-based UV with the optimized thresholds.",
    "table_content": "|| Test Set | LRT ACC | LRT  [ITALIC] τ | APR ACC | APR  [ITALIC] θ | APR Δ ||\n|| DICT01 | 0.992 | 1.5 | 0.998 | 4.0 | +0.006 (0.6%) ||\n|| BNS-1 | 0.930 | 1.2 | 0.968 | 5.0 | +0.038 (4.1%) ||\n|| BNS-2 | 0.901 | 1.1 | 0.959 | 6.0 | +0.058 (6.4%) ||",
    "claim": "APR-based UV achieves higher accuracy than LRT-based UV across all test sets.",
    "label": "support"
  },
  {
    "id": "training_402_support",
    "table_caption": "Table: Entity discovery performance (English only) in KBP2016 EDL1 evaluation window is shown as a comparison of three models trained by different combinations of training data sets.",
    "table_content": "|| training data | P | R | [ITALIC] F1 ||\n|| KBP2015 | 0.818 | 0.600 | 0.693 ||\n|| KBP2015 + WIKI | 0.859 | 0.601 | 0.707 ||\n|| KBP2015 + iFLYTEK | 0.830 | 0.652 | [BOLD] 0.731 ||",
    "claim": "The highest F1 score of 0.731 is achieved using the KBP2015 + iFLYTEK training data.",
    "label": "support"
  },
  {
    "id": "training_365_support",
    "table_caption": "Table: F1 scores of the chunking phase using different POS tags. F1 score is calculated over tokens and not entities.",
    "table_content": "|| [100pt][l]PhaseDataset | Train | Dev | Test ||\n|| SR+SR POS | 94.24 | 94.49 | [BOLD] 93.12 ||\n|| SR+Stanford POS L3W | 92.98 | 93.37 | 92.05 ||\n|| SR+CONLL POS | 90.88 | 90.82 | 89.43 ||\n|| SR+SENNA POS | 94.73 | 95.07 | [BOLD] 93.80 ||",
    "claim": "SR+SENNA POS achieves the highest score in the Test phase.",
    "label": "support"
  },
  {
    "id": "training_410_support",
    "table_caption": "Table: BERT experiments on Restaurant dataset using 45 and 75 explanations",
    "table_content": "|| [EMPTY] | 45 | 75 ||\n|| ATAE-LSTM (S [ITALIC] a) | 79.9 | 80.6 ||\n|| Self Training (S [ITALIC] a+S [ITALIC] u) | 80.9 | 81.1 ||\n|| Pseudo Labeling (S [ITALIC] a+S [ITALIC] u) | 78.7 | 81.0 ||\n|| Mean Teacher (S [ITALIC] a+S [ITALIC] u) | 79.3 | 79.8 ||\n|| NExT (E+S) | [BOLD] 81.4 | [BOLD] 82.0 ||",
    "claim": "NExT achieves the highest performance scores at both 45 and 75 explanations.",
    "label": "support"
  },
  {
    "id": "training_413_support",
    "table_caption": "Table: Document classification results, reporting number of parameters and accuracy.",
    "table_content": "|| Representation | Parameters | Accuracy ||\n|| Recurrent | 509.6 K | [BOLD] 87.4 ||\n|| Convolutional | 2.5 M | 85.7 ||\n|| + non-linearity | 1.3 M | 87.3 ||\n|| + separability | 234.4 K | 86.7 ||\n|| + bottlenecks | [BOLD] 92.5 K | 86.4 ||",
    "claim": "The optimized representation uses 5x and 27x fewer parameters than the recurrent and convolutional baselines, respectively, with an accuracy drop of only 1% compared to the recurrent baseline.",
    "label": "support"
  },
  {
    "id": "training_380_support",
    "table_caption": "Table: ROUGE-1, ROUGE-2, ROUGE-SU4 scores of generated summary in AMI and ICSI datasets. Numbers in bold are the overall best result. Numbers with underscore are the best result from previous literature. ∗ The two baseline MM models require additional human annotations of topic segmentation and visual signals from cameras. ∗∗ Results are statistically significant at level 0.05.",
    "table_content": "|| Model | AMI ROUGE-1 | AMI R-2 | AMI R-SU4 | ICSI ROUGE-1 | ICSI R-2 | ICSI R-SU4 ||\n|| Random | 35.13 | 6.26 | 13.17 | 29.28 | 3.78 | 10.29 ||\n|| Longest Greedy | 33.35 | 5.11 | 12.15 | 30.23 | 4.27 | 10.90 ||\n|| Template | 31.50 | 6.80 | 11.40 | / | / | / ||\n|| CoreRank Submodular | 36.13 | 7.33 | 14.18 | 29.82 | 4.00 | 10.61 ||\n|| PageRank Submodular | 36.1 | 7.42 | 14.32 | 30.4 | 4.42 | 11.14 ||\n|| TextRank | 35.25 | 6.9 | 13.62 | 29.7 | 4.09 | 10.64 ||\n|| ClusterRank | 35.14 | 6.46 | 13.35 | 27.64 | 3.68 | 9.77 ||\n|| UNS | 37.86 | 7.84 | 14.71 | 31.60 | 4.83 | 11.35 ||\n|| Extractive Oracle | 39.49 | 9.65 | 13.20 | 34.66 | 8.00 | 10.49 ||\n|| PGNet | 40.77 | 14.87 | 18.68 | 32.00 | 7.70 | 12.46 ||\n|| MM (TopicSeg+VFOA)∗ | [BOLD] 53.29 | 13.51 | / | / | / | / ||\n|| MM (TopicSeg)∗ | 51.53 | 12.23 | / | / | / | / ||\n|| HMNet (ours) | 52.09 | [BOLD] 19.69∗∗ | [BOLD] 24.11∗∗ | [BOLD] 39.51∗∗ | [BOLD] 10.76∗∗ | [BOLD] 17.90∗∗ ||",
    "claim": "HMNet outperforms all baseline models in all metrics except for ROUGE-1 in AMI, achieving 7.51, 3.06, and 5.44 higher ROUGE points on the ICSI dataset compared to previous best results.",
    "label": "support"
  },
  {
    "id": "training_274_support",
    "table_caption": "Table: Comparisons (CER, WER, and training time) of the WSJ task with other end-to-end ASR systems.",
    "table_content": "|| Method | Metric | dev93 | eval92 ||\n|| ESPnet with VGG2-BLSTM | CER | 10.1 | 7.6 ||\n|| + BLSTM layers (4 → 6) | CER | 8.5 | 5.9 ||\n|| + char-LSTMLM | CER | 8.3 | 5.2 ||\n|| + joint decoding | CER | 5.5 | 3.8 ||\n|| + label smoothing | CER | 5.3 | 3.6 ||\n|| [EMPTY] | WER | 12.4 | 8.9 ||\n|| seq2seq + CNN (no LM)  | WER | [EMPTY] | 10.5 ||\n|| seq2seq + FST word LM  | CER | [EMPTY] | 3.9 ||\n|| [EMPTY] | WER | [EMPTY] | 9.3 ||\n|| CTC + FST word LM  | WER | [EMPTY] | 7.3 ||",
    "claim": "The use of a deeper encoder network, integration of character-based LSTMLM, and joint CTC/attention decoding steadily improved the CER from 10.1 to 5.3 on dev93 and from 7.6 to 3.6 on eval92.",
    "label": "support"
  },
  {
    "id": "training_392_support",
    "table_caption": "Table: Results of classification of argument components in the cross-domain scenario. Macro-F1 scores reported, bold numbers denote the best results. HS – homeschooling, MS – mainstreaming, PIS – prayer in schools, PPS – private vs. public schools, RS – redshirting, SSE – single sex education. Results in the aggregated row are computed from an aggregated confusion matrix over all domains. The differences between the best feature set combination (4) and others are statistically significant (p<0.001; paired exact Liddell’s test).",
    "table_content": "|| Domain | [BOLD] Feature set combinations 0 | [BOLD] Feature set combinations 01 | [BOLD] Feature set combinations 012 | [BOLD] Feature set combinations 0123 | [BOLD] Feature set combinations 01234 | [BOLD] Feature set combinations 1234 | [BOLD] Feature set combinations 234 | [BOLD] Feature set combinations 34 | [BOLD] Feature set combinations 4 ||\n|| [BOLD] HS | 0.087 | 0.063 | 0.044 | 0.106 | 0.072 | 0.075 | 0.065 | 0.063 | [BOLD] 0.197 ||\n|| [BOLD] MS | 0.072 | 0.060 | 0.070 | 0.058 | 0.038 | 0.062 | 0.045 | 0.060 | [BOLD] 0.188 ||\n|| [BOLD] PIS | 0.078 | 0.073 | 0.083 | 0.074 | 0.086 | 0.073 | 0.096 | 0.081 | [BOLD] 0.166 ||\n|| [BOLD] PPS | 0.070 | 0.059 | 0.070 | 0.132 | 0.059 | 0.062 | 0.071 | 0.067 | [BOLD] 0.203 ||\n|| [BOLD] RS | 0.067 | 0.067 | 0.082 | 0.110 | 0.097 | 0.092 | 0.075 | 0.075 | [BOLD] 0.257 ||\n|| [BOLD] SSE | 0.092 | 0.089 | 0.066 | 0.036 | 0.120 | 0.091 | 0.071 | 0.066 | [BOLD] 0.194 ||\n|| [BOLD] Aggregated | 0.079 | 0.086 | 0.072 | 0.122 | 0.094 | 0.088 | 0.089 | 0.076 | [BOLD] 0.209 ||",
    "claim": "Using only feature set 4 results in the highest performance across all domains.",
    "label": "support"
  },
  {
    "id": "training_412_support",
    "table_caption": "Table: Summarization results when using the full training set. Our scores are averaged over three models trained with different random seeds. *Other abstractive summarization model scores are provided to contextualize performance on this task but are not directly comparable to our models.",
    "table_content": "|| [BOLD] Model | [BOLD] R1 | [BOLD] R2 | [BOLD] RL ||\n|| Other Abs. Sum. models* | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| Celikyilmaz et al. ( 2018 ) | 41.69 | 19.47 | 37.92 ||\n|| CopyTransformer (4-layer) | 39.25 | 17.54 | 36.45 ||\n|| Gehrmann et al. ( 2018 ) | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| GPT-2 (48-layer, zero-shot) | 29.34 | 08.27 | 26.58 ||\n|| Radford et al. ( 2019 ) | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| No Pre-training | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| BidirEncoder-Decoder (4-layer) | 37.74 | 16.27 | 34.76 ||\n|| Encoder-Decoder (12-layer) | 36.72 | 15.22 | 33.84 ||\n|| Transformer LM (12-layer) | 37.72 | 16.14 | 34.62 ||\n|| With Pre-training (all 12-layer) | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| Pre-train Encoder only | 36.05 | 15.48 | 33.48 ||\n|| Pre-train Decoder only | 27.48 | 06.87 | 25.40 ||\n|| Encoder-Decoder | 39.18 | 17.00 | 36.33 ||\n|| Transformer LM | 39.65 | 17.74 | 36.85 ||",
    "claim": "Pre-training improves performance by about 2 ROUGE points on average, and the Transformer LM outperforms corresponding models both with and without pre-training.",
    "label": "support"
  },
  {
    "id": "training_474_support",
    "table_caption": "Table: Validation and test BLEU for loss combination strategies. We either use token-level TokLS and sequence-level Riskindividually or combine them as a weighted combination, a constrained combination, a random choice for each sample, cf. §3.3.",
    "table_content": "|| [EMPTY] | [BOLD] valid | [BOLD] test ||\n|| TokLS | 33.11 | 32.21 ||\n|| Risk only | 33.55 | 32.45 ||\n|| Weighted | 33.91 | 32.85 ||\n|| Constrained | 33.77 | 32.79 ||\n|| Random | 33.70 | 32.61 ||",
    "claim": "The random strategy underperforms compared to the constrained strategy on both valid and test metrics.",
    "label": "support"
  },
  {
    "id": "training_425_support",
    "table_caption": "Table: Detailed Comparisons with Similar Unsupervised Approaches on 20 STS Datasets",
    "table_content": "|| Model Type | Senrence Similarity | STS12 | STS13 | STS14 | STS15 | AVE ||\n|| OT based | WMDDBLP:conf/icml/KusnerSKW15 | 60.6 | 54.5 | 65.5 | 61.8 | 60.6 ||\n|| OT based | WMEDBLP:conf/emnlp/WuYXXBCRW18 | 62.8 | 65.3 | 68 | 64.2 | 65.1 ||\n|| OT based | CoMBDBLP:conf/iclr/SinghHDJ19 | 57.9 | 64.2 | 70.3 | 73.1 | 66.4 ||\n|| Weighted average | SIFDBLP:conf/iclr/AroraLM17 | 59.5 | 61.8 | 73.5 | 76.3 | 67.8 ||\n|| Weighted average | uSIFDBLP:conf/rep4nlp/EthayarajhH18 | 65.8 | 66.1 | 78.4 | 79.0 | 72.3 ||\n|| Weighted average | DynaMaxDBLP:conf/iclr/ZhelezniakSSMFH19 | 66.0 | 65.7 | 75.9 | [BOLD] 80.1 | 72.0 ||\n|| Ours | ROTS + binary tree | [BOLD] 68.3 | 66.0 | [BOLD] 78.7 | 79.5 | 73.1 ||\n|| Ours | ROTS + dependency tree | 67.5 | [BOLD] 66.4 | [BOLD] 78.7 | 80.0 | [BOLD] 73.2 ||",
    "claim": "ROTS + dependency tree has the best average score among the models.",
    "label": "support"
  },
  {
    "id": "training_420_support",
    "table_caption": "Table: Emotion Classification",
    "table_content": "|| Model | 6 classes Acc | 6 classes F1-W | Binary Acc | Binary F1-W ||\n|| Becker:2017:MEC:3063600.3063706 | - | - | - | [BOLD] 0.84 ||\n|| BertPT | [BOLD] 0.51 | [BOLD] 0.47 | [BOLD] 0.84 | 0.83 ||\n|| AlbertPT | 0.41 | 0.28 | [BOLD] 0.84 | 0.81 ||\n|| Multilingual | 0.49 | 0.46 | [BOLD] 0.84 | 0.80 ||",
    "claim": "BertPT achieves a binary F1-W score similar to the baseline of 0.84 but does not surpass it.",
    "label": "support"
  },
  {
    "id": "training_448_support",
    "table_caption": "Table: F0.5 results on the CoNLL-13 and CoNLL-14 test sets of main model architectures.",
    "table_content": "|| [BOLD] Model | [BOLD] Performance Dev | [BOLD] Performance Test ||\n|| Word NMT + UNK replacement | 26.17 | 38.77 ||\n|| Hybrid model | 28.49 | 40.44 ||\n|| Nested Attention Hybrid Model | [BOLD] 28.61 | [BOLD] 41.53 ||",
    "claim": "The Nested Attention Hybrid Model achieves the highest performance on both the development and test sets.",
    "label": "support"
  },
  {
    "id": "training_460_support",
    "table_caption": "Table: Precision, Recall, and F1 on Chinese GALE test data. BPE indicates “with BPE”, and NER denotes restriction to NER spans.",
    "table_content": "|| Method | P | R | F1 ||\n|| Avg. attention. | 36.30 | 46.17 | 40.65 ||\n|| Avg. attention (BPE) | 37.89 | 49.82 | 43.05 ||\n|| Avg. attention (NER) | 16.57 | 35.85 | 22.66 ||\n|| FastAlign | 80.46 | 50.46 | 62.02 ||\n|| FastAlign (BPE) | 70.41 | 55.43 | 62.03 ||\n|| FastAlign (NER) | 83.70 | 49.54 | 62.24 ||\n|| DiscAlign | 72.92 | 73.91 | [BOLD] 73.41 ||\n|| DiscAlign (BPE) | 69.36 | 67.11 | [BOLD] 68.22 ||\n|| DiscAlign (NER) | 74.52 | 77.05 | [BOLD] 75.78 ||\n|| DiscAlign (NER) +prec. | 84.69 | 58.41 | 69.14 ||",
    "claim": "DiscAlign (NER) achieves the highest F1 score among the methods listed in the table.",
    "label": "support"
  },
  {
    "id": "training_463_support",
    "table_caption": "Table: Sentences per minute and average scores against gold-labelled data for sentences annotated for alignment by human annotators (Hu.), compared to DiscAlign (DA) on the same sentences.",
    "table_content": "|| Model | sents/m | P | R | F1 ||\n|| Human | 4.4 | 90.09 | 62.85 | 73.92 ||\n|| DiscAlign | - | 74.54 | 72.15 | 73.31 ||\n|| Hu. (NER) | - | 87.73 | 71.02 | 78.24 ||\n|| DA (NER) | - | 77.37 | 67.69 | 71.94 ||",
    "claim": "Human annotators outperform DiscAlign in terms of F1 score when evaluated on the same sentences.",
    "label": "support"
  },
  {
    "id": "training_345_support",
    "table_caption": "Table: Pooling in Time",
    "table_content": "|| Method | WER ||\n|| Baseline | 18.9 ||\n|| Pooling in Time, Max | 18.9 ||\n|| Pooling in Time, Stochastic | 18.8 ||\n|| Pooling in Time,  [ITALIC] lp | 18.8 ||",
    "claim": "Pooling in time with stochastic and lp methods achieves a WER of 18.8, slightly lower than the baseline WER of 18.9.",
    "label": "support"
  },
  {
    "id": "training_437_support",
    "table_caption": "Table: Performance degradation in the exaggerated voice-overs, when applying the optimized thresholds of the read-speech utterances.",
    "table_content": "|| Test Set | LRT ACC | LRT Δ | APR ACC | APR Δ ||\n|| BNS-1 | 0.813 | -0.117 | 0.952 | -0.016 ||\n|| BNS-1 | 0.813 | (-14.4%) | 0.952 | (-1.7%) ||\n|| BNS-2 | 0.674 | -0.228 | 0.900 | -0.059 ||\n|| BNS-2 | 0.674 | (-33.8%) | 0.900 | (-6.6%) ||",
    "claim": "The changes in accuracy for APR-based UV are lower than those for LRT-based UV across both test sets.",
    "label": "support"
  },
  {
    "id": "training_476_support",
    "table_caption": "Table: Generating candidates online or offline.",
    "table_content": "|| [EMPTY] | [BOLD] valid | [BOLD] test ||\n|| Online generation | 33.91 | 32.85 ||\n|| Offline generation | 33.52 | 32.44 ||",
    "claim": "Online generation results in higher accuracy than offline generation on both valid and test datasets.",
    "label": "support"
  },
  {
    "id": "training_440_support",
    "table_caption": "Table: (b) Negative PageRank",
    "table_content": "|| postag | P@50 | P@100 | P@500 | P@1000 ||\n|| [BOLD] i | [BOLD] 0.980 | [BOLD] 0.960 | [BOLD] 0.808 | [BOLD] 0.649 ||\n|| a | 0.260 | 0.200 | 0.240 | 0.231 ||\n|| v | 0.020 | 0.040 | 0.032 | 0.048 ||",
    "claim": "Idioms achieve the best result in the negative PageRank.",
    "label": "support"
  },
  {
    "id": "training_455_support",
    "table_caption": "Table: Effects of varying the fraction of tweets used for training and testing on classification accuracy in the state-prediction task, using All Words and LDA topics.",
    "table_content": "|| training fraction | testing fraction 0.2 | testing fraction 0.4 | testing fraction 0.6 | testing fraction 0.8 | testing fraction 1.0 ||\n|| 0.2 | 11.76 | 11.76 | 5.88 | 9.80 | 15.68 ||\n|| 0.4 | 19.60 | 17.64 | 17.64 | 17.64 | 25.49 ||\n|| 0.6 | 25.49 | 29.41 | 35.29 | 41.17 | 47.05 ||\n|| 0.8 | 39.21 | 41.17 | 43.13 | 50.98 | 52.94 ||\n|| 1.0 | 43.13 | 58.82 | 54.90 | 62.74 | 64.70 ||",
    "claim": "Increasing the number of tweets in the training set has a larger positive effect on accuracy than increasing the number of tweets in the testing set.",
    "label": "support"
  },
  {
    "id": "training_453_support",
    "table_caption": "Table: Same-vs-Different classification. P, R, and F1 are calculated with respect to Same.",
    "table_content": "|| Classifier | P | R | F1 | Acc ||\n|| baseline:  [ITALIC] Same | [EMPTY] | [EMPTY] | [EMPTY] | 69.26 ||\n|| MaxEnt-2C | 73.95 | 90.99 | 81.59 | 71.56 ||\n|| MaxEnt-3C | 77.15 | 80.42 | 78.75 | 69.94 ||",
    "claim": "MaxEnt-2C has higher F1 and accuracy than MaxEnt-3C.",
    "label": "support"
  },
  {
    "id": "training_433_support",
    "table_caption": "Table: Different nature of citation classes.",
    "table_content": "|| [BOLD] Label | [BOLD] SC-SVM | [BOLD] SC-Paum | [BOLD] #refs ||\n|| Usage | 0.5294 | [BOLD] 0.8325 | 17 ||\n|| Reference | [BOLD] 1.0 | 0.9455 | 110 ||\n|| Reading | 0.7143 | [BOLD] 0.8571 | 7 ||\n|| Rest | 0.4533 | [BOLD] 0.4602 | 112 ||\n|| Dataset | [BOLD] 0.667 | [BOLD] 0.667 | 15 ||\n|| Overall | 0.7075 | [BOLD] 0.7099 | 261 ||",
    "claim": "The classification performance for the \"Dataset\" label is 0.667 for both classifiers, and the \"Reference\" label achieves a score of 1.0 with the SC-SVM classifier.",
    "label": "support"
  },
  {
    "id": "training_477_support",
    "table_caption": "Table: Accuracy on Gigaword abstractive summarization in terms of F-measure Rouge-1 (RG-1), Rouge-2 (RG-2), and Rouge-L (RG-L) for token-level label smoothing, and Risk optimization of all three ROUGE F1 metrics. [T] indicates a token-level objective and [S] indicates a sequence level objectives. ABS+ refers to Rush et al. (2015), RNN MLE/MRT (Ayana et al., 2016), WFE (Suzuki and Nagata, 2017), SEASS (Zhou et al., 2017), DRGD (Li et al., 2017).",
    "table_content": "|| [EMPTY] | [BOLD] RG-1 | [BOLD] RG-2 | [BOLD] RG-L ||\n|| ABS+ [T] | 29.78 | 11.89 | 26.97 ||\n|| RNN MLE [T] | 32.67 | 15.23 | 30.56 ||\n|| RNN MRT [S] | 36.54 | 16.59 | 33.44 ||\n|| WFE [T] | 36.30 | 17.31 | 33.88 ||\n|| SEASS [T] | 36.15 | 17.54 | 33.63 ||\n|| DRGD [T] | 36.27 | 17.57 | 33.62 ||\n|| TokLS | 36.53 | 18.10 | 33.93 ||\n|| + Risk RG-1 | 36.96 | 17.61 | 34.18 ||\n|| + Risk RG-2 | 36.65 | 18.32 | 34.07 ||\n|| + Risk RG-L | 36.70 | 17.88 | 34.29 ||",
    "claim": "Risk optimization can improve the ROUGE metrics compared to the baseline, as shown by higher scores in RG-1, RG-2, and RG-L with Risk.",
    "label": "support"
  },
  {
    "id": "training_472_support",
    "table_caption": "Table: Comparison to Beam Search Optimization. We report the best likelihood (MLE) and BSO results from Wiseman and Rush (2016), as well as results from our MLE reimplementation and training with Risk. Results based on unnormalized beam search (k=5).",
    "table_content": "|| [EMPTY] | [BOLD] BLEU | Δ ||\n|| MLE | 24.03 | [EMPTY] ||\n|| + BSO | 26.36 | +2.33 ||\n|| MLE Reimplementation | 23.93 | [EMPTY] ||\n|| + Risk | 26.68 | +2.75 ||",
    "claim": "Risk improves BLEU by +2.75 compared to the baseline, which is better than the +2.33 improvement from Beam Search Optimization.",
    "label": "support"
  },
  {
    "id": "training_446_support",
    "table_caption": "Table: F0.5 results on the CoNLL-13 set of main model architectures, on different segments of the set according to whether the input contains OOVs.",
    "table_content": "|| [BOLD] Model | [BOLD] NonOOV | [BOLD] OOV | [BOLD] Overall ||\n|| Word NMT + UNK replacement | 27.61 | 21.57 | 26.17 ||\n|| Hybrid model | [BOLD] 29.36 | 25.92 | 28.49 ||\n|| Nested Attention Hybrid Model | 29.00 | [BOLD] 27.39 | [BOLD] 28.61 ||",
    "claim": "The Nested Attention Hybrid Model achieves the highest scores in the OOV and overall categories, while the Hybrid model performs best in the NonOOV category.",
    "label": "support"
  },
  {
    "id": "training_396_support",
    "table_caption": "Table: Evaluation on NMT and VQA for the various models. E column indicates whether the expectation is calculated via enumeration (Enum) or a single sample (Sample) during training. For NMT we evaluate intrinsically on perplexity (PPL) (lower is better) and extrinsically on BLEU (higher is better), where for BLEU we perform beam search with beam size 10 and length penalty (see Appendix B for further details). For VQA we evaluate intrinsically on negative log-likelihood (NLL) (lower is better) and extrinsically on VQA evaluation metric (higher is better). All results except for relaxed attention use enumeration at test time.",
    "table_content": "|| Model | Objective | E | NMT PPL | NMT BLEU | VQA NLL | VQA Eval ||\n|| Soft Attention | log [ITALIC] p( [ITALIC] y|E[ [ITALIC] z]) | - | 7.17 | 32.77 | 1.76 | 58.93 ||\n|| Marginal Likelihood | logE[ [ITALIC] p] | Enum | 6.34 | 33.29 | 1.69 | 60.33 ||\n|| Hard Attention | E [ITALIC] p[log [ITALIC] p] | Enum | 7.37 | 31.40 | 1.78 | 57.60 ||\n|| Hard Attention | E [ITALIC] p[log [ITALIC] p] | Sample | 7.38 | 31.00 | 1.82 | 56.30 ||\n|| Variational Relaxed Attention | E [ITALIC] q[log [ITALIC] p]−KL | Sample | 7.58 | 30.05 | - | - ||\n|| Variational Attention | E [ITALIC] q[log [ITALIC] p]−KL | Enum | 6.08 | 33.68 | 1.69 | 58.44 ||\n|| Variational Attention | E [ITALIC] q[log [ITALIC] p]−KL | Sample | 6.17 | 33.30 | 1.75 | 57.52 ||",
    "claim": "Hard attention underperforms soft attention, and exact marginal likelihood outperforms soft attention on both NMT and VQA tasks.",
    "label": "support"
  },
  {
    "id": "training_484_support",
    "table_caption": "Table: Results on three word similarity datasets.",
    "table_content": "|| [BOLD] RG65  [BOLD] Orig. | [BOLD] RG65  [BOLD] with FRAGE | [BOLD] WS  [BOLD] Orig. | [BOLD] WS  [BOLD] with FRAGE | [BOLD] RW  [BOLD] Orig. | [BOLD] RW  [BOLD] with FRAGE ||\n|| 75.63 | [BOLD] 78.78 | 66.74 | [BOLD] 69.35 | 52.67 | [BOLD] 58.12 ||",
    "claim": "The method with FRAGE outperforms the original method on all datasets, with a 5.4-point improvement on the RW dataset.",
    "label": "support"
  },
  {
    "id": "training_452_support",
    "table_caption": "Table: CER (%) on HKUST datasets compared to previous works.",
    "table_content": "|| model | CER ||\n|| LSTMP-9×800P512-F444  | 30.79 ||\n|| CTC-attention+joint dec. (speed perturb., one-pass) +VGG net +RNN-LM (separate)  | 28.9  [BOLD] 28.0 ||\n|| CI-phonemes-D1024-H16  | 30.65 ||\n|| Syllables-D1024-H16 (speed perturb)  | 28.77 ||\n|| Words-D1024-H16 (speed perturb) | 27.42 ||\n|| Sub-words-D1024-H16 (speed perturb) | 27.26 ||\n|| Characters-D1024-H16 (speed perturb) | [BOLD] 26.64 ||",
    "claim": "The character-based model with the Transformer achieves the best CER of 26.64%, compared to the LSTMP-9×800P512-F444 model's CER of 30.79%.",
    "label": "support"
  },
  {
    "id": "training_501_support",
    "table_caption": "Table: Accuracy on the scene data set. KK2013 results are from Krishnamurthy and Kollar (2013).",
    "table_content": "|| Model | Supervision QA | Supervision QA+E | Supervision QA+E+LF ||\n|| [ITALIC] P3 | 68 | 75 | – ||\n|| KK2013 | 67 | – | 70 ||",
    "claim": "P3 slightly outperforms KK2013 in the QA condition, and P3 with QA+E supervision outperforms KK2013 with QA+E+LF supervision.",
    "label": "support"
  },
  {
    "id": "training_461_support",
    "table_caption": "Table: Precision, Recall, and F1 on Arabic GALE test data.",
    "table_content": "|| Method | P | R | F1 ||\n|| Avg. attention. | 8.46 | 32.50 | 13.42 ||\n|| Avg. attention (BPE) | 10.11 | 17.27 | 12.75 ||\n|| FastAlign | 62.26 | 51.06 | 56.11 ||\n|| FastAlign (BPE) | 62.74 | 51.25 | 56.42 ||\n|| DiscAlign | 91.30 | 75.66 | [BOLD] 82.74 ||\n|| DiscAlign (BPE) | 87.05 | 76.98 | [BOLD] 81.71 ||",
    "claim": "DiscAlign achieves the highest F1 score among the methods.",
    "label": "support"
  },
  {
    "id": "training_481_support",
    "table_caption": "Table: Comparisons between Different Input Features with Different Routing Iterations",
    "table_content": "|| r | Word Only P | Word Only R | Word Only F{}_{1} | Word + Entity Type P | Word + Entity Type R | Word + Entity Type F{}_{1} ||\n|| 1 | 94.53 | 95.80 | 95.16 | 94.97 | 96.25 | 95.61 ||\n|| 2 | [BOLD] 95.14 | 96.85 | [BOLD] 95.99 | 94.74 | 97.30 | 96.01 ||\n|| 3 | 94.69 | 96.25 | 95.46 | 95.15 | 97.15 | 96.14 ||\n|| 4 | 94.95 | 95.80 | 95.37 | [BOLD] 95.59 | [BOLD] 97.45 | [BOLD] 96.51 ||\n|| 5 | 93.25 | [BOLD] 97.30 | 95.23 | 95.43 | 97.00 | 96.21 ||",
    "claim": "When words are only used, the model achieves its best performance with r=2. When words and entity types are both used, the model achieves its best performance with r=4.",
    "label": "support"
  },
  {
    "id": "training_482_support",
    "table_caption": "Table: Comparison of the Interest of Uni/Bi-LSTMs and Capsules",
    "table_content": "|| [EMPTY] | P | R | F{}_{1} ||\n|| All Bi-LSTMs | 94.76 | [BOLD] 97.60 | 96.16 ||\n|| Softmax layer | 95.16 | 97.30 | 96.22 ||\n|| [BOLD] Uni/Bi-LSTMs & Capsule layer | [BOLD] 95.59 | 97.45 | [BOLD] 96.51 ||",
    "claim": "The F1-score of the Uni/Bi-LSTMs & Capsule layer model is higher than that of the All Bi-LSTMs model by 0.35 points and higher than that of the Softmax layer model by 0.29 points.",
    "label": "support"
  },
  {
    "id": "training_449_support",
    "table_caption": "Table: Precision, Recall and F0.5 results on CoNLL-13,on the ”small changes” and “large changes” portions of the OOV segment.",
    "table_content": "|| [BOLD] Model | [BOLD] Performance P | [BOLD] Performance R | [BOLD] Performance  [ITALIC] F0.5 ||\n|| [BOLD] Small Changes Portion | [BOLD] Small Changes Portion | [BOLD] Small Changes Portion | [BOLD] Small Changes Portion ||\n|| Hybrid model | 43.86 | 16.29 | 32.77 ||\n|| Nested Attention Hybrid Model | 48.25 | 17.92 | 36.04 ||\n|| [BOLD] Large Changes Portion | [BOLD] Large Changes Portion | [BOLD] Large Changes Portion | [BOLD] Large Changes Portion ||\n|| Hybrid model | 32.52 | 8.32 | 20.56 ||\n|| Nested Attention Hybrid Model | 33.05 | 8.11 | 20.46 ||",
    "claim": "The Nested Attention Hybrid Model outperforms the Hybrid model in the \"small changes\" portion across all performance metrics (P, R, F0.5).",
    "label": "support"
  },
  {
    "id": "training_487_support",
    "table_caption": "Table: BLEU scores on test set of the WMT14 English-German task and IWSLT14 German-English task. Our method is denoted as “FRAGE”, “Reweighting” denotes reweighting the loss of each word by reciprocal of its frequency, and “Weight Decay” denotes putting weight decay rate (0.2) on embeddings.",
    "table_content": "|| [BOLD] WMT En→De  [BOLD] Method | [BOLD] WMT En→De  [BOLD] BLEU | [BOLD] IWSLT De→En  [BOLD] Method | [BOLD] IWSLT De→En  [BOLD] BLEU ||\n|| Transformer Base  | 27.30 | Transformer | 33.12 ||\n|| Transformer Base + Reweighting | 26.04 | Transformer + Reweighting | 31.04 ||\n|| Transformer Base + Weight Decay | 26.76 | Transformer + Weight Decay | 32.52 ||\n|| Transformer Base with FRAGE | [BOLD] 28.36 | Transformer with FRAGE | [BOLD] 33.97 ||",
    "claim": "Transformer Base with FRAGE achieves the highest BLEU scores for both WMT En→De and IWSLT De→En tasks.",
    "label": "support"
  },
  {
    "id": "training_456_support",
    "table_caption": "Table: Effects of varying the fraction of tweets used for training and testing on classification accuracy in the city-prediction task, using All Words and LDA topics.",
    "table_content": "|| training fraction | testing fraction 0.2 | testing fraction 0.4 | testing fraction 0.6 | testing fraction 0.8 | testing fraction 1.0 ||\n|| 0.2 | 6.66 | 6.66 | 6.66 | 6.66 | 6.66 ||\n|| 0.4 | 13.33 | 13.33 | 13.33 | 13.33 | 20.00 ||\n|| 0.6 | 20.00 | 26.66 | 26.66 | 26.66 | 40.00 ||\n|| 0.8 | 33.33 | 46.66 | 33.33 | 53.33 | 53.33 ||\n|| 1.0 | 46.66 | 53.33 | 60.00 | 66.66 | 80.00 ||",
    "claim": "Performance increases as the training fraction increases.",
    "label": "support"
  },
  {
    "id": "training_431_support",
    "table_caption": "Table: Evaluation of different test corpus size for SentiCite (SC).",
    "table_content": "|| [BOLD] Approach | [BOLD] 5 docs | [BOLD] 10 docs | [BOLD] 20 docs | [BOLD] 30 docs ||\n|| SC-SVM | 0.7111 | 0.7091 | 0.7141 | 0.7203 ||\n|| SC-Paum | 0.5727 | 0.7795 | 0.8218 | 0.8221 ||",
    "claim": "For SC-SVM, the f-score remains relatively stable across different corpus sizes, while SC-Paum's performance increases initially with larger corpus sizes.",
    "label": "support"
  },
  {
    "id": "training_428_support",
    "table_caption": "Table: Distribution of positive and negative references in different sections of publications.",
    "table_content": "|| [BOLD] Section | [BOLD] Positive | [BOLD] Negative ||\n|| Introduction / Motivation | 0.22 | 0.28 ||\n|| Information / Background | 0.2 | 0.06 ||\n|| Related Work | 0.11 | 0.06 ||\n|| Approach / Method | [BOLD] 0.3 | 0.1 ||\n|| Evaluation / Experiments | 0.17 | [BOLD] 0.5 ||",
    "claim": "Negative references are often found in the Evaluation/Experiments section, while positive references are frequent in the Approach/Method section.",
    "label": "support"
  },
  {
    "id": "training_390_support",
    "table_caption": "Table: Performance on the target sentiment task",
    "table_content": "|| System | Architecture | English Pre | English Rec | English F1 | Spanish Pre | Spanish Rec | Spanish F1 ||\n|| Zhang, Zhang and Vo (2015) | Pipeline | 43.71 | 37.12 | 40.06 | 45.99 | 40.57 | 43.04 ||\n|| Zhang, Zhang and Vo (2015) | Joint | 44.62 | 35.84 | 39.67 | 46.67 | 39.99 | 43.02 ||\n|| Zhang, Zhang and Vo (2015) | Collapsed | 46.32 | 32.84 | 38.36 | 47.69 | 34.53 | 40.00 ||\n|| Li and Lu (2017) | SS | 44.57 | 36.48 | 40.11 | 46.06 | 39.89 | 42.75 ||\n|| Li and Lu (2017) | +embeddings | 47.30 | 40.36 | 43.55 | 47.14 | 41.48 | 44.13 ||\n|| Li and Lu (2017) | +POS tags | 45.96 | 39.04 | 42.21 | 45.92 | 40.25 | 42.89 ||\n|| Li and Lu (2017) | +semiMarkov | 44.49 | 37.93 | 40.94 | 44.12 | 40.34 | 42.14 ||\n|| Base Line | LSTM-CRF | 53.29 | 46.90 | 49.89 | 51.17 | 46.71 | 48.84 ||\n|| [ITALIC] This work | LSTM-CRF-T | 54.21 | 48.77 | 51.34 | 51.77 | 47.37 | 49.47 ||\n|| [ITALIC] This work | LSTM-CRF-Ti | 54.58 | 49.01 | 51.64 | 52.14 | 47.56 | 49.74 ||\n|| [ITALIC] This work | LSTM-CRF-Ti(g) | [BOLD] 55.31 | [BOLD] 49.36 | [BOLD] 52.15 | [BOLD] 52.82 | [BOLD] 48.41 | [BOLD] 50.50 ||",
    "claim": "The LSTM-CRF-Ti(g) model outperforms all other models in Precision, Recall, and F1 score for both English and Spanish.",
    "label": "support"
  },
  {
    "id": "training_519_support",
    "table_caption": "Table: PPL and relative WERRs (%) with varying floor interpolation weights for the translation component in the 180 hour setup.",
    "table_content": "|| [BOLD] Floor | [BOLD] Interpolated | [BOLD] WERR % ||\n|| [BOLD] weight | [BOLD] PPL | [EMPTY] ||\n|| 0.1 | 50.28 | 5.78 ||\n|| 0.15 | 51.24 | 7.04 ||\n|| 0.25 | 52.36 | 7.86 ||\n|| 0.3 | 53.37 | 7.49 ||\n|| 0.4 | 56.34 | 6.58 ||",
    "claim": "WERR fluctuates with varying floor weights.",
    "label": "support"
  },
  {
    "id": "training_497_support",
    "table_caption": "Table: Test accuracy of sentiment classification on Stanford Sentiment Treebank. Bold font indicates the best performance.",
    "table_content": "|| [BOLD] Model | [BOLD] Accuracy (%) ||\n|| BiLSTM | 84.8 ||\n|| pipeline | 85.7 ||\n|| ste | 85.4 ||\n|| spigot | [BOLD] 86.3 ||",
    "claim": "Spigot outperforms all baselines in accuracy, and ste achieves slightly worse performance than the pipeline.",
    "label": "support"
  },
  {
    "id": "training_457_support",
    "table_caption": "Table: Document-level evaluation of three approaches in two scientific areas: computer science (CS) and biomedical (Bio).",
    "table_content": "|| Approach | CS prec. | CS yield | Bio prec. | Bio yield ||\n|| Statistical | 98.4 | 712 | 94.4 | 928 ||\n|| Hybrid | 91.5 | 1990 | 92.1 | 3126 ||\n|| Off-the-shelf | 97.4 | 873 | 77.5 | 1206 ||",
    "claim": "In both domains, the statistical approach gives the highest precision and the lowest yield, while the hybrid approach consistently gives the highest yield.",
    "label": "support"
  },
  {
    "id": "training_469_support",
    "table_caption": "Table: Results (accuracy) of SeaReader and other approaches on MedQA task",
    "table_content": "|| [EMPTY] | Valid set | Test set ||\n|| Iterative Attention | 60.7 | 59.3 ||\n|| Neural Reasoner | 54.8 | 53.0 ||\n|| R-NET | 65.2 | 64.5 ||\n|| SeaReader | [BOLD] 73.9 | [BOLD] 73.6 ||\n|| SeaReader (ensemble) | [BOLD] 75.8 | [BOLD] 75.3 ||\n|| Human passing score | 60.0 (360/600) | 60.0 (360/600) ||",
    "claim": "SeaReader and its ensemble version achieve the highest performance on both the Valid and Test sets.",
    "label": "support"
  },
  {
    "id": "training_495_support",
    "table_caption": "Table: Human Performance on FQuAD",
    "table_content": "|| Dataset | F1 [%] | EM [%] ||\n|| FQuAD1.0-test. | 92.1 | 78.4 ||\n|| FQuAD1.1-test | 91.2 | 75.9 ||\n|| \"FQuAD1.1-test new samples\" | 90.5 | 74.1 ||\n|| FQuAD1.0-dev | 92.6 | 79.5 ||\n|| FQuAD1.1-dev | 92.1 | 78.3 ||\n|| \"FQuAD1.1-dev new samples\" | 91.4 | 76.7 ||",
    "claim": "There is a gap in human performance with a 78.4% EM score on the FQuAD1.0 test set and a 74.1% EM score on the new samples of the FQuAD1.1 test set.",
    "label": "support"
  },
  {
    "id": "training_470_support",
    "table_caption": "Table: Test performance with different number of documents given per candidate answer",
    "table_content": "|| Number of documents | top-1 | top-5 | top-10 | top-20 ||\n|| SeaReader accuracy | 57.8 | 71.7 | 73.6 | 74.4 ||\n|| Relevant document ratio | 0.90 | 0.54 | 0.46 | 0.29 ||",
    "claim": "SeaReader accuracy increases with more documents, while the relevant document ratio decreases as the number of documents grows.",
    "label": "support"
  },
  {
    "id": "training_473_support",
    "table_caption": "Table: Test accuracy in terms of BLEU on IWSLT’14 German-English translation with various loss functions cf. Figure 1. W & R (2016) refers to Wiseman and Rush (2016), B (2016) to Bahdanau et al. (2016), [S] indicates sequence level-training and [T] token-level training. We report averages and standard deviations over five runs with different random initialization.",
    "table_content": "|| [EMPTY] | [BOLD] test | [BOLD] std ||\n|| MLE (W & R, 2016) [T] | 24.03 | [EMPTY] ||\n|| BSO (W & R, 2016) [S] | 26.36 | [EMPTY] ||\n|| Actor-critic (B, 2016) [S] | 28.53 | [EMPTY] ||\n|| Huang et al. ( 2017 ) [T] | 28.96 | [EMPTY] ||\n|| Huang et al. ( 2017 ) (+LM) [T] | 29.16 | [EMPTY] ||\n|| TokNLL [T] | 31.78 | 0.07 ||\n|| TokLS [T] | 32.23 | 0.10 ||\n|| SeqNLL [S] | 32.68 | 0.09 ||\n|| Risk [S] | 32.84 | 0.08 ||\n|| MaxMargin [S] | 32.55 | 0.09 ||\n|| MultiMargin [S] | 32.59 | 0.07 ||\n|| SoftmaxMargin [S] | 32.71 | 0.07 ||",
    "claim": "Risk training improves BLEU scores by up to 0.61 compared to other methods.",
    "label": "support"
  },
  {
    "id": "training_500_support",
    "table_caption": "Table: Accuracy of P3 when trained and evaluated with labeled logical forms, food webs, or both.",
    "table_content": "|| Model | Accuracy | Δ ||\n|| [ITALIC] P3 | 69.1 | [EMPTY] ||\n|| + gold logical form | 75.1 | +6.0 ||\n|| + gold food web | 82.3 | +13.2 ||\n|| + both | 91.6 | +22.5 ||",
    "claim": "Accuracy improves from 69.1 to 91.6 when both gold logical form and gold food web are provided as input.",
    "label": "support"
  },
  {
    "id": "training_512_support",
    "table_caption": "Table: Results for all test languages on the official test sets for Task 2.",
    "table_content": "|| System Test Set | Baseline 1 Test Set | Baseline 1 Test Set | Baseline 2 Test Set | Baseline 2 Test Set | Sub-1 Test Set | Sub-1 Test Set | Sub-2 Test Set | Sub-2 Test Set | Sub-3 Test Set | Sub-3 Test Set ||\n|| [EMPTY] | slots | macro | slots | macro | slots | macro | slots | macro | slots | macro ||\n|| Basque | 30 | 0.0006 | 27 | 0.0006 | 30 | 0.0005 | 30 | 0.0005 | 30 | [BOLD] 0.0007 ||\n|| Bulgarian | 35 | 0.283 | 34 | [BOLD] 0.3169 | 35 | 0.2769 | 35 | 0.2894 | 35 | 0.2789 ||\n|| English | 4 | 0.656 | 4 | [BOLD] 0.662 | 4 | 0.502 | 4 | 0.528 | 4 | 0.512 ||\n|| Finnish | 21 | 0.0533 | 21 | [BOLD] 0.055 | 21 | 0.0536 | 21 | 0.0547 | 21 | 0.0535 ||\n|| German | 9 | 0.2835 | 9 | [BOLD] 0.29 | 9 | 0.273 | 9 | 0.2735 | 9 | 0.2735 ||\n|| Kannada | 172 | 0.1549 | 172 | [BOLD] 0.1512 | 172 | 0.111 | 172 | 0.1116 | 172 | 0.111 ||\n|| Navajo | 3 | 0.0323 | 3 | [BOLD] 0.0327 | 3 | 0.004 | 3 | 0.0043 | 3 | 0.0043 ||\n|| Spanish | 29 | 0.2296 | 29 | [BOLD] 0.2367 | 29 | 0.2039 | 29 | 0.2056 | 29 | 0.203 ||\n|| Turkish | 104 | 0.1421 | 104 | [BOLD] 0.1553 | 104 | 0.1488 | 104 | 0.1539 | 104 | 0.1513 ||\n|| [BOLD] All | [EMPTY] | 0.2039 | [EMPTY] | [BOLD] 0.2112 | [EMPTY] | 0.1749 | [EMPTY] | 0.1802 | [EMPTY] | 0.1765 ||",
    "claim": "For Basque, the NYU-CUBoulder-2 system achieved the highest macro accuracy of 0.0007, outperforming both baselines.",
    "label": "support"
  },
  {
    "id": "training_389_support",
    "table_caption": "Table: Comparing our models with recent results on the Aspect Sentiment datasets.",
    "table_content": "|| Models | English E+A | English E+A+S | Spanish E+A | Spanish E+A+S | Dutch E+A | Dutch E+A+S | Russian E+A | Russian E+A+S ||\n|| LSTM-CNN-CRFMa and Hovy ( 2016 ) | 58.73 | 44.20 | 64.32 | 50.34 | 51.62 | 36.88 | 58.88 | 38.13 ||\n|| LSTM-CRF-LMLiu et al. ( 2018 ) | 62.27 | 45.04 | 63.63 | 50.15 | 51.78 | 34.77 | 62.18 | 38.80 ||\n|| LSTM-CRF | 59.11 | 48.67 | 62.98 | 52.10 | 51.35 | 37.30 | 63.41 | 42.47 ||\n|| LSTM-CRF-T | 60.87 | 49.59 | 64.24 | 52.33 | 52.79 | 37.61 | 64.72 | 43.01 ||\n|| LSTM-CRF-TI | 63.11 | 50.19 | 64.40 | 52.85 | 53.05 | 38.07 | 64.98 | 44.03 ||\n|| LSTM-CRF-TI(g) | [BOLD] 64.74 | [BOLD] 51.24 | [BOLD] 66.13 | [BOLD] 53.47 | [BOLD] 53.63 | [BOLD] 38.65 | [BOLD] 65.64 | [BOLD] 45.65 ||",
    "claim": "LSTM-CRF-TI(g) achieves the highest performance across all tasks and languages.",
    "label": "support"
  },
  {
    "id": "training_559_support",
    "table_caption": "Table: Hyper-parameters used in our systems.",
    "table_content": "|| [EMPTY] | Hyper-parameter | Value ||\n|| LSTM | hidden size | 256 ||\n|| CNN | window size | 3 ||\n|| CNN | #filter | 30 ||\n|| Dropout | input dropout | 0.33 ||\n|| Dropout | BLSTM dropout | 0.5 ||\n|| Embedding | GloVe dimension | 300 ||\n|| Embedding | ELMo dimension | 1024 ||\n|| Embedding | [ITALIC] γ | 1 ||\n|| Language Model | [ITALIC] λ | 0.05 ||\n|| Training | batch size | 16 ||\n|| Training | initial learning rate | 0.01 ||\n|| Training | decay rate | 0.05 ||",
    "claim": "The decay rate used in the training procedure is 0.05.",
    "label": "support"
  },
  {
    "id": "training_520_support",
    "table_caption": "Table: Relative WERRs (%) with different post-editing techniques. Perplexity (PPL) is evaluated on a held-out in-domain dataset. Relative WERR captures the WER reduction w.r.t baseline trained on transcribed data only.",
    "table_content": "|| [BOLD] Postprocessing | [BOLD] Approach | [BOLD] PPL | [BOLD] Relative WERR % ||\n|| None | Raw translations | 11941.08 | -1.81 ||\n|| Post-editing | NE copy-over | 2889.45 | 2.36 ||\n|| [EMPTY] | NE resampling | 1241.52 | 4.62 ||\n|| [EMPTY] | Code mixing + NE resampling | 936.64 | 5.83 ||",
    "claim": "Code mixing combined with NE resampling achieves the lowest perplexity and the highest relative WERR of 5.83%.",
    "label": "support"
  },
  {
    "id": "training_444_support",
    "table_caption": "Table: Perplexities and word errors with session-based LSTM-LMs (forward direction only). The last line reflects the use of 1-best recognition output for words in preceding utterances.",
    "table_content": "|| Model inputs | PPL devset | PPL test | WER devset | WER test ||\n|| Utterance words, letter-3grams | 50.76 | 44.55 | 9.5 | 6.8 ||\n|| + session history words | 39.69 | 36.95 | [EMPTY] | [EMPTY] ||\n|| + speaker change | 38.20 | 35.48 | [EMPTY] | [EMPTY] ||\n|| + speaker overlap | 37.86 | 35.02 | [EMPTY] | [EMPTY] ||\n|| (with 1-best history) | 40.60 | 37.90 | 9.3 | 6.7 ||",
    "claim": "There is a reduction in perplexity when session history, speaker change, and speaker overlap are added as model inputs.",
    "label": "support"
  },
  {
    "id": "training_526_support",
    "table_caption": "Table: Performance of BiLSTM AES task on public-FCE test set with and without multi-task training objectives (cumulatively) for entire essay context. * indicates that the results are statistically significant when compared to + Elm",
    "table_content": "|| Automated Essay Scoring Cost | Automated Essay Scoring Spearman | Automated Essay Scoring QWK ||\n|| [ITALIC] Eaes | 0.334 | 0.324 ||\n|| +  [ITALIC] Elm | 0.376 | 0.347 ||\n|| +  [ITALIC] Eged | 0.537* | 0.459* ||",
    "claim": "Adding the GED objective (Eged) results in the highest performance improvements in both Spearman and QWK metrics for Automated Essay Scoring.",
    "label": "support"
  },
  {
    "id": "training_492_support",
    "table_caption": "Table: Segment recall from WSJ separated by phrase type. The 10 most frequent phrase types are shown above, and the highest value in each row is bolded. P-UP=PRNP-UP, P-LM=PRPN-LM",
    "table_content": "|| Label | Count | DIORA | P-UP | P-LM ||\n|| NP | 297,872 | [BOLD] 0.767 | 0.687 | 0.598 ||\n|| VP | 168,605 | [BOLD] 0.628 | 0.393 | 0.316 ||\n|| PP | 116,338 | 0.595 | 0.497 | [BOLD] 0.602 ||\n|| S | 87,714 | [BOLD] 0.798 | 0.639 | 0.657 ||\n|| SBAR | 24,743 | [BOLD] 0.613 | 0.403 | 0.554 ||\n|| ADJP | 12,263 | [BOLD] 0.604 | 0.342 | 0.360 ||\n|| QP | 11,441 | [BOLD] 0.801 | 0.336 | 0.545 ||\n|| ADVP | 5,817 | [BOLD] 0.693 | 0.392 | 0.500 ||\n|| PRN | 2,971 | [BOLD] 0.546 | 0.127 | 0.144 ||\n|| SINV | 2,563 | 0.926 | 0.904 | [BOLD] 0.932 ||",
    "claim": "DIORA achieves the highest recall across most phrase types and is the only model to perform effectively on verb phrases, but it performs worse than PRPN-LM on prepositional phrases.",
    "label": "support"
  },
  {
    "id": "training_403_support",
    "table_caption": "Table: Experiment results on Relation Extraction and Sentiment Analysis. Average and standard deviation of F1 scores (%) over multiple runs are reported (5 runs for RE and 10 runs for SA). LF(E) denotes directly applying logical forms onto explanations. Bracket behind each method illustrates corresponding data used in the method. S denotes training data without labels, E denotes explanations, R denotes surface pattern rules transformed from explanations; Sa denotes labeled data annotated with explanations, Su denotes the remaining unlabeled data. Sl denotes labeled data annotated using same time as creating explanations E, Slu denotes remaining unlabeled data corresponding to Sl.",
    "table_content": "|| [EMPTY] | TACRED | SemEval ||\n|| LF (E) | 23.33 | 33.86 ||\n|| CBOW-GloVe (R+S) | 34.6±0.4 | 48.8±1.1 ||\n|| PCNN (S [ITALIC] a) | 34.8±0.9 | 41.8±1.2 ||\n|| PA-LSTM (S [ITALIC] a) | 41.3±0.8 | 57.3±1.5 ||\n|| BiLSTM+ATT (S [ITALIC] a) | 41.4±1.0 | 58.0±1.6 ||\n|| BiLSTM+ATT (S [ITALIC] l) | 30.4±1.4 | 54.1±1.0 ||\n|| Self Training (S [ITALIC] a+S [ITALIC] u) | 41.7±1.5 | 55.2±0.8 ||\n|| Pseudo Labeling (S [ITALIC] a+S [ITALIC] u) | 41.5±1.2 | 53.5±1.2 ||\n|| Mean Teacher (S [ITALIC] a+S [ITALIC] u) | 40.8±0.9 | 56.0±1.1 ||\n|| Mean Teacher (S [ITALIC] l+S [ITALIC] lu) | 25.9±2.2 | 52.2±0.7 ||\n|| DualRE (S [ITALIC] a+S [ITALIC] u) | 32.6±0.7 | 61.7±0.9 ||\n|| Data Programming (E+S) | 30.8±2.4 | 43.9±2.4 ||\n|| NExT (E+S) | [BOLD] 45.6±0.4 | [BOLD] 63.5±1.0 ||",
    "claim": "NExT consistently outperforms all baseline models on both TACRED and SemEval datasets.",
    "label": "support"
  },
  {
    "id": "training_490_support",
    "table_caption": "Table: NLI unsupervised unlabeled binary constituency parsing comparing to CoreNLP predicted parses. PRPN F1 was calculated using the parse trees and results provided by Htut et al. (2018). F1 median and max are calculated over five random seeds and the top F1 value in each column is bolded. Note that we use median rather than mean in order to compare with previous work.",
    "table_content": "|| [BOLD] Model | F1 [ITALIC] median | F1 [ITALIC] max | [ITALIC] δ ||\n|| Random | 27.0 | 27.0 | 4.4 ||\n|| Balanced | 21.3 | 21.3 | 3.9 ||\n|| PRPN-UP | 48.6 | - | 4.9 ||\n|| PRPN-LM | 50.4 | - | 5.1 ||\n|| DIORA | 51.2 | 53.3 | 6.4 ||\n|| PRPN-UP+PP | - | 54.8 | 5.2 ||\n|| PRPN-LM+PP | - | 50.4 | 5.1 ||\n|| DIORA+PP | 59.0 | [BOLD] 59.1 | 6.7 ||",
    "claim": "DIORA surpasses both variants of PRPN in F1 scores.",
    "label": "support"
  },
  {
    "id": "training_504_support",
    "table_caption": "Table: Different Read-Again Model. Ours denotes Read-Again models. C denotes copy mechanism. Ours-Opt-1 and Ours-Opt-2 are the models described in section 3.1.3. Size denotes the size of decoder vocabulary in a model.",
    "table_content": "|| #Input | Model | Size | Rouge-1 | Rouge-2 | Rouge-L ||\n|| 1 sent | ABS (baseline) | 69K | 24.12 | 10.24 | 22.61 ||\n|| 1 sent | GRU (baseline) | 69K | 26.79 | 12.03 | 25.14 ||\n|| 1 sent | Ours-GRU | 69K | 27.26 | 12.28 | 25.48 ||\n|| 1 sent | Ours-LSTM | 69K | [BOLD] 27.82 | [BOLD] 12.74 | [BOLD] 26.01 ||\n|| 1 sent | GRU (baseline) | 15K | 24.67 | 11.30 | 23.28 ||\n|| 1 sent | Ours-GRU | 15K | 25.04 | 11.40 | 23.47 ||\n|| 1 sent | Ours-LSTM | 15K | 25.30 | 11.76 | 23.71 ||\n|| 1 sent | Ours-GRU (C) | 15K | [BOLD] 27.41 | 12.58 | [BOLD] 25.74 ||\n|| 1 sent | Ours-LSTM (C) | 15K | 27.37 | [BOLD] 12.64 | 25.69 ||\n|| 2 sent | Ours-Opt-1 (C) | 15K | 27.95 | [BOLD] 12.65 | 26.10 ||\n|| 2 sent | Ours-Opt-2 (C) | 15K | [BOLD] 27.96 | 12.65 | [BOLD] 26.18 ||",
    "claim": "Models with the copy mechanism achieve higher ROUGE scores despite a smaller decoder vocabulary size, and the Multiple-Sentences model achieves the best performance.",
    "label": "support"
  },
  {
    "id": "training_480_support",
    "table_caption": "Table: Performance of Our Automatic Severity Classification Method",
    "table_content": "|| [EMPTY] | P | R | F{}_{1} ||\n|| Mild Stenosis | 100.00 | 98.62 | 99.31 ||\n|| Moderate Stenosis | 93.33 | 93.33 | 93.33 ||\n|| Severe Stenosis | 75.00 | 90.00 | 81.82 ||\n|| Overall Accuracy | 97.00 | 97.00 | 97.00 ||",
    "claim": "The method achieves an overall accuracy of 97.00%, with precision, recall, and F1-score of 100.00%, 98.62%, and 99.31% for mild stenosis, 93.33% for all metrics in moderate stenosis, and 75.00% precision, 90.00% recall, and 81.82% F1-score for severe stenosis.",
    "label": "support"
  },
  {
    "id": "training_260_support",
    "table_caption": "Table: Average scores for the six different responses, on the six quality: Understandable, Natural, Maintains Context, Interesting, Uses Knowledge and Overall Quality.",
    "table_content": "|| [BOLD] System | [BOLD] Und (0-1) | [BOLD] Nat (1-3) | [BOLD] MCtx (1-3) | [BOLD] Int (1-3) | [BOLD] UK (0-1) | [BOLD] OQ (1-5) ||\n|| Topical-Chat | Topical-Chat | Topical-Chat | Topical-Chat | Topical-Chat | Topical-Chat | Topical-Chat ||\n|| Original Ground-Truth | 0.95 | 2.72 | 2.72 | 2.64 | 0.72 | 4.25 ||\n|| Argmax Decoding | 0.60 | 2.08 | 2.13 | 1.94 | 0.47 | 2.76 ||\n|| Nucleus Sampling (0.3) | 0.51 | 2.02 | 1.90 | 1.82 | 0.42 | 2.40 ||\n|| Nucleus Sampling (0.5) | 0.48 | 1.92 | 1.93 | 1.72 | 0.34 | 2.29 ||\n|| Nucleus Sampling (0.7) | 0.52 | 2.01 | 1.87 | 1.80 | 0.37 | 2.39 ||\n|| New Human Generated | [BOLD] 0.99 | [BOLD] 2.92 | [BOLD] 2.93 | [BOLD] 2.90 | [BOLD] 0.96 | [BOLD] 4.80 ||\n|| PersonaChat | PersonaChat | PersonaChat | PersonaChat | PersonaChat | PersonaChat | PersonaChat ||\n|| Original Ground-Truth | 0.99 | 2.89 | 2.82 | 2.67 | 0.56 | 4.36 ||\n|| Language Model | 0.97 | 2.63 | 2.02 | 2.24 | 0.08 | 2.98 ||\n|| LSTM Seq2Seq | 0.92 | 2.64 | 2.49 | 2.29 | 0.47 | 3.47 ||\n|| KV-MemNN | 0.93 | 2.70 | 2.18 | 2.56 | 0.17 | 3.25 ||\n|| New Human Generated | [BOLD] 1.00 | [BOLD] 2.97 | [BOLD] 2.88 | [BOLD] 2.87 | [BOLD] 0.96 | [BOLD] 4.80 ||",
    "claim": "The new human-generated responses outperform all other response types, including the original ground truth, across both datasets and all qualities.",
    "label": "support"
  },
  {
    "id": "training_494_support",
    "table_caption": "Table: The number of articles, paragraphs and questions for FQuAD1.1",
    "table_content": "|| Dataset | Articles | Paragraphs | Questions ||\n|| Train | 271 | 12123 | 50741 ||\n|| Development | 30 | 1387 | 5668 ||\n|| Test | 25 | 1398 | 5594 ||",
    "claim": "The FQuAD1.1 dataset consists of 271 articles, 12,123 paragraphs, and 50,741 questions for training; 30 articles, 1,387 paragraphs, and 5,668 questions for development; and 25 articles, 1,398 paragraphs, and 5,594 questions for testing.",
    "label": "support"
  },
  {
    "id": "training_429_support",
    "table_caption": "Table: SentiCiteDB.",
    "table_content": "|| [EMPTY] | [BOLD] Total | [BOLD] Train set | [BOLD] Test set ||\n|| Positive | 210 | 50 | 160 ||\n|| Neutral | 1805 | 50 | 1755 ||\n|| Negative | 85 | 50 | 35 ||\n|| Overall | 2100 | 150 | 1983 ||",
    "claim": "SentiCiteDB contains 210 positive, 1805 neutral, and 85 negative sentiment scores, with 50 citations from each class used for the training set.",
    "label": "support"
  },
  {
    "id": "training_454_support",
    "table_caption": "Table: Good-vs-Bad classification. ‡ and † mark statistically significant differences in accuracy compared to the baseline MaxEnt classifier with confidence levels of 99% and 95%, respectively (randomized test).",
    "table_content": "|| System | P | R | F1 | Acc ||\n|| [BOLD] Top-3 at SemEval-2015 Task 3 | [BOLD] Top-3 at SemEval-2015 Task 3 | [BOLD] Top-3 at SemEval-2015 Task 3 | [EMPTY] | [EMPTY] ||\n|| JAIST | 80.23 | 77.73 | 78.96 | 79.10 ||\n|| HITSZ-ICRC | 75.91 | 77.13 | 76.52 | 76.11 ||\n|| QCRI | 74.33 | 83.05 | 78.45 | 76.97 ||\n|| [BOLD] Instance Classifiers | [BOLD] Instance Classifiers | [BOLD] Instance Classifiers | [EMPTY] | [EMPTY] ||\n|| MaxEnt | 75.67 | 84.33 | 79.77 | 78.43 ||\n|| [BOLD] Linear Chain Classifiers | [BOLD] Linear Chain Classifiers | [BOLD] Linear Chain Classifiers | [EMPTY] | [EMPTY] ||\n|| CRF | 74.89 | 83.45 | 78.94 | 77.53 ||\n|| [BOLD] Global Inference Classifiers | [BOLD] Global Inference Classifiers | [BOLD] Global Inference Classifiers | [BOLD] Global Inference Classifiers | [EMPTY] ||\n|| ILP | 77.04 | 83.53 | 80.15 | 79.14‡ ||\n|| Graph-cut | 78.30 | 82.93 | [BOLD] 80.55 | [BOLD] 79.80‡ ||\n|| ILP-3C | 78.07 | 80.42 | 79.23 | 78.73 ||\n|| Graph-cut-3C | 78.26 | 81.32 | 79.76 | 79.19† ||",
    "claim": "MaxEnt shows higher accuracy than HITSZ-ICRC and QCRI.",
    "label": "support"
  },
  {
    "id": "training_354_support",
    "table_caption": "Table: Token-level POS tagging accuracy for Simple Projection, SRNN using MultiVec bilingual word embeddings as input, RNN444For RNN models, only one (same) system is used to tag German, Greek and Spanish, Projection+RNN and methods of Das & Petrov (2011), Duong et al (2013) and Gouws & Søgaard (2015).",
    "table_content": "|| [25mm] [BOLD] ModelLang. | [BOLD] French All words | [BOLD] French OOV | [BOLD] German All words | [BOLD] German OOV | [BOLD] Greek All words | [BOLD] Greek OOV | [BOLD] Spanish All words | [BOLD] Spanish OOV ||\n|| Simple Projection | 80.3 | 77.1 | 78.9 | 73.0 | 77.5 | 72.8 | 80.0 | 79.7 ||\n|| SRNN MultiVec | 75.0 | 65.4 | 70.3 | 68.8 | 71.1 | 65.4 | 73.4 | 62.4 ||\n|| SRNN | 78.5 | 70.0 | 76.1 | 76.4 | 75.7 | 70.7 | 78.8 | 72.6 ||\n|| BRNN | 80.6 | 70.9 | 77.5 | 76.6 | 77.2 | 71.0 | 80.5 | 73.1 ||\n|| BRNN - OOV | 81.4 | 77.8 | 77.6 | 77.8 | 77.9 | 75.3 | 80.6 | 74.7 ||\n|| Projection + SRNN | 84.5 | 78.8 | 81.5 | 77.0 | 78.3 | 74.6 | 83.6 | 81.2 ||\n|| Projection + BRNN | 85.2 | 79.0 | 81.9 | 77.1 | 79.2 | 75.0 | 84.4 | 81.7 ||\n|| Projection + BRNN - OOV | [BOLD] 85.6 | [BOLD] 80.4 | 82.1 | [BOLD] 78.7 | 79.9 | [BOLD] 78.5 | [BOLD] 84.4 | [BOLD] 81.9 ||\n|| (Das, 2011) | — | — | 82.8 | — | [BOLD] 82.5 | — | 84.2 | — ||\n|| (Duong, 2013) | — | — | [BOLD] 85.4 | — | 80.4 | — | 83.3 | — ||\n|| (Gouws, 2015a) | — | — | 84.8 | — | — | — | 82.6 | — ||",
    "claim": "The BRNN model has better performance than the SRNN model, and replacing OOVs with the closest words using CBOW increases tagging accuracy.",
    "label": "support"
  },
  {
    "id": "training_516_support",
    "table_caption": "Table: F1-scores on the NER target domain (CBS SciTech News).",
    "table_content": "|| Target Samples | 0 | 50 ||\n|| CT (Bapna et al. ( 2017 )) | 61.43 | 65.85 ||\n|| RZT (Shah et al. ( 2019 )) | 61.94 | 65.21 ||\n|| BiLSTM-CRF | 61.77 | 66.57 ||\n|| Coach | 64.08 | [BOLD] 68.35 ||\n|| Coach + TR | [BOLD] 64.54 | 67.45 ||",
    "claim": "Coach achieves the highest performance with 50 target samples.",
    "label": "support"
  },
  {
    "id": "training_464_support",
    "table_caption": "Table: Effects of applying the coverage regularizer and the reference beam search to structural models, evaluated on test-1951. Combining both yields the highest scores.",
    "table_content": "|| | [ITALIC] V| | [BOLD] R-2 | [BOLD] Train Speed | [BOLD] InVcb | [BOLD] InVcb+Src ||\n|| 1K | 13.99 | 2.5h/epoch | 60.57 | 76.04 ||\n|| 2K | 15.35 | 2.7h/epoch | 69.71 | 80.72 ||\n|| 5K | 17.25 | 3.2h/epoch | 79.98 | 86.51 ||\n|| 10K | 17.62 | 3.8h/epoch | 88.26 | 92.18 ||",
    "claim": "R-2 F1-score increases from 17.25 to 17.62 when the vocabulary size is increased from 5K to 10K, with a corresponding increase in training time from 3.2h/epoch to 3.8h/epoch.",
    "label": "support"
  },
  {
    "id": "training_458_support",
    "table_caption": "Table: Results of the entity extraction model on the development set of SemEval-2017 task 10.",
    "table_content": "|| Description | F1 ||\n|| Without LM | 49.9 ||\n|| With LM | 54.1 ||\n|| Avg. of 15 models with LM | 55.2 ||",
    "claim": "Including LM embeddings improves the F1 score by 4.2 points, and an ensemble of 15 models with LM further improves the F1 score by 1.1 points.",
    "label": "support"
  },
  {
    "id": "training_525_support",
    "table_caption": "Table: Performance of BiLSTM model on public-FCE test set for GED with multi-task training objectives (cumulatively) for entire essay contexts.",
    "table_content": "|| Error Detection Cost | Error Detection Precision | Error Detection Recall | Error Detection  [ITALIC] F0.5 ||\n|| [ITALIC] Eged | 0.492 | 0.251 | 0.413 ||\n|| +  [ITALIC] Elm | 0.588 | 0.221 | 0.442 ||\n|| +  [ITALIC] Eaes | 0.543 | 0.265 | 0.449 ||",
    "claim": "Error Detection F0.5 increases from 0.413 to 0.449 when using the AES training objective.",
    "label": "support"
  },
  {
    "id": "training_523_support",
    "table_caption": "Table: Relative WERRs (%) with varying levels of in-domain transcribed data.",
    "table_content": "|| [BOLD] Transcribed | [BOLD] WERR % ||\n|| [BOLD] Volume | [EMPTY] ||\n|| 10K | 15.65 ||\n|| 20K | 13.18 ||\n|| 50K | 9.42 ||\n|| 100K | 8.98 ||\n|| 200K | 7.86 ||",
    "claim": "WERR decreases as the volume of transcribed data increases from 10K to 200K.",
    "label": "support"
  },
  {
    "id": "training_471_support",
    "table_caption": "Table: Comparing recent language model results on lm1b.",
    "table_content": "|| [EMPTY] | Segmentation | Context Length | # of params | Perplexity | Bits/Byte ||\n|| shazeer2017outrageously | Word | Fixed | 6.0B | 28.0 | 0.929 ||\n|| shazeer2018mesh | Word-Piece | Fixed | 4.9B | 24.0 | 0.886 ||\n|| baevski2018adaptive | Word | Fixed | 1.0B | 23.0 | 0.874 ||\n|| transformerxl | Word | Arbitrary | 0.8B | 21.8 | 0.859 ||\n|| bytelmaaai | Byte | Fixed | 0.2B | 40.6 | 1.033 ||\n|| Ours | Byte | Fixed | 0.8B | 23.0 | 0.874 ||",
    "claim": "The perplexity of the byte-level model improves from 40.6 to 23.0 as the model capacity increases from 0.2B to 0.8B parameters, making it competitive with word-based models.",
    "label": "support"
  },
  {
    "id": "training_569_support",
    "table_caption": "Table: Limited left context per layer for audio encoder.",
    "table_content": "|| Audio Mask left | Audio Mask right | Label Mask left | WER (%) Test-clean | WER (%) Test-other ||\n|| 10 | 0 | 20 | 4.2 | 11.3 ||\n|| 6 | 0 | 20 | 4.3 | 11.8 ||\n|| 2 | 0 | 20 | 4.5 | 14.5 ||",
    "claim": "Using more audio history (higher audio mask left) results in lower WER on both Test-clean and Test-other datasets.",
    "label": "support"
  },
  {
    "id": "training_406_support",
    "table_caption": "Table: TACRED results on 130 explanations and 100 explanations",
    "table_content": "|| Metric | TACRED 130 Precision | TACRED 130 Recall | TACRED 130 F1 | TACRED 100 Precision | TACRED 100 Recall | TACRED 100 F1 ||\n|| LF (E) | [BOLD] 83.5 | 12.8 | 22.2 | [BOLD] 85.2 | 11.8 | 20.7 ||\n|| CBOW-GloVe (R+S) | 26.0±2.3 | [BOLD] 39.9±5.0 | 31.2±0.5 | 24.4±1.3 | [BOLD] 41.7±3.7 | 30.7±0.1 ||\n|| PCNN (S [ITALIC] a) | 41.8±2.7 | 28.8±1.8 | 34.1±1.1 | 28.2±3.4 | 22.2±1.3 | 24.8±1.9 ||\n|| PA-LSTM (S [ITALIC] a) | 44.9±1.7 | 33.5±2.9 | 38.3±1.3 | 39.9±2.1 | 38.2±1.1 | 39.0±1.3 ||\n|| BiLSTM+ATT (S [ITALIC] a) | 40.1±2.6 | 36.2±3.4 | 37.9±1.1 | 36.1±0.4 | 37.6±3.0 | 36.8±1.4 ||\n|| BiLSTM+ATT (S [ITALIC] l) | 35.0±9.0 | 25.4±1.6 | 28.9±2.7 | 43.3±2.2 | 23.1±3.3 | 30.0±3.1 ||\n|| Self Training (S [ITALIC] a+S [ITALIC] u) | 43.6±3.3 | 35.1±2.1 | 38.7±0.0 | 41.9±5.9 | 32.0±7.4 | 35.5±2.5 ||\n|| Pseudo Labeling (S [ITALIC] a+S [ITALIC] u) | 44.2±1.9 | 34.2±1.9 | 38.5±0.6 | 39.7±2.0 | 34.9±3.3 | 37.1±1.5 ||\n|| Mean Teacher (S [ITALIC] a+S [ITALIC] u) | 38.8±0.9 | 35.6±1.3 | 37.1±0.5 | 37.4±4.0 | 37.4±0.2 | 37.3±2.0 ||\n|| Mean Teacher (S [ITALIC] l+S [ITALIC] lu) | 21.1±3.3 | 28.7±1.8 | 24.2±1.8 | 17.5±4.7 | 18.4±.59 | 17.9±5.0 ||\n|| DualRE (S [ITALIC] a+S [ITALIC] u) | 34.9±3.6 | 30.5±2.3 | 32.3±1.0 | 40.6±4.3 | 19.1±1.5 | 25.9±0.6 ||\n|| Data Programming (E+S) | 34.3±16.1 | 18.7±1.4 | 23.5±4.9 | 43.5±2.3 | 15.0±2.3 | 22.2±2.4 ||\n|| NEXT (E+S) | 45.3±2.4 | 39.2±0.3 | [BOLD] 42.0±1.1 | 43.9±3.7 | 36.2±1.9 | [BOLD] 39.6±0.5 ||",
    "claim": "NEXT (E+S) achieves the highest F1 scores for both TACRED 130 and TACRED 100.",
    "label": "support"
  },
  {
    "id": "training_496_support",
    "table_caption": "Table: Answer type comparison for the development sets of FQuAD1.1 and SQuAD1.1",
    "table_content": "|| Answer type | FQuAD1.1 [%] | SQuAD1.1 [%] ||\n|| Common noun | 26.6 | 31.8 ||\n|| Person | 14.6 | 12.9 ||\n|| Other proper nouns | 13.8 | 15.3 ||\n|| Location | 14.1 | 4.4 ||\n|| Date | 7.3 | 8.9 ||\n|| Other numeric | 13.6 | 10.9 ||\n|| Verb | 6.6 | 5.5 ||\n|| Adjective | 2.6 | 3.9 ||\n|| Other | 0.9 | 2.7 ||",
    "claim": "The most represented answer type for both datasets is Common Noun, with FQuAD1.1 scoring 26.6% and SQuAD1.1 scoring 31.8%. FQuAD1.1 scores higher than SQuAD1.1 for Person, Location, and Other Numeric categories, while SQuAD1.1 scores higher for the Date category.",
    "label": "support"
  },
  {
    "id": "training_528_support",
    "table_caption": "Table: Transfer learning results of the classification accuracy on all the datasets. Amazon (En) and Amazon (Fr) are the English and French versions of the task, training the models on the data for each language. The state-of-the-art results are cited from ijcai2018-802 ijcai2018-802 for both Amazon Reviews tasks and NIPS2017_7209 NIPS2017_7209 for SST and SNLI.",
    "table_content": "|| [BOLD] Model | Amazon (En) | Amazon (Fr) | SST (En) | SNLI (En) ||\n|| Proposed model:  [ITALIC] Encoder-Classifier | 76.60 | 82.50 | 79.63 | 76.70 ||\n|| + Pre-trained Encoder | 80.70 | 83.18 | 84.18 | 84.42 ||\n|| + Freeze Encoder | 84.13 | 85.65 | 84.51 | 84.41 ||\n|| State-of-the-art Models | 83.50 | 87.50 | 90.30 | 88.10 ||",
    "claim": "Freezing the pre-trained encoder after initialization improves performance on the Amazon English and French tasks compared to just using the pre-trained encoder.",
    "label": "support"
  },
  {
    "id": "training_554_support",
    "table_caption": "Table: WERs (%) for hybrid systems on the LibriSpeech 960hr. 4-gram model is used in the first pass to generates lattices for rescoring. The row ”Lattice” shows oracle WERs of the lattices.",
    "table_content": "|| LM | [ITALIC] L | Para. in M | dev clean | dev clean | dev other | dev other | test clean | test clean | test other | test other ||\n|| LM | [ITALIC] L | in M | PPL | WER | PPL | WER | PPL | WER | PPL | WER ||\n|| 4-gram | - | 230 | 151.7 | 3.4 | 140.6 | 8.3 | 158.1 | 3.8 | 145.7 | 8.8 ||\n|| Lattice | - | - | - | 1.0 | - | 2.3 | - | 1.3 | - | 2.6 ||\n|| LSTM | 2 | 1048 | 60.2 | 2.3 | 60.2 | 5.4 | 64.8 | 2.6 | 61.7 | 5.9 ||\n|| Trans- | 24 | 281 | 57.8 | 2.2 | 58.3 | [BOLD] 5.2 | 62.2 | [BOLD] 2.5 | 59.4 | 5.7 ||\n|| former | 42 | 338 | 54.5 | [BOLD] 2.1 | 55.5 | [BOLD] 5.2 | 59.1 | [BOLD] 2.5 | 56.4 | 5.7 ||\n|| former | 96 | 431 | [BOLD] 53.2 | [BOLD] 2.1 | [BOLD] 54.2 | [BOLD] 5.2 | [BOLD] 57.6 | [BOLD] 2.5 | [BOLD] 55.0 | [BOLD] 5.6 ||",
    "claim": "Transformer language models achieve lower WER values compared to LSTM baselines across all conditions.",
    "label": "support"
  },
  {
    "id": "training_475_support",
    "table_caption": "Table: Effect of initializing sequence-level training (Risk) with parameters from token-level likelihood (TokNLL) or label smoothing (TokLS).",
    "table_content": "|| [EMPTY] | [BOLD] valid | [BOLD] test ||\n|| TokNLL | 32.96 | 31.74 ||\n|| Risk init with TokNLL | 33.27 | 32.07 ||\n|| Δ | +0.31 | +0.33 ||\n|| TokLS | 33.11 | 32.21 ||\n|| Risk init with TokLS | 33.91 | 32.85 ||\n|| Δ | +0.8 | +0.64 ||",
    "claim": "The improvement of initializing with TokNLL is 0.31 BLEU on the valid set and 0.33 BLEU on the test set, whereas the improvement from initializing with TokLS is 0.8 BLEU on the valid set and 0.64 BLEU on the test set.",
    "label": "support"
  },
  {
    "id": "training_384_support",
    "table_caption": "Table: ASR results (WER ↓) of APC with varying n during pre-training and different transfer learning approaches (Frozen vs. Finetuned). log Mel is the baseline that uses log Mel spectrograms as input features. The best transfer learning result is marked in bold.",
    "table_content": "|| Features | [ITALIC] n 1 | [ITALIC] n 2 | [ITALIC] n 3 | [ITALIC] n 5 | [ITALIC] n 10 | [ITALIC] n 20 ||\n|| log Mel | 18.3 | 18.3 | 18.3 | 18.3 | 18.3 | 18.3 ||\n|| R-APC Scratch | 23.2 | 23.2 | 23.2 | 23.2 | 23.2 | 23.2 ||\n|| R-APC Frozen | 17.2 | 15.8 | 15.2 | 16.3 | 17.8 | 20.9 ||\n|| R-APC Finetuned | 18.2 | 17.6 | 16.9 | 18.2 | 19.7 | 21.7 ||\n|| T-APC Scratch | 25.0 | 25.0 | 25.0 | 25.0 | 25.0 | 25.0 ||\n|| T-APC Frozen | 19.0 | 16.1 | 14.1 | [BOLD] 13.7 | 15.4 | 21.3 ||\n|| T-APC Finetuned | 22.4 | 17.0 | 15.5 | 14.6 | 16.9 | 23.3 ||",
    "claim": "The best n for R-APC is 3 and for T-APC it is 5. For all n, keeping pre-trained APC weights fixed (Frozen) works better than fine-tuning them (Finetuned). Training APC from scratch (Scratch) always performs the worst. WER is reduced from 18.3 to 13.7 with APC transfer learning.",
    "label": "support"
  },
  {
    "id": "training_507_support",
    "table_caption": "Table: Low resource setup (3K tokens), evaluated with LAS.",
    "table_content": "|| [EMPTY] | DE | ES | FR ||\n|| Sup | 58.93 | 61.99 | 60.45 ||\n|| Cas | 64.08 | [BOLD] 70.45 | [BOLD] 68.72 ||\n|| SMTL | 63.57 | 69.01 | 65.04 ||\n|| [ITALIC]  + weighted sampling | [ITALIC] 63.50 | [ITALIC] 70.17 | [ITALIC] 68.52 ||\n|| MTL | 62.43 | 66.67 | 64.23 ||\n|| [ITALIC]  + weighted sampling | [BOLD] 64.22 | [ITALIC] 68.42 | [ITALIC] 66.67 ||\n|| Duong et al. | 61.2 | 69.1 | 65.3 ||\n|| Duong et al. + Dict | 61.8 | 70.5 | 67.2 ||",
    "claim": "SMTL and MTL performance improves with weighted sampling in low-resource settings.",
    "label": "support"
  },
  {
    "id": "training_509_support",
    "table_caption": "Table: Automatic evaluation: BLEU. Significant differences at p<0.01 are in bold.",
    "table_content": "|| [BOLD] model | [BOLD] BLEU ||\n|| baseline | 29.46 ||\n|| concatenation (previous sentence) | 29.53 ||\n|| context encoder (previous sentence) | [BOLD] 30.14 ||\n|| context encoder (next sentence) | 29.31 ||\n|| context encoder (random context) | 29.69 ||",
    "claim": "The context encoder (previous sentence) model achieves the highest BLEU score of 30.14.",
    "label": "support"
  },
  {
    "id": "training_543_support",
    "table_caption": "Table: Distribution of sentences over classes",
    "table_content": "|| Assigned label | #sent | % ||\n|| CFS | 5,318 | 23,87 ||\n|| UFS | 2,328 | 10.45 ||\n|| NFS | 14,635 | 65.68 ||\n|| total | 22,281 | 100.00 ||",
    "claim": "NFS is the most assigned label, with 14,635 sentences, accounting for 65.68% of the total.",
    "label": "support"
  },
  {
    "id": "training_563_support",
    "table_caption": "Table: POS tagging accuracy using encoder and decoder representations with/without attention.",
    "table_content": "|| Attn | POS Accuracy ENC | POS Accuracy DEC | BLEU Ar-En | BLEU En-Ar ||\n|| ✓ | 89.62 | 86.71 | 24.69 | 13.37 ||\n|| ✗ | 74.10 | 85.54 | 11.88 | 5.04 ||",
    "claim": "Models with attention achieve higher POS accuracy and BLEU scores compared to models without attention.",
    "label": "support"
  },
  {
    "id": "training_518_support",
    "table_caption": "Table: Averaged F1-scores for seen and unseen slots over all target domains. ‡ represent the number of training samples utilized for the target domain.",
    "table_content": "|| Target Samples‡ | 0 samples unseen | 0 samples seen | 20 samples unseen | 20 samples seen | 50 samples unseen | 50 samples seen ||\n|| CT | 27.1 | 44.18 | 50.13 | 61.21 | 62.05 | 69.64 ||\n|| RZT | 28.28 | 47.15 | 52.56 | 63.26 | 63.96 | 73.10 ||\n|| Coach | 32.89 | 50.78 | 61.96 | 73.78 | 74.65 | 76.95 ||\n|| Coach+TR | [BOLD] 34.09 | [BOLD] 51.93 | [BOLD] 64.16 | [BOLD] 73.85 | [BOLD] 76.49 | [BOLD] 80.16 ||",
    "claim": "Coach+TR achieves the highest performance across all conditions for both seen and unseen slots.",
    "label": "support"
  },
  {
    "id": "training_530_support",
    "table_caption": "Table: Comparison of our best zero-shot result on the French SNLI test set to other baselines. See text for details.",
    "table_content": "|| [BOLD] Model | SNLI (Fr) ||\n|| Our best zero-shot  [ITALIC] Encoder-Classifier | [BOLD] 73.88 ||\n|| INVERT  | 62.60 ||\n|| BiCVM  | 59.03 ||\n|| RANDOM  | 63.21 ||\n|| RATIO  | 58.64 ||",
    "claim": "Our best zero-shot Encoder-Classifier outperforms all other methods listed in the table on SNLI (Fr).",
    "label": "support"
  },
  {
    "id": "training_552_support",
    "table_caption": "Table: Effect of activation functions. Perplexity after 1 epoch (10 sub-epochs in our setup) for (24, 2048, 512, 8).",
    "table_content": "|| Activation | Perplexity Train | Perplexity Dev ||\n|| ReLU  | 76.4 | 72.5 ||\n|| GLU  | 76.5 | 72.8 ||\n|| GELU  | [BOLD] 75.7 | [BOLD] 72.2 ||",
    "claim": "GELU achieves the lowest perplexity on both train and dev datasets.",
    "label": "support"
  },
  {
    "id": "training_553_support",
    "table_caption": "Table: Effect of activation functions. Perplexity after 1 epoch (10 sub-epochs in our setup) for (24, 2048, 512, 8).",
    "table_content": "|| [ITALIC] H | Params. in M | Perplexity Train | Perplexity Dev ||\n|| 1 | 243 | 71.9 | 70.8 ||\n|| 4 | 243 | 69.1 | 68.6 ||\n|| 8 | 243 | 67.6 | 67.1 ||\n|| 16 | 243 | [BOLD] 66.9 | [BOLD] 66.6 ||",
    "claim": "16 heads give the best performance with the lowest perplexity values in both training and development.",
    "label": "support"
  },
  {
    "id": "training_502_support",
    "table_caption": "Table: Classification performance of the different representations. The upper part of the table presents one-hot-encoding methods, while the bottom part methods that depend on the dimensionality of the distributed representations. We report the best performance obtained when the size N of the training data N∈{1,50,100,150,200}×103. The best achieved performance per classification problem is shown in bold. We have performed statistical significance tests to test if improvements obtained compared to tf-idf representations are statistically important, which is noted by putting a dagger (†) to the accuracy scores.",
    "table_content": "|| [ITALIC] hash | PubMed1,000 0.63 | PubMed1,000 0.63 | PubMed1,000 0.63 | PubMed5,000 0.427 | PubMed5,000 0.427 | PubMed5,000 0.427 | PubMed10,000 0.456 | PubMed10,000 0.456 | PubMed10,000 0.456 ||\n|| [ITALIC] tf-idf | 0.65 | 0.65 | 0.65 | 0.469 | 0.469 | 0.469 | 0.492 | 0.492 | 0.492 ||\n|| [EMPTY] | D=100 | D=200 | D=400 | D=100 | D=200 | D=400 | D=100 | D=200 | D=400 ||\n|| [ITALIC] x_conc | 0.592 | 0.614 | 0.626 | 0.362 | 0.41 | 0.436 | 0.386 | 0.434 | 0.454 ||\n|| [ITALIC] hash+x_conc | 0.651 | 0.654 | 0.646 | 0.464 | 0.476 | 0.473 | 0.488 | 0.495 | 0.491 ||\n|| [ITALIC] tfidf+x_conc | [BOLD] 0.66† | [BOLD] 0.66† | 0.656 | 0.484† | 0.486† | [BOLD] 0.487† | 0.507† | 0.507† | [BOLD] 0.512† ||",
    "claim": "Tf-idf performs better than both x_conc and hash, and combining tf-idf or hash with x_conc improves performance across all datasets.",
    "label": "support"
  },
  {
    "id": "training_576_support",
    "table_caption": "Table: IR performance (recall in percentages) of various Elasticsearch setups on the HotpotQA dev set using the original question.",
    "table_content": "|| [BOLD] IR System | [BOLD] R@10 for  [ITALIC] d1 | [BOLD] R@10 for  [ITALIC] d2 ||\n|| Final system | 87.85 | 36.91 ||\n|| w/o Title Boosting | 86.85 | 32.64 ||\n|| w/o Reranking | 86.32 | 34.77 ||\n|| w/o Both | 84.67 | 29.55 ||",
    "claim": "The final system achieves higher R@10 scores for both d1 and d2 compared to configurations without title boosting, reranking, or both.",
    "label": "support"
  },
  {
    "id": "training_462_support",
    "table_caption": "Table: F1 results on OntoNotes test for systems trained on data projected via FastAlign and DiscAlign.",
    "table_content": "|| Method | # train | P | R | F1 ||\n|| Zh Gold | 36K | 75.46 | 80.55 | 77.81 ||\n|| FastAlign | 36K | 38.99 | 36.61 | 37.55 ||\n|| FastAlign | 53K | 39.46 | 36.65 | 37.77 ||\n|| DiscAlign | 36K | 51.94 | 52.37 | 51.76 ||\n|| DiscAlign | 53K | 51.92 | 51.93 | 51.57 ||",
    "claim": "Zh Gold achieves the highest F1 score among the methods listed, regardless of the training size.",
    "label": "support"
  },
  {
    "id": "training_450_support",
    "table_caption": "Table: WER of LSTMLM lattice rescoring algorithms",
    "table_content": "|| Algorithm | WER (%) | [EMPTY] ||\n|| [EMPTY] | LM used in | LM used in ||\n|| [EMPTY] | Lattice generation | Lattice generation ||\n|| [EMPTY] | 2-gram | 5-gram ||\n|| [ITALIC] Push-forward (Algorithm  1 ) | [ITALIC] Push-forward (Algorithm  1 ) | [EMPTY] ||\n|| [ITALIC] k=1 | 12.8 | 12.4 ||\n|| [ITALIC] k=10 | 12.7 | 12.4 ||\n|| [ITALIC] k=50 | 12.6 | 12.4 ||\n|| Expand lattice to N-gram order,  [ITALIC] Push-forward,  [ITALIC] k=1 | Expand lattice to N-gram order,  [ITALIC] Push-forward,  [ITALIC] k=1 | [EMPTY] ||\n|| [ITALIC] N≤2 | 12.8 | 12.4 ||\n|| [ITALIC] N=3 | 12.6 | 12.4 ||\n|| [ITALIC] N≥4 | 12.4 | 12.3 ||\n|| [ITALIC] LSTM State Pooling (Algorithm  2 ) | [ITALIC] LSTM State Pooling (Algorithm  2 ) | [EMPTY] ||\n|| Uniform (Equation  12 ) | 13.0 | 12.6 ||\n|| Max Prob (Equation  10 ) | 12.8 | 12.5 ||\n|| Sum Prob (Equation  11 ) | 12.8 | 12.5 ||\n|| [ITALIC] Arc Beam (Algorithm  3 ) | [ITALIC] Arc Beam (Algorithm  3 ) | [EMPTY] ||\n|| Default setting | 12.8 | 12.5 ||",
    "claim": "The WER for 5-gram lattices is consistently lower than for 2-gram lattices across all configurations.",
    "label": "support"
  },
  {
    "id": "training_486_support",
    "table_caption": "Table: BLEU scores on test set on WMT2014 English-German and IWSLT German-English tasks.",
    "table_content": "|| [BOLD] WMT En→De  [BOLD] Method | [BOLD] WMT En→De  [BOLD] BLEU | [BOLD] IWSLT De→En  [BOLD] Method | [BOLD] IWSLT De→En  [BOLD] BLEU ||\n|| ByteNet kalchbrenner2016neural  | 23.75 | DeepConv gehring2016convolutional  | 30.04 ||\n|| ConvS2S gehring2017convolutional  | 25.16 | Dual transfer learning  Wang2018Dual  | 32.35 ||\n|| Transformer Base vaswani2017attention  | 27.30 | ConvS2S+SeqNLL  edunov2017classical  | 32.68 ||\n|| Transformer Base with FRAGE | [BOLD] 28.36 | ConvS2S+Risk  edunov2017classical  | 32.93 ||\n|| Transformer Big vaswani2017attention  | 28.40 | Transformer | 33.12 ||\n|| Transformer Big with FRAGE | [BOLD] 29.11 | Transformer with FRAGE | [BOLD] 33.97 ||",
    "claim": "Transformer Base with FRAGE outperforms Transformer Base by 1.06 BLEU in the WMT14 English-German task, and Transformer Big with FRAGE outperforms Transformer Big by 0.71 BLEU. In the IWSLT14 German-English task, Transformer with FRAGE outperforms Transformer by 0.85 BLEU.",
    "label": "support"
  },
  {
    "id": "training_624_support",
    "table_caption": "Table: Results on SemEval 2018 Task 11 (MCScript). Here, models with * are ensemble models.",
    "table_content": "|| [BOLD] Models | [BOLD] Dev | [BOLD] Test ||\n|| TriAN | 82.78 | 81.08 ||\n|| TriAN + ConceptNet | 83.84 | 81.94 ||\n|| *TriAN + ConceptNet | 85.27 | 83.84 ||\n|| Integrated TriAN | 83.90 | 82.10 ||\n|| Integrated TriAN + ConceptNet | [BOLD] 84.42 | [BOLD] 83.12 ||\n|| *Integrated TriAN + ConceptNet | [BOLD] 85.94 | [BOLD] 84.41 ||",
    "claim": "*Integrated TriAN + ConceptNet achieves the highest performance on both the Dev and Test datasets.",
    "label": "support"
  },
  {
    "id": "training_592_support",
    "table_caption": "Table: Results on the Weibo NER dataset",
    "table_content": "|| [BOLD] Model | [BOLD] P (%) | [BOLD] R (%) | [BOLD] F1 (%) ||\n|| [peng2016improving] | - | - | 58.99 ||\n|| [he2017unified] | - | - | 58.23 ||\n|| [zhang2018lattice] | - | - | 58.79 ||\n|| LEMON | 70.86 | 55.42 | [BOLD] 62.19 ||",
    "claim": "LEMON achieved the highest F1-score.",
    "label": "support"
  },
  {
    "id": "training_539_support",
    "table_caption": "Table: Results on FEVER development and test sets.",
    "table_content": "|| [EMPTY] | dev ACC | dev FEVER | test ACC | test FEVER ||\n|| baseline Zhou et al. ( 2019 ) | 73.67 | 68.69 | 71.01 | 65.64 ||\n|| baseline (ours) | 73.72 | 69.26 | 70.80 | 65.88 ||\n|| GSN | [BOLD] 74.89 | [BOLD] 70.51 | [BOLD] 72.00 | [BOLD] 67.13 ||",
    "claim": "The GSN system improves over the baseline systems by more than 1% in both ACC and FEVER scores on dev and test datasets.",
    "label": "support"
  },
  {
    "id": "training_489_support",
    "table_caption": "Table: Full WSJ (test set) unsupervised unlabeled binary constituency parsing including punctuation. † indicates trained to optimize NLI task. Mean and max are calculated over five random restarts. PRPN F1 was calculated using the parse trees and results provided by Htut et al. (2018). The depth (δ) is the average tree height. +PP refers to post-processing heuristic that attaches trailing punctuation to the root of the tree. The top F1 value in each column is bolded.",
    "table_content": "|| [BOLD] Model | F1 [ITALIC] μ | F1 [ITALIC] max | [ITALIC] δ ||\n|| LB | 13.1 | 13.1 | 12.4 ||\n|| RB | 16.5 | 16.5 | 12.4 ||\n|| Random | 21.4 | 21.4 | 5.3 ||\n|| Balanced | 21.3 | 21.3 | 4.6 ||\n|| RL-SPINN† | 13.2 | 13.2 | - ||\n|| ST-Gumbel - GRU† | 22.8 ±1.6 | 25.0 | - ||\n|| PRPN-UP | 38.3 ±0.5 | 39.8 | 5.9 ||\n|| PRPN-LM | 35.0 ±5.4 | 42.8 | 6.2 ||\n|| ON-LSTM | 47.7 ±1.5 | 49.4 | 5.6 ||\n|| DIORA | 48.9 ±0.5 | 49.6 | 8.0 ||\n|| PRPN-UP+PP | - | 45.2 | 6.7 ||\n|| PRPN-LM+PP | - | 42.4 | 6.3 ||\n|| DIORA+PP | [BOLD] 55.7 ±0.4 | [BOLD] 56.2 | 8.5 ||",
    "claim": "DIORA+PP achieves a mean F1 score 7 points higher than ON-LSTM and an increase of over 6.5 max F1 points. DIORA exhibits less variance between random seeds than ON-LSTM. PRPN-UP and DIORA benefit more from the +PP heuristic than PRPN-LM.",
    "label": "support"
  },
  {
    "id": "training_560_support",
    "table_caption": "Table: 5 most improved NE types when using ELMo.",
    "table_content": "|| Named Entity | GloVe | ELMo | Token Length ||\n|| Book | 48.65 | 76.92 | 3.2 ||\n|| Printing Other | 60.38 | 83.33 | 3.5 ||\n|| Spaceship | 61.90 | 80.00 | 2.7 ||\n|| Earthquake | 75.00 | 90.20 | 3.8 ||\n|| Public Institution | 80.00 | 95.00 | 4.2 ||",
    "claim": "ELMo achieves higher performance scores than GloVe across all named entity types.",
    "label": "support"
  },
  {
    "id": "training_571_support",
    "table_caption": "Table: Limited left context per layer for label encoder.",
    "table_content": "|| Audio Mask left | Audio Mask right | Label Mask left | WER (%) Test-clean | WER (%) Test-other ||\n|| 10 | 0 | 20 | 4.2 | 11.3 ||\n|| 10 | 0 | 4 | 4.2 | 11.4 ||\n|| 10 | 0 | 3 | 4.2 | 11.4 ||\n|| 10 | 0 | 2 | 4.3 | 11.5 ||\n|| 10 | 0 | 1 | 4.4 | 12 ||",
    "claim": "As the label mask left decreases, the WER slightly increases, indicating that limited left context is sufficient for maintaining performance.",
    "label": "support"
  },
  {
    "id": "training_623_support",
    "table_caption": "Table: Result on SICK. Here, * stands for multimodal NLU models.",
    "table_content": "|| [BOLD] Models | [BOLD] Acc(%) ||\n|| *Cap2Both 2018:NAACL_sentence_visual | 81.7 ||\n|| BERT | 90.4 ||\n|| BERT + STVF | [BOLD] 90.6 ||",
    "claim": "BERT + STVF achieves the highest accuracy at 90.6%.",
    "label": "support"
  },
  {
    "id": "training_466_support",
    "table_caption": "Table: Percentages of source dependency relations (of various types) preserved in the system summaries.",
    "table_content": "|| [BOLD] System | [BOLD] nsubj | [BOLD] dobj | [BOLD] amod | [BOLD] nmod | [BOLD] nmod:poss | [BOLD] mark | [BOLD] case | [BOLD] conj | [BOLD] cc | [BOLD] det ||\n|| Baseline | 7.23 | 12.07 | 20.45 | 8.73 | 12.46 | 15.83 | 14.84 | 9.72 | 5.03 | 2.22 ||\n|| Struct+Input | 7.03 | 11.72 | 19.72 | [BOLD] 9.17↑ | 12.46 | 15.35 | 14.69 | 9.55 | 4.67 | 1.97 ||\n|| Struct+Hidden | [BOLD] 7.78↑ | [BOLD] 12.34↑ | [BOLD] 21.11↑ | [BOLD] 9.18↑ | [BOLD] 14.86↑ | 14.93 | [BOLD] 15.84↑ | 9.47 | 3.93 | [BOLD] 2.65↑ ||\n|| Struct+2Way+Word | [BOLD] 7.46↑ | [BOLD] 12.69↑ | [BOLD] 20.59↑ | [BOLD] 9.03↑ | [BOLD] 13.00↑ | 15.83 | 14.43 | 8.86 | 3.48 | 1.91 ||\n|| Struct+2Way+Relation | [BOLD] 7.35↑ | [BOLD] 12.07↑ | [BOLD] 20.59↑ | 8.68 | [BOLD] 13.47↑ | 15.41 | 14.39 | 9.12 | 4.30 | 1.89 ||",
    "claim": "Models with structure-infused copy mechanisms, such as \"Struct+Hidden\" and \"Struct+2Way+Word,\" have higher preservation rates for important dependency relations like nsubj, dobj, amod, nmod, and nmod:poss compared to the baseline.",
    "label": "support"
  },
  {
    "id": "training_579_support",
    "table_caption": "Table: Label accuracy on the difficult dev set with different ERNet layers and evidence aggregators (%).",
    "table_content": "|| [BOLD] ERNet Layers | [BOLD] Aggregator  [BOLD] Attention | [BOLD] Aggregator  [BOLD] Max | [BOLD] Aggregator  [BOLD] Mean ||\n|| 0 | 66.17 | 65.36 | 65.03 ||\n|| 1 | 67.13 | 66.63 | 66.76 ||\n|| 2 | [BOLD] 67.44 | [BOLD] 67.24 | [BOLD] 67.56 ||\n|| 3 | 66.53 | 66.72 | 66.89 ||",
    "claim": "Models with 2 ERNet layers achieve the best results across all aggregation methods.",
    "label": "support"
  },
  {
    "id": "training_537_support",
    "table_caption": "Table: Results comparison with average and standard deviation of five runs. “ANS”, “SUP” and “JOINT” indicate the jointly trained models’ performance in terms of measurements on answer span prediction, supporting sentence prediction and joint tasks.",
    "table_content": "|| [EMPTY] | ANS EM | ANS  [ITALIC] F1 | SUP EM | SUP  [ITALIC] F1 | JOINT EM | JOINT  [ITALIC] F1 ||\n|| baseline | 62.99±0.16 | 76.90±0.31 | 61.35±0.17 | 88.73±0.09 | 41.76±0.40 | 69.64±0.28 ||\n|| GSN | 63.56±0.31 | 77.26±0.11 | 63.26±0.16 | 89.35±0.04 | 43.51±0.27 | 70.43±0.14 ||",
    "claim": "GSN shows improved performance over the baseline in supporting sentence prediction (SUP EM and SUP F1) and joint metrics (JOINT EM and JOINT F1).",
    "label": "support"
  },
  {
    "id": "training_597_support",
    "table_caption": "Table: Results on the OntoNotes-4 dataset",
    "table_content": "|| [BOLD] Model | [BOLD] P (%) | [BOLD] R (%) | [BOLD] F1 (%) ||\n|| [wang2013effective] {\\dagger} | 76.43 | 72.32 | 74.32 ||\n|| [che2013named] {\\dagger} | 77.71 | 72.51 | 75.02 ||\n|| [yang2016combining] {\\dagger} | 72.98 | [BOLD] 80.15 | 76.40 ||\n|| LEMON {\\dagger} | 79.27 | 78.29 | [BOLD] 78.78 ||\n|| [zhang2018lattice] | 76.35 | 71.56 | 73.88 ||\n|| LEMON | [BOLD] 80.61 | 71.05 | [BOLD] 75.53 ||",
    "claim": "LEMON achieved the highest F1-score of 78.78% in the table.",
    "label": "support"
  },
  {
    "id": "training_568_support",
    "table_caption": "Table: Comparison of WERs for Hybrid (streamable), LAS (e2e), RNN-T (e2e & streamable) and Transformer Transducer models (e2e & streamable) on LibriSpeech test sets.",
    "table_content": "|| Model | Param size | No LM (%) clean | No LM (%) other | With LM (%) clean | With LM (%) other ||\n|| Hybrid  | - | - | - | 2.26 | 4.85 ||\n|| LAS | 361M | 2.8 | 6.8 | 2.5 | 5.8 ||\n|| BiLSTM RNN-T | 130M | 3.2 | 7.8 | - | - ||\n|| FullAttn T-T (Ours) | 139M | 2.4 | 5.6 | [BOLD] 2.0 | [BOLD] 4.6 ||",
    "claim": "FullAttn T-T achieves the lowest WER with LM on both clean (2.0%) and other (4.6%) datasets.",
    "label": "support"
  },
  {
    "id": "training_503_support",
    "table_caption": "Table: Rouge-N limited-length recall on DUC2004. Size denotes the size of decoder vocabulary in a model.",
    "table_content": "|| Models | Size | Rouge-1 | Rouge-2 | Rouge-L ||\n|| ZOPIARY (Zajic et al. ( 2004 )) | - | 25.12 | 6.46 | 20.12 ||\n|| ABS (Rush et al. ( 2015 )) | 69K | 26.55 | 7.06 | 23.49 ||\n|| ABS+ (Rush et al. ( 2015 )) | 69K | 28.18 | 8.49 | 23.81 ||\n|| RAS-LSTM (Chopra et al. ( 2016 )) | 69K | 27.41 | 7.69 | 23.06 ||\n|| RAS-Elman (Chopra et al. ( 2016 )) | 69K | 28.97 | 8.26 | 24.06 ||\n|| big-words-lvt2k-1sent (Nallapati et al. ( 2016 )) | 69K | 28.35 | [BOLD] 9.46 | 24.59 ||\n|| big-words-lvt5k-1sent (Nallapati et al. ( 2016 )) | 200K | 28.61 | 9.42 | 25.24 ||\n|| Ours-GRU (C) | 15K | 29.08 | 9.20 | 25.25 ||\n|| Ours-LSTM (C) | 15K | [BOLD] 29.89 | 9.37 | 25.93 ||\n|| Ours-Opt-2 (C) | 15K | 29.74 | 9.44 | [BOLD] 25.94 ||",
    "claim": "\"Ours-LSTM (C)\" achieves the highest ROUGE-1 score, \"big-words-lvt2k-1sent\" achieves the highest ROUGE-2 score, and \"Ours-Opt-2 (C)\" achieves the highest ROUGE-L score.",
    "label": "support"
  },
  {
    "id": "training_508_support",
    "table_caption": "Table: Monolingual (Conll↔Univ) performance. SV∗ is used for computing the Avg values.",
    "table_content": "|| [EMPTY] | Sup UAS | Sup LAS | Cas UAS | Cas LAS | MTL UAS | MTL LAS ||\n|| [EMPTY] | Monolingual (Conll→Univ) | Monolingual (Conll→Univ) | Monolingual (Conll→Univ) | Monolingual (Conll→Univ) | Monolingual (Conll→Univ) | Monolingual (Conll→Univ) ||\n|| DE | 84.24 | 78.40 | 85.02 | 80.05 | [BOLD] 85.73 | [BOLD] 80.64 ||\n|| ES | 85.31 | 81.23 | [BOLD] 85.90 | [BOLD] 81.73 | 85.80 | 81.45 ||\n|| PT | 88.40 | 86.54 | 89.12 | 87.32 | [BOLD] 89.40 | [BOLD] 87.60 ||\n|| SV | 84.91 | 79.88 | 87.17 | 82.83 | [BOLD] 87.27 | [BOLD] 83.52 ||\n|| SV∗ | 82.61 | 77.42 | [BOLD] 85.39 | 80.60 | 85.29 | [BOLD] 81.22 ||\n|| 1-7 Avg | [ITALIC] 85.14 | [ITALIC] 80.90 | [ITALIC] 86.35 | [ITALIC] 82.43 | [ITALIC] 86.56 | [ITALIC] 82.73 ||\n|| [EMPTY] | Monolingual (Univ→Conll) | Monolingual (Univ→Conll) | Monolingual (Univ→Conll) | Monolingual (Univ→Conll) | Monolingual (Univ→Conll) | Monolingual (Univ→Conll) ||\n|| DE | 89.06 | 86.48 | 89.64 | 86.66 | [BOLD] 89.98 | [BOLD] 87.50 ||\n|| ES | 85.41 | 80.50 | [BOLD] 86.46 | 81.37 | 86.07 | [BOLD] 81.41 ||\n|| PT | [BOLD] 90.16 | [BOLD] 85.53 | 89.50 | 85.03 | 89.98 | 85.23 ||\n|| SV | 88.49 | 81.98 | 89.07 | 82.91 | [BOLD] 91.60 | [BOLD] 85.22 ||\n|| SV∗ | 79.61 | 72.71 | 82.91 | 74.96 | [BOLD] 84.86 | [BOLD] 77.36 ||\n|| 1-7 Avg | [ITALIC] 86.06 | [ITALIC] 81.31 | [ITALIC] 87.13 | [ITALIC] 82.01 | [ITALIC] 87.72 | [ITALIC] 82.88 ||",
    "claim": "MTL systems outperform the supervised baselines in both conditions.",
    "label": "support"
  },
  {
    "id": "training_583_support",
    "table_caption": "Table: BLEU scores for English-to-German translation for different beam sizes and feature sets: CTC score (c), language model (l), ratio of the blank symbols (r), and the number of trailing blank symbols (t).",
    "table_content": "|| Beam Size | 1 | 5 | 10 | 20 ||\n|| [ITALIC] c+ [ITALIC] l+ [ITALIC] r+ [ITALIC] t | 22.68 | 25.50 | 25.93 | 26.03 ||\n|| [ITALIC] c+ [ITALIC] l+ [ITALIC] r | 22.21 | 24.92 | 25.12 | 25.35 ||\n|| [ITALIC] c+ [ITALIC] l | 22.05 | 24.64 | 24.77 | 25.12 ||\n|| [ITALIC] c | 21.67 | 22.06 | 22.13 | 22.17 ||",
    "claim": "Combining features improves performance, with more substantial improvements observed at larger beam sizes.",
    "label": "support"
  },
  {
    "id": "training_488_support",
    "table_caption": "Table: F1 for different model variants on the binary WSJ validation set with included punctuation. The binary trees are as-is (∅) or modified according to the post-processing heuristic (+PP). The mean F1 is shown across three random seeds.",
    "table_content": "|| [BOLD] Composition | [BOLD] Loss | F1 [ITALIC] μ ∅ | F1 [ITALIC] μ +PP ||\n|| TreeLSTM | Margin | 49.9 | 53.1 ||\n|| TreeLSTM | Softmax | 52.0 | 52.9 ||\n|| MLP | Margin | 49.7 | 54.4 ||\n|| MLP | Softmax | 52.6 | 55.5 ||\n|| MLPKernel | Softmax | 51.8 | 54.8 ||\n|| MLPShared | Softmax | 50.8 | 56.7 ||",
    "claim": "MLP composition performs better than TreeLSTM, MLP benefits from the Softmax loss, and the best performance is achieved with MLPShared using Softmax.",
    "label": "support"
  },
  {
    "id": "training_643_support",
    "table_caption": "Table: Prealigned Data (CzEng 1.0); Experiment: Effectiveness",
    "table_content": "|| [BOLD] Recall (%) | 63.02 ||\n|| [BOLD] Precision (%) | 93.74 ||",
    "claim": "63.02% of all existing pairs of parallel sentences were detected, and 93.74% of all detected pairs were correct.",
    "label": "support"
  },
  {
    "id": "training_564_support",
    "table_caption": "Table: POS tagging accuracy using word-based and char-based encoder/decoder representations.",
    "table_content": "|| [EMPTY] | POS Accuracy ENC | POS Accuracy DEC | BLEU Ar-En | BLEU En-Ar ||\n|| Word | 89.62 | 86.71 | 24.69 | 13.37 ||\n|| Char | 95.35 | 91.11 | 28.42 | 13.00 ||",
    "claim": "Char-based representations achieve higher POS accuracy and better BLEU scores for Arabic-to-English translation compared to word-based representations.",
    "label": "support"
  },
  {
    "id": "training_627_support",
    "table_caption": "Table: Result on SNLI. Here, * stands for multimodal NLU models.",
    "table_content": "|| [BOLD] Models | [BOLD] Test ||\n|| *GroundSent 2018:NAACL_sentence_visual | 76.1 ||\n|| *Picturebook hinton:2018 | 86.5 ||\n|| ESIM + ELMo SNLI_ELMO | 88.7 ||\n|| 300D DMAN SNLI_MAN | 88.8 ||\n|| SLRC SNLI_SLRC | 89.1 ||\n|| LMTransformer SNLI_2 | 89.9 ||\n|| MT-DNN SNLI_1 | 91.1 ||\n|| BERT | 91.2 ||\n|| BERT + STVF | [BOLD] 91.3 ||",
    "claim": "BERT + STVF achieves the highest performance on the SNLI dataset with a score of 91.3%.",
    "label": "support"
  },
  {
    "id": "training_536_support",
    "table_caption": "Table: Confusion Matrix of Empirical Test",
    "table_content": "|| Empirical Test | Actual Value Human-Written | Actual Value RevGAN+PD ||\n|| Human | 119 | 61 ||\n|| Machine | 118 | 62 ||",
    "claim": "The empirical test results show that the actual values for human-written reviews and RevGAN+PD generated reviews are very close, with human reviews at 119 and machine reviews at 118.",
    "label": "support"
  },
  {
    "id": "training_478_support",
    "table_caption": "Table: Class-wise Performance (in Terms of F{}_{1}-score) of Our RCN Model and Baseline Methods",
    "table_content": "|| Method | Input | r:modifier (e1,e2) | r:negative (e2,e1) | r:position (e1,e2) | r:percentage (e1,e2) | r:percentage (e2,e1) ||\n|| CNN + MaxPooling [nguyen2015relation] | Word + Position | 96.33 | [BOLD] 99.54 | 95.45 | 80.60 | 97.53 ||\n|| CNN + MaxPooling [nguyen2015relation] | Word + Position + Entity Type* | 96.28 | [BOLD] 99.54 | [BOLD] 96.21 | 80.00 | 98.77 ||\n|| BiLSTM + MaxPooling [zhang2015relation] | Word Only | 95.33 | 99.09 | 95.35 | 73.85 | 95.65 ||\n|| BiLSTM + MaxPooling [zhang2015relation] | Word + Entity Type* | 96.66 | 99.09 | 94.62 | 81.16 | 96.20 ||\n|| BiLSTM + Attention [zhou2016attention] | Word Only | 94.87 | 99.09 | 93.44 | 73.85 | 93.17 ||\n|| BiLSTM + Attention [zhou2016attention] | Word + Entity Type* | [BOLD] 97.15 | 99.09 | 93.85 | [BOLD] 83.87 | 98.14 ||\n|| CRNN + MaxPooling [raj2017learning] | Word Only | 95.28 | 98.62 | 93.54 | 77.61 | 98.16 ||\n|| CRNN + MaxPooling [raj2017learning] | Word + Entity Type* | 96.23 | [BOLD] 99.54 | 94.70 | 80.00 | 96.34 ||\n|| CRNN + Attention [raj2017learning] | Word Only | 94.77 | 98.64 | 93.94 | 81.69 | 96.89 ||\n|| CRNN + Attention [raj2017learning] | Word + Entity Type* | 96.50 | [BOLD] 99.54 | 94.62 | 80.60 | 98.16 ||\n|| [BOLD] Our RCN model | Word Only | 96.35 | [BOLD] 99.54 | 93.08 | 76.47 | 98.14 ||\n|| [BOLD] Our RCN model | Word + Entity Type* | [BOLD] 97.15 | [BOLD] 99.54 | 94.74 | 82.35 | [BOLD] 99.38 ||\n|| The entity type features are proposed by us. | The entity type features are proposed by us. | The entity type features are proposed by us. | The entity type features are proposed by us. | The entity type features are proposed by us. | [EMPTY] | [EMPTY] ||",
    "claim": "Our RCN model achieves the highest performance in r:modifier (e1,e2), r:negative (e2,e1), and r:percentage (e2,e1) relations, and the second-highest in r:percentage (e1,e2) relation.",
    "label": "support"
  },
  {
    "id": "training_611_support",
    "table_caption": "Table: Results of streaming Transformer models. The number of layers is 12.",
    "table_content": "|| Model | IC | Size (M) | [ITALIC] N | [ITALIC] dk | Context | dev ||\n|| [EMPTY] | ✓ | 50.5 | 16 | 512 | [-∞,∞] | 18.8 ||\n|| [EMPTY] | ✓ | 50.5 | 16 | 512 | [-∞,16] | 20.6 ||\n|| [EMPTY] | ✓ | 50.5 | 16 | 512 | [-∞,28] | 20.7 ||\n|| [EMPTY] | ✓ | 50.5 | 16 | 512 | [-∞,40] | 20.0 ||\n|| Transformer | ✗ | 53.5 | 8 | 624 | [-∞,∞] | 18.4 ||\n|| [EMPTY] | ✗ | 53.5 | 8 | 624 | [-∞,4] | 23.0 ||\n|| [EMPTY] | ✗ | 53.5 | 8 | 624 | [-∞,16] | 21.1 ||\n|| [EMPTY] | ✗ | 53.5 | 8 | 624 | [-∞,28] | 21.8 ||\n|| [EMPTY] | ✗ | 53.5 | 8 | 624 | [-∞,40] | 19.8 ||\n|| Transformer-XL | ✓ | 50.5 | 16 | 512 | [-40, 40] | 20.4 ||\n|| [EMPTY] | ✗ | 53.5 | 8 | 624 | [-40, 40] | 21.0 ||\n|| BLSTM | – | 55.0 | – | – | [−∞,∞] | 19.5 ||\n|| LC-BLSTM | – | 55.0 | – | – | [-1, 40] | 20.2 ||",
    "claim": "The Transformer with an attention mask of [−∞,40] achieves a development set performance of 20.0, while Transformer-XL with a context size of [−40,40] achieves 20.4.",
    "label": "support"
  },
  {
    "id": "training_575_support",
    "table_caption": "Table: Pipeline ablative analysis of GoldEn Retriever end-to-end QA performance by replacing each query generator with a query oracle.",
    "table_content": "|| [BOLD] System | [BOLD] Ans F1 | [BOLD] Sup F1 | [BOLD] Joint F1 ||\n|| GoldEn Retriever | 49.79 | 64.58 | 40.21 ||\n|| w/ Hop 1 oracle | 52.53 | 68.06 | 42.68 ||\n|| w/ Hop 1 & 2 oracles | 62.32 | 77.00 | 52.18 ||",
    "claim": "Using both Hop 1 and 2 oracles results in the highest performance across all metrics (Ans F1, Sup F1, Joint F1).",
    "label": "support"
  },
  {
    "id": "training_595_support",
    "table_caption": "Table: Results on the MSRA dataset",
    "table_content": "|| [BOLD] Model | [BOLD] P (%) | [BOLD] R (%) | [BOLD] F1 (%) ||\n|| [chen2006chinese] | 91.22 | 81.71 | 86.20 ||\n|| [zhang2006sighan] | 92.20 | 90.08 | 91.18 ||\n|| [lu2016multi] | - | - | 87.94 ||\n|| [dong2016character] | 91.28 | 90.62 | 90.95 ||\n|| [zhang2018lattice] | 93.57 | [BOLD] 92.79 | 93.18 ||\n|| LEMON | [BOLD] 95.39 | 91.77 | [BOLD] 93.55 ||",
    "claim": "LEMON achieved the highest F1-score among the models listed in the table.",
    "label": "support"
  },
  {
    "id": "training_636_support",
    "table_caption": "Table: Clustering errors by model",
    "table_content": "|| Model | REP | REP-FR | REP-FR-DE | REP-DE | REP-POS | REP-DE-POS ||\n|| Error | 51 | 26 | 24 | 22 | 8 | 0 ||",
    "claim": "The REP-DE-POS model has the lowest clustering error of 0, while the REP model has the highest error of 51.",
    "label": "support"
  },
  {
    "id": "training_596_support",
    "table_caption": "Table: Results on the Resume NER dataset",
    "table_content": "|| [BOLD] Model | [BOLD] P (%) | [BOLD] R (%) | [BOLD] F1 (%) ||\n|| word{\\dagger} | 93.72 | 93.44 | 93.58 ||\n|| word+char+bichar{\\dagger} | 94.07 | 94.42 | 94.24 ||\n|| char {\\dagger} | 93.66 | 93.31 | 93.48 ||\n|| char+bichar+softword{\\dagger} | 94.53 | [BOLD] 94.29 | 94.41 ||\n|| [zhang2018lattice] | 94.81 | 94.11 | 94.46 ||\n|| LEMON | [BOLD] 95.59 | 94.07 | [BOLD] 94.82 ||",
    "claim": "LEMON achieved the highest F1-score among the models listed in the table.",
    "label": "support"
  },
  {
    "id": "training_546_support",
    "table_caption": "Table: Cross-corpora generalization accuracy using TransfBig generator and UniT discriminator (except for the last row which used a bidirectional transformer). Each row specifies the corpora used at training time, Ctrain. Each column shows the corpus used at test time, Ctest.",
    "table_content": "|| train corpora | test corpora Books | test corpora CCNews | test corpora Wiki ||\n|| Wiki | 70.9 | 73.6 | 76.4 ||\n|| Books | 91.7 | 63.5 | 59.1 ||\n|| Books + Wiki | 91.5 | 73.6 | 78.3 ||\n|| CCNews | 60.6 | 88.4 | 65.5 ||\n|| Books + CCNews | 90.4 | 88.5 | 68.3 ||\n|| CCNews + Wiki | 73.5 | 88.3 | 81.0 ||\n|| ALL (UniT) | 90.4 | 88.5 | 80.9 ||\n|| ALL (BiT) | 94.1 | 94.1 | - ||",
    "claim": "When testing on Wikitext, a discriminator trained with Books achieves an accuracy of 59.1%, and with CCNews, it achieves 65.5%. Training on the union of two corpora provides a large benefit over training on just one when testing on the third.",
    "label": "support"
  },
  {
    "id": "training_671_support",
    "table_caption": "Table: Time Usage Comparison",
    "table_content": "|| Model | CPU(seconds) | GPU (milliseconds) ||\n|| Uncompressed | 2.7 | 38 ||\n|| HashNet | 80.6 | - ||\n|| SE | 0.7 | 25 ||",
    "claim": "On CPU, HashNet is slower than the Uncompressed model, and the SE model is faster than both.",
    "label": "support"
  },
  {
    "id": "training_529_support",
    "table_caption": "Table: Zero-Shot performance on all French test sets. ∗Note that we use the English accuracy in the bridged column for SST.",
    "table_content": "|| [BOLD] Model | Amazon (Fr) Bridged | Amazon (Fr) Zero-Shot | SST (Fr) Bridged∗ | SST (Fr) Zero-Shot | SNLI (Fr) Bridged | SNLI (Fr) Zero-Shot ||\n|| Proposed model:  [ITALIC] Encoder-Classifier | 73.30 | 51.53 | 79.63 | 59.47 | 74.41 | 37.62 ||\n|| + Pre-trained Encoder | 79.23 | 75.78 | 84.18 | 81.05 | 80.65 | 72.35 ||\n|| + Freeze Encoder | 83.10 | 81.32 | 84.51 | 83.14 | 81.26 | 73.88 ||",
    "claim": "Using a pre-trained encoder increases zero-shot performance significantly, and freezing the encoder further improves it. On the Amazon Review task, the zero-shot system is within 2% of the best bridged system, and on the SST task, it achieves an accuracy of 83.14%, within 1.5% of the bridged system.",
    "label": "support"
  },
  {
    "id": "training_626_support",
    "table_caption": "Table: Result of SemEval 2018 Task 11 (MCScript) on different train/test image bases.",
    "table_content": "|| [BOLD] Train Base | [BOLD] Test Base | [BOLD] Test ||\n|| MSCOCO | MSCOCO | 83.12 ||\n|| MSCOCO | Flicker 30k | 83.09 ||\n|| Flicker 30k | MSCOCO | 82.81 ||\n|| Flicker 30k | Flicker 30k | 82.81 ||",
    "claim": "The model achieves higher performance when trained and tested on the MSCOCO base compared to when trained and tested on the Flicker 30k base.",
    "label": "support"
  },
  {
    "id": "training_586_support",
    "table_caption": "Table: PDP and PCFP results for all languages and models, averaged over 4 runs. Metrics are defined in § 3.3. An refers to the Analogy metric and LE to the Lexicon Expansion metric.",
    "table_content": "|| [EMPTY] | Cells | Paradigms | [BOLD] PDP F_{\\mathrm{cell}} | [BOLD] PDP F_{\\mathrm{par}} | [BOLD] PDP F_{\\mathrm{grid}} | [BOLD] PCFP An | [BOLD] PCFP LE ||\n|| [BOLD] Arabic nouns – 8,732 forms | [BOLD] Arabic nouns – 8,732 forms | [BOLD] Arabic nouns – 8,732 forms | [BOLD] Arabic nouns – 8,732 forms | [BOLD] Arabic nouns – 8,732 forms | [BOLD] Arabic nouns – 8,732 forms | [BOLD] Arabic nouns – 8,732 forms | [BOLD] Arabic nouns – 8,732 forms ||\n|| sup | 27 | 4,283 | [EMPTY] | [EMPTY] | [EMPTY] | 85.9 | 87.0 ||\n|| bench | 12.8 | 5,279.3 | 39.9 | 48.5 | 43.7 | 16.8 | 49.5 ||\n|| gold k | 27 | 4,930.3 | 25.9 | 46.4 | 33.1 | 16.1 | 57.2 ||\n|| [BOLD] German nouns – 19,481 forms | [BOLD] German nouns – 19,481 forms | [BOLD] German nouns – 19,481 forms | [BOLD] German nouns – 19,481 forms | [BOLD] German nouns – 19,481 forms | [BOLD] German nouns – 19,481 forms | [BOLD] German nouns – 19,481 forms | [BOLD] German nouns – 19,481 forms ||\n|| sup | 8 | 17,018 | [EMPTY] | [EMPTY] | [EMPTY] | 72.2 | 74.9 ||\n|| bench | 7.3 | 17,073.3 | 35.2 | 59.4 | 43.3 | 14.2 | 56.7 ||\n|| gold k | 8 | 16,836.0 | 29.4 | 66.6 | 40.8 | 14.8 | 60.4 ||\n|| [BOLD] English verbs – 3,330 forms | [BOLD] English verbs – 3,330 forms | [BOLD] English verbs – 3,330 forms | [BOLD] English verbs – 3,330 forms | [BOLD] English verbs – 3,330 forms | [BOLD] English verbs – 3,330 forms | [BOLD] English verbs – 3,330 forms | [BOLD] English verbs – 3,330 forms ||\n|| sup | 5 | 1,801 | [EMPTY] | [EMPTY] | [EMPTY] | 80.4 | 80.7 ||\n|| bench | 7.5 | 1,949.5 | 64.0 | 80.1 | 71.1 | 52.0 | 67.5 ||\n|| gold k | 5 | 1,977.3 | 79.6 | 82.1 | 80.8 | 54.7 | 69.4 ||\n|| [BOLD] Latin nouns – 6,903 forms | [BOLD] Latin nouns – 6,903 forms | [BOLD] Latin nouns – 6,903 forms | [BOLD] Latin nouns – 6,903 forms | [BOLD] Latin nouns – 6,903 forms | [BOLD] Latin nouns – 6,903 forms | [BOLD] Latin nouns – 6,903 forms | [BOLD] Latin nouns – 6,903 forms ||\n|| sup | 12 | 3,013 | [EMPTY] | [EMPTY] | [EMPTY] | 80.0 | 88.0 ||\n|| bench | 13.0 | 3,746.5 | 38.8 | 73.2 | 50.6 | 17.2 | 72.9 ||\n|| gold k | 12 | 3,749.0 | 39.9 | 71.6 | 51.3 | 17.5 | 72.6 ||\n|| [BOLD] Russian nouns – 36,321 forms | [BOLD] Russian nouns – 36,321 forms | [BOLD] Russian nouns – 36,321 forms | [BOLD] Russian nouns – 36,321 forms | [BOLD] Russian nouns – 36,321 forms | [BOLD] Russian nouns – 36,321 forms | [BOLD] Russian nouns – 36,321 forms | [BOLD] Russian nouns – 36,321 forms ||\n|| sup | 14 | 14,502 | [EMPTY] | [EMPTY] | [EMPTY] | 94.7 | 96.8 ||\n|| bench | 16.5 | 19,792.0 | 44.5 | 72.2 | 55.0 | 31.9 | 86.2 ||\n|| gold k | 14 | 20,944.0 | 45.7 | 69.1 | 55.0 | 31.6 | 84.3 ||",
    "claim": "Clustering assigns lexicon forms to paradigms (46–82%) more accurately than to cells (26–80%).",
    "label": "support"
  },
  {
    "id": "training_621_support",
    "table_caption": "Table: Splits of the Benchmark Evaluation Parallel Data",
    "table_content": "|| [BOLD] Type | [BOLD] Sent pairs | [BOLD] Sources ||\n|| [ITALIC] Igbo-English | 5,836 | https://www.bbc.com/igbo ||\n|| [ITALIC] English-Igbo | 5,748 | Mostly from local newspapers (e.g. Punch) ||\n|| [ITALIC] Total | 11,584 | [EMPTY] ||",
    "claim": "There are 5,836 Igbo-English sentence pairs sourced from https://www.bbc.com/igbo and 5,748 English-Igbo sentence pairs mostly from local newspapers.",
    "label": "support"
  },
  {
    "id": "training_625_support",
    "table_caption": "Table: Results on six most frequent question types.",
    "table_content": "|| [BOLD] Models | [BOLD] y/n | [BOLD] what | [BOLD] why | [BOLD] who | [BOLD] where | [BOLD] when ||\n|| TriAN | 81.4 | 85.4 | [BOLD] 84.9 | 89.7 | 84.7 | 78.9 ||\n|| Integrated | [BOLD] 81.6 | [BOLD] 86.3 | 84.4 | 89.7 | [BOLD] 85.3 | 78.9 ||",
    "claim": "The Integrated model outperforms TriAN in the \"y/n,\" \"what,\" and \"where\" question categories.",
    "label": "support"
  },
  {
    "id": "training_610_support",
    "table_caption": "Table: Results of the 12-layer Transformer model with different number of attention heads. VGG net was used as the encoding layer for all the Transformers. N denotes the number of attention heads, and dk is the model dimension as in Eq(1).",
    "table_content": "|| Model | IC | Size (M) | [ITALIC] N | [ITALIC] dk | dev ||\n|| [EMPTY] | ✓ | 50.5 | 4 | 512 | 19.6 ||\n|| [EMPTY] | ✓ | 50.5 | 8 | 512 | 19.7 ||\n|| [EMPTY] | ✓ | 50.5 | 16 | 512 | 18.8 ||\n|| Transformer | ✗ | 52.0 | 4 | 620 | 19.4 ||\n|| [EMPTY] | ✗ | 53.5 | 8 | 624 | 18.4 ||\n|| [EMPTY] | ✗ | 53.5 | 16 | 624 | 18.6 ||\n|| BLSTM | – | 55.0 | – | – | 19.5 ||",
    "claim": "The Transformer with interleaved convolution and 16 attention heads achieved the lowest WER of 18.8, while the vanilla Transformer with 8 attention heads achieved a WER of 18.4.",
    "label": "support"
  },
  {
    "id": "training_648_support",
    "table_caption": "Table: Performance of our model when using one and two layers",
    "table_content": "|| Entity | Two layers Pre. | Two layers Rec. | Two layers  [ITALIC] F1 | One layer Pre. | One layer Rec. | One layer  [ITALIC] F1 ||\n|| LOC | 83.63 | 82.48 | 83.05 | 82.22 | 80.64 | 81.41 ||\n|| MISC | 84.14 | 78.37 | 81.07 | 85.15 | 74.29 | 79.32 ||\n|| ORG | 49.85 | 50.51 | 50.07 | 44.10 | 40.88 | 42.39 ||\n|| PER | 72.77 | 65.73 | 69.06 | 72.70 | 62.15 | 66.91 ||\n|| ALL | 75.88 | 72.26 | [BOLD] 74.02 | 74.83 | 68.91 | 71.74 ||",
    "claim": "The performance increases from 71.74% to 74.02% when using two layers of Bi-LSTM.",
    "label": "support"
  },
  {
    "id": "training_535_support",
    "table_caption": "Table: Comparison of Experimental Results on Amazon Review Dataset (** stands for significance under 99% confidence, * stands for 95% confidence)",
    "table_content": "|| Models | Log-Likelihood | WMD | PPL | BLEU-4(%) | ROUGE-L(%) ||\n|| SeqGAN | -86699 | 1.869 | 22.60 | 15.06 | 38.30 ||\n|| LeakGAN | -108581 | 2.324 | 24.09 | 14.98 | 37.73 ||\n|| RankGAN | -73309 | 1.862 | 22.45 | 14.92 | 37.72 ||\n|| charRNN | -100430 | 1.976 | 22.07 | 11.46 | 33.60 ||\n|| MLE | -54338 | 2.106 | 17.15 | 9.62 | 31.89 ||\n|| Attr2Seq | -56298 | 2.077 | 21.00 | 11.48 | 32.22 ||\n|| RevGAN+CD | -80386 | 2.097 | 19.71 | 21.32 | 39.47 ||\n|| RevGAN+CD+SA | -51549 | 2.030 | 17.45 | 24.44 | 41.32 ||\n|| RevGAN+CD+SA+PD | [BOLD] -34305** | [BOLD] 1.762** | [BOLD] 17.00* | [BOLD] 27.16** | [BOLD] 44.63** ||",
    "claim": "RevGAN+CD+SA+PD achieves the best performance across all metrics: Log-Likelihood, WMD, PPL, BLEU-4, and ROUGE-L.",
    "label": "support"
  },
  {
    "id": "training_639_support",
    "table_caption": "Table: ABX errors on the ZeroResource Speech Challenge 2017 (120s). Within- (“W.”) and across-speaker (“A.”) phoneme discriminability scores on English, French and Mandarin speech for CPC features with and without data augmentation. For comparison, the best systems plus supervised topline of the ZeroSpeech leaderboard trained on the provided datasets.",
    "table_content": "|| [EMPTY] | English W. | English A. | French W. | French A. | Mandarin W. | Mandarin A. | AVG ||\n|| [ITALIC] Trained on ZeroSpeech2017 (45h, 24h, 2h30, resp.) | [ITALIC] Trained on ZeroSpeech2017 (45h, 24h, 2h30, resp.) | [ITALIC] Trained on ZeroSpeech2017 (45h, 24h, 2h30, resp.) | [ITALIC] Trained on ZeroSpeech2017 (45h, 24h, 2h30, resp.) | [ITALIC] Trained on ZeroSpeech2017 (45h, 24h, 2h30, resp.) | [ITALIC] Trained on ZeroSpeech2017 (45h, 24h, 2h30, resp.) | [ITALIC] Trained on ZeroSpeech2017 (45h, 24h, 2h30, resp.) | [EMPTY] ||\n|| Superv. topline [dunbar2017] | 5.3 | 6.9 | 6.8 | 9.1 | 4.2 | 5.7 | 6.33 ||\n|| Heck et al. [heck2017] | 6.2 | 8.7 | 8.7 | 11.7 | 7.9 | [BOLD] 7.4 | 8.43 ||\n|| Chorow. et al. [chor2019] | 5.5 | 8.0 | [BOLD] 7.5 | [BOLD] 10.8 | 10.7 | 11.2 | 8.95 ||\n|| CPC2 | 8.6 | 12.0 | 12.2 | 16.4 | 12.0 | 14.0 | 12.53 ||\n|| CPC2+WavAug | 6.6 | 9.3 | 9.3 | 14.1 | 11.2 | 11.9 | 10.4 ||\n|| [ITALIC] Trained on out-of-domain (100h, 76h, 80h, resp.) | [ITALIC] Trained on out-of-domain (100h, 76h, 80h, resp.) | [ITALIC] Trained on out-of-domain (100h, 76h, 80h, resp.) | [ITALIC] Trained on out-of-domain (100h, 76h, 80h, resp.) | [ITALIC] Trained on out-of-domain (100h, 76h, 80h, resp.) | [ITALIC] Trained on out-of-domain (100h, 76h, 80h, resp.) | [ITALIC] Trained on out-of-domain (100h, 76h, 80h, resp.) | [EMPTY] ||\n|| CPC2 | 6.1 | 8.7 | 10.3 | 12.9 | 9.3 | 9.6 | 9.48 ||\n|| CPC2+WavAug | 4.7 | 6.5 | 8.6 | 11.1 | 7.9 | 7.8 | 7.77 ||\n|| CPC2-3L+WavAug | [BOLD] 4.6 | [BOLD] 5.8 | 7.6 | 10.9 | [BOLD] 7.8 | 8.0 | [BOLD] 7.45 ||",
    "claim": "CPC2-3L+WavAug achieves the best average score when trained on out-of-domain datasets.",
    "label": "support"
  },
  {
    "id": "training_686_support",
    "table_caption": "Table: Results on SST dataset. For each dataset, we conduct significance test against the best reproducible model, and * means that the improvement is significant at 0.05 significance level.",
    "table_content": "|| Model | SST | SST-B ||\n|| Lai ET AL., 2015 | 47.21 | - ||\n|| Zhou ET AL., 2015 | 49.20 | 87.80 ||\n|| Liu ET AL., 2016 | 49.60 | 87.90 ||\n|| Tai ET AL., 2016 | 51.00 | 88.00 ||\n|| Kumar ET AL., 2016 | [BOLD] 52.10 | [BOLD] 88.60 ||\n|| 24-layers Transformer | 49.37 | 86.66 ||\n|| ENAS-macro | 51.55 | [BOLD] 88.90 ||\n|| ENAS-micro | 47.00 | 87.52 ||\n|| DARTS | [BOLD] 51.65 | 87.12 ||\n|| SMASH | 46.65 | 85.94 ||\n|| One-Shot | 50.37 | 87.08 ||\n|| Random Search | 49.20 | 87.15 ||\n|| TextNAS | [BOLD] 52.51 | [BOLD] 90.33∗ ||",
    "claim": "TextNAS achieves the highest accuracy on both the SST and SST-B datasets.",
    "label": "support"
  },
  {
    "id": "training_556_support",
    "table_caption": "Table: Effect of sinusoidal positional encoding. Perplexity after 5 epochs (13M updates) for (L, 2048, 512, 8) models.",
    "table_content": "|| [ITALIC] L | Position. encoding | Params. in M. | Perplexity Train | Perplexity Dev | Perplexity Test ||\n|| 12 | Sinusoidal | 243 | 61.8 | 63.1 | 66.1 ||\n|| 12 | None | 243 | 58.0 | [BOLD] 60.5 | [BOLD] 63.4 ||\n|| 24 | Sinusoidal | 281 | 55.6 | 58.0 | 60.8 ||\n|| 24 | None | 281 | 52.7 | [BOLD] 56.6 | [BOLD] 59.2 ||\n|| 42 | Sinusoidal | 338 | 51.2 | 55.0 | 57.7 ||\n|| 42 | None | 338 | 50.5 | [BOLD] 54.2 | [BOLD] 56.8 ||",
    "claim": "Models without positional encoding have lower perplexity scores across train, dev, and test sets compared to models with sinusoidal encoding.",
    "label": "support"
  },
  {
    "id": "training_602_support",
    "table_caption": "Table: Distribution of Assertion classes in the data.",
    "table_content": "|| [BOLD] Class | [BOLD] Model  [BOLD] Baseline | [BOLD] Model  [BOLD] Baseline | [BOLD] Model  [BOLD] Scope Localization model | [BOLD] Model  [BOLD] Scope Localization model ||\n|| [BOLD] Class | [BOLD] Dataset-I | [BOLD] Dataset-II | [BOLD] Dataset-I | [BOLD] Dataset-II ||\n|| Present | 0.97 | 0.92 | 0.90 | 0.84 ||\n|| Absent | 0.27 | 0.34 | 0.84 | 0.93 ||\n|| Conditional | 0.39 | 0.45 | 0.74 | 0.65 ||\n|| Hypothetical | 0.76 | 0.69 | 0.87 | 0.75 ||\n|| Possibility | 0.0 | 0.07 | 0.0 | 0.13 ||\n|| AWSE | 0.42 | 0.39 | 0.60 | 0.0 ||\n|| None | 0.81 | 0.89 | 0.96 | 0.95 ||\n|| Macro | 0.52 | 0.53 | 0.70 | 0.61 ||",
    "claim": "The Model Scope Localization model outperforms the Model Baseline for the Absent class in both Dataset-I and Dataset-II.",
    "label": "support"
  },
  {
    "id": "training_577_support",
    "table_caption": "Table: Document retrieval evaluation on dev set (%). (’-’ denotes a missing value)",
    "table_content": "|| [BOLD] Model | [BOLD] OFEVER ||\n|| Athene | [BOLD] 93.55 ||\n|| UCL MRG | - ||\n|| UNC NLP | 92.82 ||\n|| Our Model | 93.33 ||",
    "claim": "Athene achieves the highest OFEVER score of 93.55.",
    "label": "support"
  },
  {
    "id": "training_541_support",
    "table_caption": "Table: Parsing accuracy on the testing data of WSJ. EX indicates the exact match score.",
    "table_content": "|| Model | dev (all) F1 | test≤40 F1 | test≤40 EX | test (all) F1 | test (all) EX ||\n|| LVG-G-16 | [EMPTY] | [EMPTY] | [EMPTY] | 88.70 | 35.80 ||\n|| LVG-D-16 | [EMPTY] | [EMPTY] | [EMPTY] | 89.30 | [BOLD] 39.40 ||\n|| Multi-Scale | [EMPTY] | 89.70 | 39.60 | 89.20 | 37.20 ||\n|| Berkeley Parser | [EMPTY] | 90.60 | 39.10 | 90.10 | 37.10 ||\n|| CVG (SU-RNN) | 91.20 | 91.10 | [EMPTY] | 90.40 | [EMPTY] ||\n|| GM-LVeG-S | [BOLD] 91.24 | [BOLD] 91.38 | [BOLD] 41.51 | [BOLD] 91.02 | 39.24 ||",
    "claim": "GM-LVeG-S produces the best F1 scores on both the development data and the testing data, surpassing the Berkeley parser by 0.92% in F1 score on the testing data. Its exact match score on the testing data is slightly lower than that of LVG-D-16.",
    "label": "support"
  },
  {
    "id": "training_644_support",
    "table_caption": "Table: Cross-Dataset performance, comparing detector training using baseline to augmented-baseline training sets",
    "table_content": "|| Trainset- Testset | Accuracy Baseline | Accuracy Augmented | Accuracy (%) | Precision Baseline | Precision Augmented | Precision (%) | Recall Baseline | Recall Augmented | Recall (%) | F1 Baseline | F1 Augmented | F1 (%) ||\n|| FN-SE | 0.613 | 0.645 | +5.22 | 0.689 | 0.570 | -17.27 | 0.155 | 0.644 | +315.48 | 0.253 | 0.605 | +139.13 ||\n|| FN-WH | 0.846 | 0.850 | +0.47 | 0.636 | 0.528 | -16.98 | 0.063 | 0.507 | +704.76 | 0.114 | 0.517 | +353.50 ||\n|| DV-WH | 0.794 | 0.820 | +3.27 | 0.278 | 0.441 | +58.63 | 0.189 | 0.507 | +168.25 | 0.225 | 0.472 | +109.77 ||\n|| DV-WS | 0.652 | 0.832 | +27.60 | 0.471 | 0.870 | +84.71 | 0.107 | 0.599 | +459.81 | 0.174 | 0.709 | +307.47 ||\n|| SE-WS | 0.662 | 0.754 | +13.89 | 0.875 | 0.872 | -0.34 | 0.017 | 0.331 | +1,847.06 | 0.034 | 0.480 | +1,311.76 ||\n|| SE-FN | 0.926 | 0.924 | -0.21 | 0.694 | 0.489 | -29.53 | 0.031 | 0.226 | +629.03 | 0.059 | 0.309 | +423.72 ||\n|| WH-SE | 0.584 | 0.593 | +1.54 | 0.572 | 0.522 | -8.74 | 0.069 | 0.450 | +552.17 | 0.124 | 0.484 | +290.32 ||\n|| WH-DV | 0.752 | 0.777 | +3.32 | 0.714 | 0.610 | -14.56 | 0.053 | 0.364 | +586.79 | 0.099 | 0.456 | +360.60 ||\n|| WS-WH | 0.749 | 0.824 | +10.01 | 0.332 | 0.450 | +35.54 | 0.579 | 0.489 | -15.54 | 0.422 | 0.469 | +11.13 ||\n|| WS-FN | 0.822 | 0.895 | +8.88 | 0.135 | 0.233 | +72.59 | 0.255 | 0.176 | -30.98 | 0.177 | 0.201 | +13.55 ||\n|| Average | 0.740 | 0.791 | [BOLD] +6.95 | 0.539 | 0.559 | [BOLD] +3.50 | 0.151 | 0.429 | [BOLD] +182.81 | 0.168 | 0.470 | [BOLD] +179.71 ||",
    "claim": "The average metrics of all cross-dataset pairs reveal a consistent increase in accuracy (+6.9%), precision (+3.5%), recall (+182.8%), and F1 (+179.7%) with augmented training sets.",
    "label": "support"
  },
  {
    "id": "training_682_support",
    "table_caption": "Table: Performance for paraphrase identification on the Quora dataset.",
    "table_content": "|| Models | Accuracy ||\n|| Siamese-CNN | 79.60 ||\n|| Multi-Perspective-CNN | 81.38 ||\n|| Siamese-LSTM | 82.58 ||\n|| Multi-Perspective-LSTM | 83.21 ||\n|| L.D.C. | 85.55 ||\n|| BiMPM | [BOLD] 88.17 ||",
    "claim": "The \"BiMPM\" model outperforms the \"L.D.C.\" model by more than two percent.",
    "label": "support"
  },
  {
    "id": "training_498_support",
    "table_caption": "Table: Syntactic parsing performance (in unlabeled attachment score, UAS) and DM semantic parsing performance (in labeled F1) on different groups of the development data. Both systems predict the same syntactic parses for instances from Same, and they disagree on instances from Diff (§5).",
    "table_content": "|| [BOLD] Split | [BOLD] # Sent. | [BOLD] Model | [BOLD] UAS | [BOLD] DM ||\n|| Same | 1011 | pipeline | 97.4 | 94.0 ||\n|| Same | 1011 | spigot | 97.4 | 94.3 ||\n|| Diff | 681 | pipeline | 91.3 | 88.1 ||\n|| Diff | 681 | spigot | 89.6 | 89.2 ||",
    "claim": "For the Same condition, both pipeline and spigot models achieve a UAS of 97.4, with spigot having a slightly higher DM score of 94.3 compared to pipeline's 94.0. For the Diff condition, the pipeline model achieves a UAS of 91.3 and a DM of 88.1, while the spigot model has a UAS of 89.6 and a DM of 89.2.",
    "label": "support"
  },
  {
    "id": "training_604_support",
    "table_caption": "Table: Discourse classification task performance: Unweighted average F1 across discourse markers on the test set, and overall accuracy. Ngram-bow is a bag-of-words model built on mixture of ngram features. GloVe-bow averages word embedding with correction to frequency Arora et al. (2017). BiLSTM is the DisSent sentence encoder model. BERT is finetuned on all of the DisSent tasks.",
    "table_content": "|| Model | All F1 | All Acc | Books 8 F1 | Books 8 Acc | Books 5 F1 | Books 5 Acc ||\n|| GloVe-bow | 17.1 | 41.8 | 27.6 | 47.3 | 41.7 | 52.5 ||\n|| Ngram-bow | 28.1 | 51.8 | 44.0 | 58.1 | 54.1 | 63.3 ||\n|| BiLSTM | 47.2 | 67.5 | 64.4 | 73.5 | 72.1 | 77.3 ||\n|| BERT | 60.1 | 77.5 | 76.2 | 82.9 | 82.6 | 86.1 ||",
    "claim": "BERT achieves the highest F1 and accuracy scores across all datasets compared to GloVe-bow, Ngram-bow, and BiLSTM.",
    "label": "support"
  },
  {
    "id": "training_665_support",
    "table_caption": "Table: Study on the effect of different smoothing methods for word weights generation. Baseline is the same as before. w.w. without smoothing means the word weights (w.w.) are computed without smoothing in the log domain. w.w. (mean smooth.) indicates smoothing the word scores via using a mean average filter before thresholding and w.w. (gauss. smooth.) indicates using a normal distributed filter before thresholding. The approaches regarding different smoothing methods are described in Section 2.2.",
    "table_content": "|| System | BLEU [%] | TER [%] ||\n|| Baseline | 24.37 | 61.66 ||\n|| +w.w. without smooth. | 21.38 | 66.25 ||\n|| +w.w. (mean smooth.) | 25.99 | 60.70 ||\n|| +w.w. (gauss. smooth.) | 26.14 | 60.34 ||",
    "claim": "The system with word weights using Gaussian smoothing achieves the highest BLEU score of 26.14% and the lowest TER score of 60.34%.",
    "label": "support"
  },
  {
    "id": "training_664_support",
    "table_caption": "Table: OCR performances for different languages with overall CRR, total Insertion, Deletion and Substitution errors.",
    "table_content": "|| [BOLD] Language | [BOLD] Bhagavad Gītā CRR | [BOLD] Bhagavad Gītā Ins | [BOLD] Bhagavad Gītā Del | [BOLD] Bhagavad Gītā Sub | [BOLD] Sahaśranāma CRR | [BOLD] Sahaśranāma Ins | [BOLD] Sahaśranāma Del | [BOLD] Sahaśranāma Sub | [BOLD] Combined CRR ||\n|| English | 84.92 | 23 | 63 | 1868 | 64.06 | 73 | 696 | 1596 | 80.08 ||\n|| French | 84.90 | 21 | 102 | 1710 | 63.91 | 91 | 702 | 1670 | 80.04 ||\n|| Finnish | 82.61 | 15 | 141 | 1902 | 61.31 | 80 | 730 | 1821 | 78.81 ||\n|| Italian | 83.45 | 20 | 73 | 1821 | 62.19 | 84 | 690 | 1673 | 79.03 ||\n|| Irish | 84.52 | 12 | 78 | 1810 | 63.81 | 72 | 709 | 1841 | 79.93 ||\n|| German | 84.40 | 33 | 72 | 1821 | 63.79 | 87 | 723 | 1874 | 79.12 ||",
    "claim": "English and French have the highest CRR among the listed languages.",
    "label": "support"
  },
  {
    "id": "training_672_support",
    "table_caption": "Table: Perplexity results for single models on BillionW. Bold number denotes results on a single GPU.",
    "table_content": "|| Model | Perplexity | #P[Billions] ||\n|| Interpolated Kneser-Ney 5-gram  | 67.6 | 1.76 ||\n|| 4-layer IRNN-512  | 69.4 | [EMPTY] ||\n|| RNN-2048 + BlackOut sampling  | [BOLD] 68.3 | [EMPTY] ||\n|| RNN-1024 + MaxEnt 9-gram  | 51.3 | 20 ||\n|| LSTM-2048-512  | 43.7 | 0.83 ||\n|| LightRNN  | [BOLD] 66.0 | 0.041 ||\n|| LSTM-2048-512  | 43.7 | 0.83 ||\n|| 2-layer LSTM-8192-1024  | 30.6 | 1.8 ||\n|| 2-layer LSTM-8192-1024 + CNN inputs  | 30.0 | 1.04 ||\n|| 2-layer LSTM-8192-1024 + CNN inputs + CNN softmax  | 39.8 | 0.29 ||\n|| LSTM-2048 Adaptive Softmax  | [BOLD] 43.9 | >0.29 ||\n|| 2-layer LSTM-2048 Adaptive Softmax  | [BOLD] 39.8 | [EMPTY] ||\n|| GCNN-13  | [BOLD] 38.1 | [EMPTY] ||\n|| MOE  | 28.0 | >4.37 ||\n|| SE (2-layer 2048 LSTM NCE) | [BOLD] 39.9 | 0.32 ||\n|| SE (3-layer 2048 LSTM NCE) | [BOLD] 39.5 | 0.25 ||\n|| SE (3-layer 2048 LSTM IS ) | [BOLD] 38.3 | 0.25 ||",
    "claim": "The SE (3-layer 2048 LSTM IS) model achieves a perplexity of 38.3.",
    "label": "support"
  },
  {
    "id": "training_585_support",
    "table_caption": "Table: Performance of different models. ∗Moses simply ignore the unknown words.",
    "table_content": "|| Data | Model | Informativeness ROUGE-1(%) | Informativeness ROUGE-2(%) | Text Quality PPL | Text Quality Length | Text Quality UNK(%) | Text Quality Copy(%) ||\n|| Summarization | LEAD | 28.1 | 14.1 | 176 | 19.9 | 0 | 100 ||\n|| Summarization | Moses | 27.8 | 14.1 | 214 | 73.0 | 0∗ | 99.6 ||\n|| Summarization | ABS | 28.1 | 12.4 | 113 | 13.7 | 0.88 | 92.0 ||\n|| Summarization | CoRe | [BOLD] 30.5 | [BOLD] 16.2 | [BOLD] 95 | 14.0 | 0.14 | 88.6 ||\n|| Simplification | LEAD | 66.4 | 49.4 | 66.5 | 20.8 | 0 | 100 ||\n|| Simplification | Moses | 70.9 | 52.1 | 70.3 | 24.4 | 0∗ | 97.6 ||\n|| Simplification | ABS | 68.4 | 50.3 | 69.5 | 22.7 | 5.6 | 87.7 ||\n|| Simplification | CoRe | [BOLD] 72.7 | [BOLD] 55.3 | [BOLD] 60.9 | 19.6 | 2.3 | 85.9 ||",
    "claim": "CoRe achieves the highest ROUGE scores and the lowest PPL values on both summarization and text simplification tasks.",
    "label": "support"
  },
  {
    "id": "training_683_support",
    "table_caption": "Table: Performance for natural language inference on the SNLI dataset.",
    "table_content": "|| Models | Accuracy ||\n||  | 77.6 ||\n||  | 81.4 ||\n||  | 82.1 ||\n||  | 83.5 ||\n||  | 85.0 ||\n||  | 85.1 ||\n||  | 86.1 ||\n||  | 86.3 ||\n||  | 86.8 ||\n||  | 87.3 ||\n||  | 87.5 ||\n||  (Single) | 87.7 ||\n||  (Ensemble) | 88.3 ||\n|| Only  [ITALIC] P→ [ITALIC] Q | 85.6 ||\n|| Only  [ITALIC] P← [ITALIC] Q | 86.3 ||\n|| BiMPM | 86.9 ||\n|| BiMPM (Ensemble) | [BOLD] 88.8 ||",
    "claim": "\"Only P←Q\" has higher accuracy than \"Only P→Q\", and the \"BiMPM\" model has higher accuracy than \"Only P←Q\". The \"BiMPM (Ensemble)\" achieves the highest accuracy.",
    "label": "support"
  },
  {
    "id": "training_637_support",
    "table_caption": "Table: Table S3: Additive noise augmented CPC, ABX errors (Libri-light dev set). Within- and across-speaker phoneme discriminability scores (lower is better) on the Libri-light clean and other dev sets for CPC training as a function of varying types of additive noise augmentation.",
    "table_content": "|| [EMPTY] | Within spk. dev | Within spk. dev | Across spk. dev | Across spk. dev ||\n|| System | clean | other | clean | other ||\n|| MFCC Baseline | 10.95 | 13.55 | 20.94 | 29.41 ||\n|| CPC LL-60k | 6.11 | 8.17 | 8.05 | 12.83 ||\n|| [ITALIC] CPC2 – Trained on LibriSpeech clean 80h | [ITALIC] CPC2 – Trained on LibriSpeech clean 80h | [ITALIC] CPC2 – Trained on LibriSpeech clean 80h | [ITALIC] CPC2 – Trained on LibriSpeech clean 80h | [EMPTY] ||\n|| no augmentation | 6.06 | 8.18 | 7.59 | 12.8 ||\n|| [ITALIC] Band pass – Musan – past only | [ITALIC] Band pass – Musan – past only | [ITALIC] Band pass – Musan – past only | [ITALIC] Band pass – Musan – past only | [EMPTY] ||\n|| no filtering | 5.81 | 7.40 | 8.03 | 12.7 ||\n|| [0,80] Hz | 5.55 | 7.56 | 6.82 | 12.0 ||\n|| [80,240] Hz | 5.38 | 7.58 | 6.99 | 12.1 ||\n|| [240,720] Hz | 6.22 | 8.32 | 7.89 | 12.9 ||\n|| [720,2160] Hz | 6.71 | 9.11 | 8.52 | 13.8 ||\n|| [2160,8000] Hz | 6.64 | 8.74 | 8.30 | 13.4 ||\n|| [ITALIC] Band pass – Musan – past + future | [ITALIC] Band pass – Musan – past + future | [ITALIC] Band pass – Musan – past + future | [ITALIC] Band pass – Musan – past + future | [EMPTY] ||\n|| no filtering | 6.52 | 8.79 | 8.20 | 13.5 ||\n|| [0,80] Hz | 5.28 | 7.48 | 6.83 | 12.1 ||\n|| [80,240] Hz | [BOLD] 5.16 | [BOLD] 7.33 | [BOLD] 6.77 | [BOLD] 11.7 ||\n|| [240,720] Hz | 6.01 | 8.36 | 7.45 | 12.9 ||\n|| [720,2160] Hz | 7.40 | 9.83 | 9.06 | 14.2 ||\n|| [2160,8000] Hz | 7.40 | 9.72 | 9.00 | 14.2 ||",
    "claim": "The optimal additive noise performance is achieved by bandpass filtering MUSAN sounds in the [80,240] Hz range, as it results in the lowest error rates across all conditions.",
    "label": "support"
  },
  {
    "id": "training_668_support",
    "table_caption": "Table: Link prediction results on WN18RR dataset.",
    "table_content": "|| [BOLD] Metric | [BOLD] TransE | [BOLD] DistMult | [BOLD] ComplEx | [BOLD] ConvE | [BOLD] RotatE | [BOLD] GNTP | [BOLD] CBR ||\n|| hits@1 | - | 0.39 | 0.41 | 0.40 | [BOLD] 0.43 | 0.41 | 0.38 ||\n|| hits@3 | - | 0.44 | 0.46 | 0.44 | [BOLD] 0.49 | 0.44 | 0.46 ||\n|| hits@10 | 0.50 | 0.49 | 0.51 | 0.52 | [BOLD] 0.57 | 0.48 | 0.51 ||\n|| MRR | 0.23 | 0.43 | 0.44 | 0.43 | [BOLD] 0.48 | 0.43 | 0.43 ||",
    "claim": "RotatE achieves the highest scores across all metrics compared to other models.",
    "label": "support"
  },
  {
    "id": "training_679_support",
    "table_caption": "Table: Results on the CELEX dataset",
    "table_content": "|| [EMPTY] | 13SIA | 2PIE | 2PKE | rP | Avg. ||\n|| med Kann and Schütze ( 2016a ) | 83.9 | 95 | 87.6 | 84 | 87.62 ||\n|| nwfst Rastogi et al. ( 2016 ) | 86.8 | 94.8 | 87.9 | 81.1 | 87.65 ||\n|| lat Dreyer et al. ( 2008 ) | [BOLD] 87.5 | 93.4 | 87.4 | 84.9 | 88.3 ||\n|| Soft | 83.1 | 93.8 | 88 | 83.2 | 87 ||\n|| Hard | 85.8 | [BOLD] 95.1 | [BOLD] 89.5 | [BOLD] 87.2 | [BOLD] 89.44 ||",
    "claim": "The hard attention model achieves the highest overall accuracy (89.44) compared to the soft attention model and other models in the table.",
    "label": "support"
  },
  {
    "id": "training_538_support",
    "table_caption": "Table: Results comparison with average and standard deviation of five runs. “ANS-only” and “SUP-only” indicate the model is only trained on two separate tasks.",
    "table_content": "|| [EMPTY] | ANS-only EM | ANS-only  [ITALIC] F1 | SUP-only EM | SUP-only  [ITALIC] F1 ||\n|| baseline | 63.87±0.16 | 77.69±0.16 | 62.14±0.16 | 88.94±0.07 ||\n|| GSN | 64.39±0.06 | 78.27±0.10 | 62.96±0.14 | 89.29±0.07 ||",
    "claim": "GSN achieves higher ANS-only EM, ANS-only F1, SUP-only EM, and SUP-only F1 scores compared to the baseline.",
    "label": "support"
  },
  {
    "id": "training_594_support",
    "table_caption": "Table: Results on the OntoNotes-4 development set with different features",
    "table_content": "|| [BOLD] Features \\ Data | [BOLD] Features \\ Data | [BOLD] P (%)  [BOLD] Ground truth | [BOLD] R (%)  [BOLD] Ground truth | [BOLD] F1 (%)  [BOLD] Ground truth | [BOLD] P (%)  [BOLD] Automatically labelled | [BOLD] R (%)  [BOLD] Automatically labelled | [BOLD] F1 (%)  [BOLD] Automatically labelled ||\n|| [BOLD] NCRF | [BOLD] char | 66.37 | 60.21 | 63.14 | - | - | - ||\n|| [BOLD] NCRF | [BOLD] char + seg | 70.58 | 69.96 | 70.27 | 70.77 | 63.33 | 66.85 ||\n|| [BOLD] NCRF | [BOLD] char + pos | 71.81 | 74.48 | 73.12 | 70.20 | [BOLD] 70.26 | 70.23 ||\n|| [BOLD] NCRF | [BOLD] char + seg + pos | 75.63 | 72.35 | 73.08 | 72.88 | 68.18 | 70.45 ||\n|| [BOLD] No Lex | [BOLD] char | 67.6 | 55.03 | 60.67 | - | - | - ||\n|| [BOLD] No Lex | [BOLD] char + seg | 72.16 | 66.09 | 68.99 | 70.48 | 62.65 | 66.33 ||\n|| [BOLD] No Lex | [BOLD] char + pos | 74.39 | 65.44 | 69.63 | 72.87 | 63.73 | 67.99 ||\n|| [BOLD] No Lex | [BOLD] char + seg + pos | 74.97 | 72.23 | 73.58 | 76.29 | 64.43 | 69.86 ||\n|| [BOLD] Lex | [BOLD] char | 77.27 | 60.73 | 68.01 | - | - | - ||\n|| [BOLD] Lex | [BOLD] char + seg | 78.40 | 70.75 | 74.38 | 76.11 | 64.63 | 69.91 ||\n|| [BOLD] Lex | [BOLD] char + pos | 77.71 | 72.35 | 74.93 | 77.46 | 66.89 | 71.79 ||\n|| [BOLD] Lex | [BOLD] char + seg + pos | [BOLD] 78.70 | [BOLD] 74.95 | [BOLD] 76.78 | [BOLD] 76.41 | 68.61 | [BOLD] 72.30 ||",
    "claim": "Features derived from word segmentation and POS-tagging improve the performance of all models.",
    "label": "support"
  },
  {
    "id": "training_651_support",
    "table_caption": "Table: Experimental results for comparative paragraph generation on the proposed dataset. For human captions, mean and standard deviation are given for a one-vs-rest scheme across twenty-five runs. We observed that CIDEr-D scores had little correlation with description quality. The Neural Naturalist model benefits from a strong joint encoding and Transformer-based comparative module, achieving the highest BLEU-4 and ROUGE-L scores.",
    "table_content": "|| [EMPTY] | Dev BLEU-4 | Dev ROUGE-L | Dev CIDEr-D | Test BLEU-4 | Test ROUGE-L | Test CIDEr-D ||\n|| Most Frequent | 0.20 | 0.31 | [BOLD] 0.42 | 0.20 | 0.30 | [BOLD] 0.43 ||\n|| Text-Only | 0.14 | 0.36 | 0.05 | 0.14 | 0.36 | 0.07 ||\n|| Nearest Neighbor | 0.18 | 0.40 | 0.15 | 0.14 | 0.36 | 0.06 ||\n|| CNN + LSTM Vinyals et al. ( 2015 ) | 0.22 | 0.40 | 0.13 | 0.20 | 0.37 | 0.07 ||\n|| CNN + Attn. + LSTM Xu et al. ( 2015 ) | 0.21 | 0.40 | 0.14 | 0.19 | 0.38 | 0.11 ||\n|| Neural Naturalist – Simple Joint Encoding | 0.23 | 0.44 | 0.23 | - | - | - ||\n|| Neural Naturalist – No Comparative Module | 0.09 | 0.27 | 0.09 | - | - | - ||\n|| Neural Naturalist – Small Decoder | 0.22 | 0.42 | 0.25 | - | - | - ||\n|| Neural Naturalist – Full | [BOLD] 0.24 | [BOLD] 0.46 | 0.28 | [BOLD] 0.22 | [BOLD] 0.43 | 0.25 ||\n|| Human | 0.26 +/- 0.02 | 0.47 +/- 0.01 | 0.39 +/- 0.04 | 0.27 +/- 0.01 | 0.47 +/- 0.01 | 0.42 +/- 0.03 ||",
    "claim": "Neural Naturalist – Full achieves the highest Dev BLEU-4 and Dev ROUGE-L scores among the models listed, with Test BLEU-4 and Test ROUGE-L scores also being the highest among non-human models.",
    "label": "support"
  },
  {
    "id": "training_544_support",
    "table_caption": "Table: Validation and test perplexity on CC-News and Toronto Book Corpus. * denotes models initialized with RoBERTa trained on additional data. The joint model perplexity ranges are estimated using 100,000 samples, see Eq. 5. The number of parameters of each model is shown in parentheses.",
    "table_content": "|| Model (#parameters) | CC-News Val | CC-News Test | Toronto Book Corpus Val | Toronto Book Corpus Test ||\n|| base LM (203M) | 18.41 | 17.57 | 16.16 | 18.29 ||\n|| RALM (LM+203M) | 17.01 | 16.17 | 15.71 | 17.85 ||\n|| BALM (408M) | 16.50 | 15.74 | 15.00 | 16.99 ||\n|| joint UniT (LM+203M) | 16.42-16.44 | 15.57-15.58 | 15.12-15.13 | 16.98-17.00 ||\n|| joint BiT-Base (LM+125M) | 15.32-15.35 | 14.61-14.64 | - | - ||\n|| joint BiT-Base* (LM+125M) | 15.40-15.46 | 14.75-14.76 | 14.63-14.63 | 16.36-16.37 ||\n|| joint BiT-Base* (LM+125M) | 15.11-15.17 | 14.37-14.42 | 14.14-14.16 | 15.72-15.74 ||\n|| joint BiT-Large* (LM+355M) | [BOLD] 14.59- [BOLD] 14.61 | [BOLD] 13.97- [BOLD] 14.00 | [BOLD] 13.80- [BOLD] 13.83 | [BOLD] 15.33- [BOLD] 15.36 ||\n|| Base LM-24L (203M) | 15.71 | 14.89 | 15.61 | 18.14 ||\n|| RALM (LM-24L+203M) | 15.70 | 14.89 | 15.63 | 18.17 ||\n|| BALM-24L (408M) | 14.58 | 13.92 | 15.20 | 18.24 ||\n|| joint UniT (LM-24L+203M) | 14.59-14.61 | 13.81-13.82 | 15.12−15.16 | 17.46-17.48 ||\n|| joint BiT-Base (LM-24L+125M) | 13.68-13.69 | 13.01-13.03 | - | - ||\n|| joint BiT-Base* (LM-24L+125M) | 13.60-13.62 | 12.93-12.95 | 14.11-14.12 | 16.17-16.18 ||\n|| joint BiT-Med (LM-24L+203M) | 12.97-13.01 | 12.38-12.42 | - | - ||\n|| joint BiT-Large* (LM-24L+355M) | [BOLD] 12.71- [BOLD] 12.77 | [BOLD] 12.10- [BOLD] 12.16 | [BOLD] 13.30- [BOLD] 13.34 | [BOLD] 15.17- [BOLD] 15.22 ||",
    "claim": "Joint BiT-Large* achieves the best performance across both datasets, and joint BiT-Base* outperforms joint UniT with fewer parameters.",
    "label": "support"
  },
  {
    "id": "training_615_support",
    "table_caption": "Table: Performance breakdown of NABERT\\textscLARGE and MTMSN\\textscLARGE by gold answer types.",
    "table_content": "|| Type | (%) | NABERT EM | NABERT F1 | MTMSN EM | MTMSN F1 ||\n|| Date | 1.6 | 55.7 | 60.8 | 55.7 | 69.0 ||\n|| Number | 61.9 | 63.8 | 64.0 | 80.9 | 81.1 ||\n|| Single Span | 31.7 | 75.9 | 80.6 | 77.5 | 82.8 ||\n|| Multi Span | 4.8 | 0 | 22.7 | 25.1 | 62.8 ||",
    "claim": "The F1 score in the multi-span category increases by more than 40 points from NABERT to MTMSN.",
    "label": "support"
  },
  {
    "id": "training_670_support",
    "table_caption": "Table: Link prediction results on NELL-995 for few shot relations.",
    "table_content": "|| [BOLD] Model | hits@1 | hits@10 | MRR ||\n|| NeuralLP yang2017differentiable | 0.048 | 0.351 | 0.179 ||\n|| NTP- [ITALIC] λ ntp | 0.102 | 0.334 | 0.155 ||\n|| MINERVA das2018go | 0.162 | 0.283 | 0.201 ||\n|| MultiHop(DistMult) LinRX2018:MultiHopKG | 0.145 | 0.306 | 0.200 ||\n|| MultiHop(ConvE) LinRX2018:MultiHopKG | 0.178 | 0.329 | 0.231 ||\n|| Meta-KGR(DistMult) lv2019adapting | 0.197 | 0.345 | 0.248 ||\n|| Meta-KGR(ConvE) lv2019adapting | 0.197 | 0.347 | 0.253 ||\n|| CBR (ours) | [BOLD] 0.234 | [BOLD] 0.403 | [BOLD] 0.293 ||",
    "claim": "CBR achieves the highest performance in hits@1, hits@10, and MRR compared to other models.",
    "label": "support"
  },
  {
    "id": "training_687_support",
    "table_caption": "Table: Detailed settings for experiments of text classification.",
    "table_content": "|| Exp | batch size | max length | [ITALIC] l2 | lr | sliding window | hidden size ||\n|| AG | 128 | 256 | 1×10−6 | 0.02 | no | 256 ||\n|| Sogou | 64 | 1024 | 1×10−6 | 0.02 | yes | 32 ||\n|| DBP | 128 | 256 | 1×10−6 | 0.02 | no | 64 ||\n|| Yelp-B | 128 | 512 | 1×10−6 | 0.02 | no | 64 ||\n|| Yelp | 128 | 512 | 1×10−6 | 0.02 | no | 64 ||\n|| Yahoo | 64 | 1024 | 1×10−6 | 0.02 | yes | 32 ||\n|| Amz | 128 | 256 | 1×10−6 | 0.02 | yes | 128 ||\n|| Amz-B | 128 | 256 | 1×10−6 | 0.02 | yes | 128 ||",
    "claim": "The sliding window technique is used in the Sogou, Yahoo, Amz, and Amz-B experiments.",
    "label": "support"
  },
  {
    "id": "training_599_support",
    "table_caption": "Table: Distribution of Assertion classes in the data.",
    "table_content": "|| [BOLD] Class | [BOLD] Dataset-I  [BOLD] Train | [BOLD] Dataset-I  [BOLD] Val | [BOLD] Dataset-I  [BOLD] Test | [BOLD] Dataset-II  [BOLD] Train | [BOLD] Dataset-II  [BOLD] Val | [BOLD] Dataset-II  [BOLD] Test ||\n|| Present | 3711 | 511 | 524 | 17407 | 2215 | 2452 ||\n|| Absent | 596 | 73 | 73 | 6136 | 708 | 805 ||\n|| Conditional | 169 | 31 | 19 | 393 | 44 | 49 ||\n|| Hypothetical | 147 | 22 | 18 | 69 | 10 | 5 ||\n|| Possibility | 62 | 5 | 11 | 219 | 37 | 25 ||\n|| AWSE | 15 | 3 | 2 | 21 | 4 | 2 ||",
    "claim": "The present class has the highest number of instances across both Dataset-I and Dataset-II, while the AWSE class has the fewest instances.",
    "label": "support"
  },
  {
    "id": "training_598_support",
    "table_caption": "Table: Comparison between our method, our Joint Neural Embedding (JNE)(Marín et al., 2018) and AdaMine (Carvalho et al., 2018) re-implementation. For all models we were using selected matching pairs generated by reducing noisy instruction sentences as described above. Recall rates are averaged over the evaluation batches.",
    "table_content": "|| Image to Recipe | Image to Recipe | Image to Recipe MedR | Image to Recipe R@1 | Image to Recipe R@5 | Image to Recipe R@10 ||\n|| 1k samples | Random (Marín et al.,  2018 ) | 500.0 | 0.001 | 0.005 | 0.01 ||\n|| 1k samples | JNE (Marín et al.,  2018 ) | 5.0±0.1 | 25.9 | 52.6 | 64.1 ||\n|| 1k samples | AdaMine (Carvalho et al.,  2018 ) | 3.0±0.1 | 33.1 | 64.3 | 75.2 ||\n|| 1k samples | IA | 2.9±0.3 | 34.6 | 66.0 | 76.6 ||",
    "claim": "IA achieves the best performance with the lowest median rank (2.9) and the highest recall percentages (R@1: 34.6, R@5: 66.0, R@10: 76.6) among the models evaluated.",
    "label": "support"
  },
  {
    "id": "training_649_support",
    "table_caption": "Table: Performance of our model when adding more features",
    "table_content": "|| Features | Pre. | Rec. | [ITALIC] F1 ||\n|| Word | 75.88 | 72.26 | 74.02 ||\n|| Word+POS | 84.23 | 87.64 | 85.90 ||\n|| Word+Chunk | 90.73 | 83.18 | 86.79 ||\n|| Word+Case | 83.68 | 84.45 | 84.06 ||\n|| Word+Regex | 76.58 | 71.86 | 74.13 ||\n|| Word+POS+Chunk+Case+Regex | 90.25 | 92.55 | 91.39 ||\n|| Word+POS+Chunk+Regex | 91.09 | 93.03 | [BOLD] 92.05 ||",
    "claim": "Using only word features results in an F1 score of 74.02%, while combining Word+POS+Chunk+Regex achieves the highest F1 score of 92.05%.",
    "label": "support"
  },
  {
    "id": "training_675_support",
    "table_caption": "Table: Performance on the test portion of the robust image captioning split on COCO dataset.",
    "table_content": "|| Method | BLEU4 | METEOR | CIDEr | SPICE | Accuracy ||\n|| Att2in  | 31.5 | 24.6 | 90.6 | 17.7 | 39.0 ||\n|| Up-Down  | 31.6 | 25.0 | 92.0 | 18.1 | 39.7 ||\n|| NBT | [BOLD] 31.7 | [BOLD] 25.2 | [BOLD] 94.1 | [BOLD] 18.3 | [BOLD] 42.4 ||\n|| NBToracle | 31.9 | 25.5 | 95.5 | 18.7 | 45.7 ||",
    "claim": "NBToracle has the highest scores across all metrics compared to other methods.",
    "label": "support"
  },
  {
    "id": "training_656_support",
    "table_caption": "Table: Performance of the individual groups of hand-crafted features.",
    "table_content": "|| [BOLD] Features | [BOLD] P | [BOLD] R | [BOLD] F1 | [BOLD] Acc ||\n|| Lexical | 75.53 | 74.59 | 75.02 | 79.89 ||\n|| Stylometric | 74.35 | 65.99 | 67.68 | 77.52 ||\n|| Grammatical | 73.23 | 50.60 | 42.99 | 71.48 ||\n|| Embeddings | 61.48 | 53.95 | 51.67 | 71.22 ||",
    "claim": "Among the hand-crafted features, lexical features yield the best results across all metrics.",
    "label": "support"
  },
  {
    "id": "training_660_support",
    "table_caption": "Table: Results of ML training on standard “test-1951”",
    "table_content": "|| model name | R-1 | R-2 | R-L | svar ||\n|| Summarization models | Summarization models | Summarization models | Summarization models | Summarization models ||\n|| ABS | 29.55 | 11.32 | 26.42 | - ||\n|| ABS+ | 29.76 | 11.88 | 26.96 | - ||\n|| Luong-NMT | 33.10 | 14.45 | 30.71 | - ||\n|| RAS-LSTM | 32.55 | 14.70 | 30.03 | - ||\n|| RAS-ELman | 33.78 | 15.97 | 31.15 | - ||\n|| seq2seq (our impl.) | 32.24 | 14.92 | 30.21 | 14.23 ||\n|| Length-control models | Length-control models | Length-control models | Length-control models | Length-control models ||\n|| LenLInit (our) | [BOLD] 30.47 | [BOLD] 13.35 | [BOLD] 28.40 | 2.15 ||\n|| LenInit | 29.97 | 13.03 | 28.07 | 2.11 ||\n|| LenMC (our) | 29.45 | 12.65 | 27.41 | 0.87 ||\n|| LenEmb | 28.83 | 11.89 | 26.92 | [BOLD] 0.85 ||",
    "claim": "LenLInit achieves the highest R-1, R-2, and R-L scores among the length-control models.",
    "label": "support"
  },
  {
    "id": "training_666_support",
    "table_caption": "Table: E-commerce English → Chinese BLEU results on test set. Baseline is trained on mixed in-domain and out-of-domain data. No. 2 is continuing training from baseline with objective defined as Eq. 1. No. 3 is continuing training from baseline with sentence-level weights and No. 4 is with word weights, as defined in Section 2.2. No. 5 refers to assigning wt using LCW method described in Section 2.3. No. 6 is equivalent to directly fine-tuning on in-domain datasets starting from the baseline model and No. 7 is equivalent to fine-tuning on in-domain datasets after No. 5 is finished.",
    "table_content": "|| No. | System description | Item descriptions BLEU [%] | Item descriptions TER [%] ||\n|| 1 | Baseline | 24.37 | 61.66 ||\n|| 2 | 1 + continue training without word weights | 24.31 | 61.69 ||\n|| 3 | 1 + continue training with sentence weights | 25.79 | 60.82 ||\n|| 4 | 1 + continue training with word weights | 26.14 | 60.34 ||\n|| 5 | 1 + continue training with chunk weights | 26.42 | 60.10 ||\n|| 6 | 1 + fine-tuning on in-domain | 26.06 | 59.93 ||\n|| 7 | 5 + fine-tuning on in-domain | 27.30 | 58.29 ||",
    "claim": "Fine-tuning on in-domain data improves the BLEU score from 24.37% to 26.06% and reduces the TER from 61.66% to 59.93%.",
    "label": "support"
  },
  {
    "id": "training_589_support",
    "table_caption": "Table: Avg. token counts in COCO captions and the first and last descriptions in PhotoBook, plus their cosine distance to the caption’s cluster mean vector. The distance between first and last descriptions is 0.083.",
    "table_content": "|| [BOLD] Source | [BOLD] # Tokens | [BOLD] # Content | [BOLD] Distance ||\n|| COCO captions | 11.167 | 5.255 | – ||\n|| First description | 9.963 | 5.185 | 0.091 ||\n|| Last description | 5.685 | 5.128 | 0.156 ||",
    "claim": "Last descriptions have fewer tokens than both COCO captions and first descriptions.",
    "label": "support"
  },
  {
    "id": "training_557_support",
    "table_caption": "Table: Test set performance for different training regimes and feature sets. The models use the same decoders for testing and training. For each setting, the average and standard deviation across 5 runs with different random initializations are reported. Boldface: best (averaged) result per dataset/measure.",
    "table_content": "|| Model | Training | Features | PTB UAS (%) | PTB UEM (%) | CTB UAS (%) | CTB UEM (%) ||\n|| Arc-standard | Local | { \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{2% .27622pt}{→←} [ITALIC] ts2, \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244% pt}{2.27622pt}{→←} [ITALIC] ts1, \\raisebox −1.422638 [ITALIC] pt\\resizebox% {7.682244pt}{2.27622pt}{→←} [ITALIC] ts0, \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{2.27622pt}{→←} [ITALIC] tb0} | 93.95±0.12 | 52.29±0.66 | 88.01±0.26 | 36.87±0.53 ||\n|| Arc-hybrid | Local | { \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{2% .27622pt}{→←} [ITALIC] ts2, \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244% pt}{2.27622pt}{→←} [ITALIC] ts1, \\raisebox −1.422638 [ITALIC] pt\\resizebox% {7.682244pt}{2.27622pt}{→←} [ITALIC] ts0, \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{2.27622pt}{→←} [ITALIC] tb0} | 93.89±0.10 | 50.82±0.75 | 87.87±0.17 | 35.47±0.48 ||\n|| Arc-hybrid | Local | { \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{2% .27622pt}{→←} [ITALIC] ts0, \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244% pt}{2.27622pt}{→←} [ITALIC] tb0} | 93.80±0.12 | 49.66±0.43 | 87.78±0.09 | 35.09±0.40 ||\n|| Arc-hybrid | Global | { \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{2% .27622pt}{→←} [ITALIC] ts0, \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244% pt}{2.27622pt}{→←} [ITALIC] tb0} | 94.43±0.08 | 53.03±0.71 | 88.38±0.11 | 36.59±0.27 ||\n|| Arc-eager | Local | { \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{2% .27622pt}{→←} [ITALIC] ts2, \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244% pt}{2.27622pt}{→←} [ITALIC] ts1, \\raisebox −1.422638 [ITALIC] pt\\resizebox% {7.682244pt}{2.27622pt}{→←} [ITALIC] ts0, \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{2.27622pt}{→←} [ITALIC] tb0} | 93.80±0.12 | 49.66±0.43 | 87.49±0.20 | 33.15±0.72 ||\n|| Arc-eager | Local | { \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{2% .27622pt}{→←} [ITALIC] ts0, \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244% pt}{2.27622pt}{→←} [ITALIC] tb0} | 93.77±0.08 | 49.71±0.24 | 87.33±0.11 | 34.17±0.41 ||\n|| Arc-eager | Global | { \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{2% .27622pt}{→←} [ITALIC] ts0, \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244% pt}{2.27622pt}{→←} [ITALIC] tb0} | [BOLD] 94.53±0.05 | 53.77±0.46 | [BOLD] 88.62±0.09 | [BOLD] 37.75±0.87 ||\n|| Edge-factored | Global | { \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{2% .27622pt}{→←} [ITALIC] th, \\raisebox −1.422638 [ITALIC] pt\\resizebox{7.682244pt}{% 2.27622pt}{→←} [ITALIC] tm} | 94.50±0.13 | [BOLD] 53.86±0.78 | 88.25±0.12 | 36.42±0.52 ||",
    "claim": "The Arc-eager model with global training achieves the highest PTB UAS, CTB UAS, and CTB UEM scores.",
    "label": "support"
  },
  {
    "id": "training_658_support",
    "table_caption": "Table: Performance of different models.",
    "table_content": "|| [BOLD] Feature Group | [BOLD] P | [BOLD] R | [BOLD] F1 | [BOLD] Acc ||\n|| Baseline | 35.61 | 50.00 | 41.59 | 71.22 ||\n|| TF.IDF | 75.53 | 74.59 | 75.02 | 79.89 ||\n|| AttNN | 78.52 | 78.74 | 78.63 | 81.99 ||\n|| TF.IDF &AttNN | 79,89 | 79.40 | 79.63 | 83.44 ||\n|| TF.IDF &Feats &AttNN | 80.07 | 79.49 | 79.77 | 83.57 ||",
    "claim": "The combination of TF.IDF, hand-crafted features, and attention mechanism embeddings (TF.IDF & Feats & AttNN) achieves the highest performance in terms of F1 score and accuracy.",
    "label": "support"
  },
  {
    "id": "training_678_support",
    "table_caption": "Table: Results on the Wiktionary datasets",
    "table_content": "|| [EMPTY] | DE-N | DE-V | ES-V | FI-NA | FI-V | FR-V | NL-V | Avg. ||\n|| durrettdenero2013 | 88.31 | 94.76 | 99.61 | 92.14 | 97.23 | 98.80 | 90.50 | 94.47 ||\n|| nicolai-cherry-kondrak:2015:NAACL-HLT | 88.6 | 97.50 | 99.80 | 93.00 | [BOLD] 98.10 | [BOLD] 99.20 | 96.10 | 96.04 ||\n|| faruquiTND15 | 88.12 | [BOLD] 97.72 | [BOLD] 99.81 | 95.44 | 97.81 | 98.82 | 96.71 | 96.34 ||\n|| yu2016online | 87.5 | 92.11 | 99.52 | 95.48 | [BOLD] 98.10 | 98.65 | 95.90 | 95.32 ||\n|| Soft | 88.18 | 95.62 | 99.73 | 93.16 | 97.74 | 98.79 | 96.73 | 95.7 ||\n|| Hard | [BOLD] 88.87 | 97.35 | 99.79 | [BOLD] 95.75 | 98.07 | 99.04 | [BOLD] 97.03 | [BOLD] 96.55 ||",
    "claim": "The hard attention model achieves the highest average performance across the datasets compared to other models.",
    "label": "support"
  },
  {
    "id": "training_704_support",
    "table_caption": "Table: Ablation of cross-entropy loss vs. cross-entropy+maxmargin loss for our BiDAF-based generative model (on dev set).",
    "table_content": "|| Models | recall@1 | recall@2 | recall@5 ||\n|| Cross-entropy (XE) | 13.12 | 23.45 | 54.78 ||\n|| XE+Max-margin | 15.61 | 27.39 | 57.02 ||",
    "claim": "XE+Max-margin achieves higher recall@1, recall@2, and recall@5 scores compared to Cross-entropy (XE).",
    "label": "support"
  },
  {
    "id": "training_614_support",
    "table_caption": "Table: Ablation tests of base and large models on the DROP dev set.",
    "table_content": "|| Model | BASE EM | BASE F1 | LARGE EM | LARGE F1 ||\n|| MTMSN | 68.2 | 72.8 | 76.7 | 80.5 ||\n|| w/o Add/Sub | 46.7 | 51.3 | 53.8 | 58.0 ||\n|| w/o Count | 62.5 | 66.4 | 71.8 | 75.6 ||\n|| w/o Negation | 59.4 | 63.6 | 67.2 | 70.9 ||\n|| w/o Multi-Span | 67.5 | 70.7 | 75.6 | 78.4 ||\n|| w/o Reranking | 66.9 | 71.2 | 74.9 | 78.7 ||",
    "claim": "The MTMSN model achieves higher EM and F1 scores compared to its ablated versions across both BASE and LARGE settings.",
    "label": "support"
  },
  {
    "id": "training_570_support",
    "table_caption": "Table: Limited right context per layer for audio encoder.",
    "table_content": "|| Audio Mask left | Audio Mask right | Label Mask left | WER (%) Test-clean | WER (%) Test-other ||\n|| 512 | 512 | 20 | 2.4 | 5.6 ||\n|| 512 | 10 | 20 | 2.7 | 6.6 ||\n|| 512 | 6 | 20 | 2.8 | 6.9 ||\n|| 512 | 2 | 20 | 3.0 | 7.7 ||\n|| 10 | 0 | 20 | 4.2 | 11.3 ||",
    "claim": "Using 2 frames of right context per layer results in a WER of 3.0% on Test-clean and 7.7% on Test-other, compared to 4.2% and 11.3% respectively for a streamable model with 0 frames of right context.",
    "label": "support"
  },
  {
    "id": "training_710_support",
    "table_caption": "Table: Conpono is more effective at classifying the most plausible sentence from the extended context than BERT-Base. We report the BERT-Large exact match score, where the model selects only the target entity from the context, for reference. All scores are on the validation set.",
    "table_content": "|| Model | Accuracy ||\n|| BERT-Base | 61.2 ||\n|| Conpono | 63.2 ||\n|| BERT-Large | 69.8 [EM] ||",
    "claim": "BERT-Large achieves the highest accuracy among the models, with an accuracy of 69.8.",
    "label": "support"
  },
  {
    "id": "training_684_support",
    "table_caption": "Table: Performance for answer sentence selection on TREC-QA and WikiQA datasets.",
    "table_content": "|| Models | TREC-QA MAP | TREC-QA MRR | WikiQA MAP | WikiQA MRR ||\n||  | 0.695 | 0.763 | 0.652 | 0.665 ||\n||  | 0.728 | 0.832 | – | – ||\n|| Wang and Itty. wang2015faq | 0.746 | 0.820 | – | – ||\n||  | 0.753 | 0.851 | 0.689 | 0.696 ||\n||  | – | – | 0.692 | 0.711 ||\n||  | – | – | 0.689 | 0.707 ||\n||  | 0.771 | 0.845 | 0.706 | 0.723 ||\n||  | 0.777 | 0.836 | 0.709 | 0.723 ||\n||  | 0.801 | [BOLD] 0.877 | 0.701 | 0.718 ||\n||  | – | – | 0.734 | 0.742 ||\n||  | – | – | [BOLD] 0.743 | [BOLD] 0.755 ||\n|| BiMPM | [BOLD] 0.802 | 0.875 | 0.718 | 0.731 ||",
    "claim": "BiMPM achieves the highest TREC-QA MAP score of 0.802 and the second-highest TREC-QA MRR score of 0.875.",
    "label": "support"
  },
  {
    "id": "training_674_support",
    "table_caption": "Table: Performance on the test portion of Karpathy et al. [20]’s splits on Flickr30k Entities dataset.",
    "table_content": "|| Method | BLEU1 | BLEU4 | METEOR | CIDEr | SPICE ||\n|| Hard-Attention  | 66.9 | 19.9 | 18.5 | - | - ||\n|| ATT-FCN  | 64.7 | 23.0 | 18.9 | - | - ||\n|| Adaptive  | 67.7 | 25.1 | 20.4 | 53.1 | 14.5 ||\n|| NBT | [BOLD] 69.0 | [BOLD] 27.1 | [BOLD] 21.7 | [BOLD] 57.5 | [BOLD] 15.6 ||\n|| NBToracle | 72.0 | 28.5 | 23.1 | 64.8 | 19.6 ||",
    "claim": "NBToracle achieves the highest scores across all metrics compared to other methods.",
    "label": "support"
  },
  {
    "id": "training_741_support",
    "table_caption": "Table: NFH Resolution accuracies on the development and test sets.",
    "table_content": "|| Model | Development | Test ||\n|| Base | 65.6 | 60.8 ||\n|| + Elmo | [BOLD] 77.2 | [BOLD] 74.0 ||",
    "claim": "The model with ELMo embeddings achieves higher accuracy on both the development and test sets compared to the base model.",
    "label": "support"
  },
  {
    "id": "training_709_support",
    "table_caption": "Table: Our model improves accuracy over BERT-Base for RTE and COPA benchmarks. Improvements are comparable to BERT-Large but still lag behind much larger models trained on more data, such as ALBERT. All scores are on the validation set.",
    "table_content": "|| Model | RTE | COPA ||\n|| BERT-Base | 66.4 | 62.0 ||\n|| BERT-Base BSO | 71.1 | 67.0 ||\n|| Conpono | 70.0 | 69.0 ||\n|| BERT-Large | 70.4 | 69.0 ||\n|| ALBERT | 86.6 | - ||",
    "claim": "BERT-Base BSO shows improvements over BERT-Base on both RTE and COPA tasks.",
    "label": "support"
  },
  {
    "id": "training_612_support",
    "table_caption": "Table: Ablation tests of different architecture choices using MTMSN\\textscLARGE.",
    "table_content": "|| Model | EM | F1 ||\n|| MTMSN | 76.7 | 80.5 ||\n|| w/o Q/P Vectors | 75.1 | 79.2 ||\n|| w/o CLS Vector | 74.0 | 78.4 ||\n|| Q/P Vectors Using Last Hidden | 76.5 | 80.2 ||\n|| w/o Gated Span Prediction | 75.8 | 79.7 ||\n|| Combine Add/Sub with Negation | 75.5 | 79.4 ||",
    "claim": "Removing the question and passage vectors leads to a 1.3% drop in F1. Removing the CLS vector results in a larger drop in both EM and F1. Using the last hidden representation for question and passage vectors slightly reduces performance. Removing the gated span prediction causes a 0.8% decline in both metrics. Sharing parameters between the arithmetic expression and negation components results in a 1.1% drop in F1.",
    "label": "support"
  },
  {
    "id": "training_753_support",
    "table_caption": "Table: Comparison of optimization methods: Separate optimization of the word, character and meta model is more accurate on average than full back-propagation using a single loss function.The results are statistically significant with two-tailed paired t-test for xpos with p<0.001 and for morphology with p<0.0001.",
    "table_content": "|| [BOLD] Optimization | Avg. F1 Score morphology | Avg. F1 Score xpos ||\n|| separate | [BOLD] 94.57 | [BOLD] 94.85 ||\n|| jointly | 94.15 | 94.48 ||",
    "claim": "Separate optimization achieves higher average F1 scores than joint optimization for both morphology and xpos tasks.",
    "label": "support"
  },
  {
    "id": "training_655_support",
    "table_caption": "Table: Model comparison on 1 Billion Word Language Modeling Benchmark. Models marked with * are from (Jozefowicz et al., 2016).",
    "table_content": "|| Model | Test Perplexity | Test Perplexity | ops/timestep (millions) | #Params excluding embed. & softmax | Total #Params | [ITALIC] Drop-  [ITALIC] Prob | TFLOPS per GPU ||\n|| [EMPTY] | 10 epochs | (final) | [EMPTY] | (millions) | (billions) | [EMPTY] | (observed) ||\n|| Kneser-Ney 5-gram* | [EMPTY] | 67.6 | 0.00001 | [EMPTY] | 1.8 | [EMPTY] | [EMPTY] ||\n|| LSTM-512-512* | [EMPTY] | 54.1 | 2.4 | 2.4 | 0.8 | 0.1 | [EMPTY] ||\n|| LSTM-1024-512* | [EMPTY] | 48.2 | 4.7 | 4.7 | 0.8 | 0.1 | [EMPTY] ||\n|| LSTM-2048-512* | 45.0 | 43.7 | 9.4 | 9.4 | 0.8 | 0.1 | 0.61 ||\n|| LSTM-2048-512 | 44.7 | [EMPTY] | 9.4 | 9.4 | 0.8 | 0.1 | 1.21 ||\n|| 4xLSTM-512 | 46.0 | [EMPTY] | 8.4 | 8.4 | 0.8 | 0.1 | 1.07 ||\n|| MoE-1-Wide | 46.1 | [EMPTY] | 8.4 | 8.4 | 0.8 | 0.1 | 1.29 ||\n|| MoE-1-Deep | 45.7 | [EMPTY] | 8.4 | 8.4 | 0.8 | 0.1 | 1.29 ||\n|| MoE-4 | 45.0 | [EMPTY] | 8.4 | 8.4 | 0.8 | 0.1 | 0.52 ||\n|| MoE-32 | 39.7 | [EMPTY] | 8.4 | 37.8 | 0.9 | 0.1 | 0.87 ||\n|| MoE-256 | 35.7 | [EMPTY] | 8.6 | 272.9 | 1.1 | 0.1 | 0.81 ||\n|| MoE-256-h | 36.0 | [EMPTY] | 8.4 | 272.9 | 1.1 | 0.1 | 0.89 ||\n|| MoE-1024-h | 34.6 | [EMPTY] | 8.5 | 1079.0 | 1.9 | 0.2 | 0.90 ||\n|| MoE-4096-h | 34.1 | [EMPTY] | 8.9 | 4303.4 | 5.1 | 0.2 | 0.74 ||\n|| 2xLSTM-8192-1024* | 34.7 | 30.6 | 151.0 | 151.0 | 1.8 | 0.25 | 1.09 ||\n|| MoE-34M | 31.3 | [EMPTY] | 33.8 | 4313.9 | 6.0 | 0.3 | 1.22 ||\n|| MoE-143M | [BOLD] 28.0 | [EMPTY] | 142.7 | 4371.1 | 6.0 | 0.4 | [BOLD] 1.56 ||",
    "claim": "For baseline models without MoE, computational efficiency ranged from 1.07 to 1.29 TFLOPS/GPU, while for low-computation MoE models, it ranged from 0.74 to 0.90 TFLOPS/GPU, except for the 4-expert model. The highest-computation MoE model achieved 1.56 TFLOPS/GPU.",
    "label": "support"
  },
  {
    "id": "training_695_support",
    "table_caption": "Table: Speed comparison of our systems with other open-sourced systems.",
    "table_content": "|| [BOLD] System | [BOLD] Speed (Sents/s) | [BOLD] Speedup ||\n|| [BOLD] Only Segmenter | [EMPTY] | [EMPTY] ||\n|| CODRA Joty et al. ( 2015 ) | 3.06 | 1.0x ||\n|| WLY Wang et al. ( 2018 ) | 4.30 | 1.4x ||\n|| SPADE Soricut and Marcu ( 2003 ) | 5.24 | 1.7x ||\n|| Our (CPU) | 12.05 | 3.9x ||\n|| Our (GPU) | 35.54 | 11.6x ||\n|| [BOLD] Only Parser | [EMPTY] | [EMPTY] ||\n|| SPADE Soricut and Marcu ( 2003 ) | 5.07 | 1.0x ||\n|| CODRA Joty et al. ( 2015 ) | 7.77 | 1.5x ||\n|| Our (CPU) | 12.57 | 2.5x ||\n|| Our (GPU) | 30.45 | 6.0x ||\n|| [BOLD] End-to-End (Segmenter → Parser) | [EMPTY] | [EMPTY] ||\n|| CODRA Joty et al. ( 2015 ) | 3.05 | 1.0x ||\n|| SPADE Soricut and Marcu ( 2003 ) | 4.90 | 1.6x ||\n|| Our (CPU) | 11.99 | 3.9x ||\n|| Our (GPU) | 28.96 | 9.5x ||",
    "claim": "Our segmenter is 6.8x faster than SPADE. Our parser is 3.9x faster than CODRA. Our end-to-end system is 5.9x faster than SPADE. Even when tested only on CPU, our model is faster than all the other models.",
    "label": "support"
  },
  {
    "id": "training_698_support",
    "table_caption": "Table: Language modeling results on Yelp and arXiv data. Upper block are baselines, and lower are our models.",
    "table_content": "|| [BOLD] Model | [BOLD] Yelp  [BOLD] NLL | [BOLD] Yelp  [BOLD] KL | [BOLD] Yelp  [BOLD] PPL | [BOLD] arXiv  [BOLD] NLL | [BOLD] arXiv  [BOLD] KL | [BOLD] arXiv  [BOLD] PPL ||\n|| [ITALIC] flat-LM | 162.6 | - | 48.0 | 218.7 | - | 57.6 ||\n|| [ITALIC] flat-VAE | ≤ 163.1 | 0.01 | ≤ 49.2 | ≤ 219.5 | 0.01 | ≤ 58.4 ||\n|| [ITALIC] ml-LM | 162.4 | - | 47.9 | 219.3 | - | 58.1 ||\n|| [ITALIC] ml-VAE-S | ≤ 160.8 | 3.6 | ≤ 46.6 | ≤ 216.8 | 5.3 | ≤ 55.6 ||\n|| [ITALIC] ml-VAE-D | ≤  [BOLD] 160.2 | 6.8 | ≤  [BOLD] 45.8 | ≤  [BOLD] 215.6 | 12.7 | ≤  [BOLD] 54.3 ||",
    "claim": "ml-VAE-D achieves the lowest NLL and PPL values for both Yelp and arXiv datasets.",
    "label": "support"
  },
  {
    "id": "training_726_support",
    "table_caption": "Table: Ablation study of Q-WAAE model",
    "table_content": "|| [BOLD] Test sets | [BOLD] COCO-ambiguous  [BOLD] BLEU | [BOLD] COCO-ambiguous  [BOLD] METEOR ||\n|| Baseline | 28.50 | 48.80 ||\n|| Baseline +  [ITALIC] G + no  [ITALIC] v | 29.43 | 49.60 ||\n|| Baseline +  [ITALIC] G | 29.91 | 49.24 ||\n|| [ITALIC] Q-WAAE + no  [ITALIC] v | 30.57 | 50.15 ||\n|| [ITALIC] Q-WAAE | 31.41 | 50.95 ||",
    "claim": "Q-WAAE achieves the highest BLEU and METEOR scores on the COCO-ambiguous test set compared to other configurations.",
    "label": "support"
  },
  {
    "id": "training_677_support",
    "table_caption": "Table: The performance (accuracy) of ESIM with our proposed attention enhancement on MultiNLI (Williams et al., 2017) development set. (d is the output hidden size of BiLSTM)",
    "table_content": "|| [EMPTY] | [BOLD] Cross-Domain | [BOLD] In-Domain ||\n|| Our ESIM without CoVe ( [ITALIC] d=300) | 73.4 | 73.3 ||\n|| Our ESIM without CoVe + fully-aware ( [ITALIC] d=250) | 76.9 | 76.2 ||\n|| Our ESIM without CoVe + fully-aware + multi-level ( [ITALIC] d=250) | 78.2 | 77.9 ||\n|| Our ESIM ( [ITALIC] d=300) | 73.9 | 73.7 ||\n|| Our ESIM + fully-aware ( [ITALIC] d=250) | 77.3 | 76.5 ||\n|| Our ESIM + fully-aware + multi-level ( [ITALIC] d=250) | [BOLD] 78.4 | [BOLD] 78.2 ||",
    "claim": "Augmenting ESIM with fully-aware attention and multi-level fusion yields the highest performance in both cross-domain and in-domain tasks.",
    "label": "support"
  },
  {
    "id": "training_691_support",
    "table_caption": "Table: Semantic F1 scores on the English test set for each language pair.",
    "table_content": "|| eng-only | +cat | +ces | +deu | +jpn | +spa | +zho ||\n|| 86.54 | 86.79 | 87.07 | 87.07 | 87.11 | 87.24 | 87.10 ||",
    "claim": "English SRL F1 scores improve with the addition of other languages, with scores ranging from 86.54 for English-only to 87.24 with Spanish.",
    "label": "support"
  },
  {
    "id": "training_681_support",
    "table_caption": "Table: Ablation studies on the dev set.",
    "table_content": "|| Models | Accuracy ||\n|| Only  [ITALIC] P→ [ITALIC] Q | 87.74 ||\n|| Only  [ITALIC] P← [ITALIC] Q | 87.47 ||\n|| w/o Full-Matching | 87.86 ||\n|| w/o Maxpooling-Matching | 87.64 ||\n|| w/o Attentive-Matching | 87.87 ||\n|| w/o MaxAttentive-Matching | 87.98 ||\n|| Full Model | [BOLD] 88.69 ||",
    "claim": "Single-direction matching models (\"Only P→Q\" and \"Only P←Q\") have lower accuracy compared to the Full Model, and removing any matching strategy reduces performance compared to the Full Model.",
    "label": "support"
  },
  {
    "id": "training_702_support",
    "table_caption": "Table: BLEU scores for NMT models on WMT14 English-German and English-French and IWSLT 2015 English-Vietnamese dataset. “AD” denotes auxiliary decoder, “RL” denotes reinforcement learning.",
    "table_content": "|| [BOLD] NMT Models | [BOLD] WMT14  [BOLD] En→De | [BOLD] WMT14  [BOLD] De→En | [BOLD] WMT14  [BOLD] En→Fr | [BOLD] IWSLT15  [BOLD] En→Vi ||\n|| Baseline | 22.6 | 26.8 | 32.3 | 24.9 ||\n|| Baseline + AD | 24.0 | 28.2 | 33.6 | 26.2 ||\n|| Baseline + AD (Denoising) | 24.3 | 28.4 | 34.0 | 26.6 ||\n|| Baseline + AD (RL) | 24.4 | 28.5 | 33.9 | 26.8 ||\n|| BiDAN (All modules converge) | 24.6 | 28.7 | 34.1 | 27.0 ||\n|| [BOLD] BiDAN | [BOLD] 24.7 | [BOLD] 28.9 | [BOLD] 34.2 | [BOLD] 27.1 ||",
    "claim": "BiDAN achieves the highest BLEU scores across all datasets compared to other models.",
    "label": "support"
  },
  {
    "id": "training_689_support",
    "table_caption": "Table: Per-label breakdown of F1 scores for Catalan and Spanish. These numbers reflect labels for each argument; the combination is different from the overall semantic F1, which includes predicate sense disambiguation.",
    "table_content": "|| [EMPTY] | [BOLD] arg0 | [BOLD] arg1 | [BOLD] arg2 | [BOLD] arg3 | [BOLD] arg4 | [BOLD] arg [ITALIC] L | [BOLD] arg [ITALIC] M ||\n|| Gold label count (cat) | 2117 | 4296 | 1713 | 61 | 71 | 49 | 2968 ||\n|| Monolingual cat  [ITALIC] F1 | 82.06 | 79.06 | 68.95 | 28.89 | 42.42 | 39.51 | 60.85 ||\n|| + eng improvement | +2.75 | +2.58 | +4.53 | +18.17 | +9.81 | +1.35 | +1.10 ||\n|| Gold label count (spa) | 2438 | 4295 | 1677 | 49 | 82 | 46 | 3237 ||\n|| Monolingual spa  [ITALIC] F1 | 82.44 | 77.93 | 70.24 | 28.89 | 41.15 | 22.50 | 58.89 ||\n|| + eng improvement | +0.37 | +0.43 | +1.35 | -3.40 | -3.48 | +4.01 | +1.26 ||",
    "claim": "In both languages, there is a small but consistent improvement in F1 scores for the most common label categories (arg1 and argM) with the addition of English data.",
    "label": "support"
  },
  {
    "id": "training_703_support",
    "table_caption": "Table: BLEU scores on WMT14 English-to-French for NMT models with different encoders.",
    "table_content": "|| [BOLD] Encoder Source | [BOLD] BLEU ( [ITALIC] p1) | [BOLD] BLEU ( [ITALIC] p2) ||\n|| Random Encoder | 7.2 | 0.1 ||\n|| En→De Encoder | 11.1 | 0.2 ||\n|| En→De Encoder (BiDAN) | 27.8 | 2.5 ||\n|| En→Fr Encoder (Original) | [BOLD] 62.4 | [BOLD] 39.1 ||",
    "claim": "The En→Fr Encoder (Original) achieves the highest BLEU scores, while the Random Encoder has the lowest scores.",
    "label": "support"
  },
  {
    "id": "training_616_support",
    "table_caption": "Table: Performance breakdown of NABERT\\textscLARGE and MTMSN\\textscLARGE by predicted answer types.",
    "table_content": "|| Type | NABERT (%) | NABERT EM | NABERT F1 | MTMSN (%) | MTMSN EM | MTMSN F1 ||\n|| Span | 43.0 | 67.9 | 74.2 | 42.7 | 72.2 | 81.0 ||\n|| Add/Sub | 43.6 | 62.0 | 62.1 | 32.4 | 78.1 | 78.2 ||\n|| Count | 13.4 | 62.4 | 62.4 | 13.4 | 70.4 | 70.4 ||\n|| Negation | 0 | 0 | 0 | 11.5 | 96.3 | 96.3 ||",
    "claim": "MTMSN achieves higher EM and F1 scores than NABERT in the Negation, Add/Sub, and Count types.",
    "label": "support"
  },
  {
    "id": "training_737_support",
    "table_caption": "Table: Performance of the proposed TPRN model compared to BiDAF proposed in [Seo et al.2016]",
    "table_content": "|| Single Model | EM(dev) | F1(dev) | EM(test) | F1(test) ||\n|| TPRN | 63.8 | 74.4 | 66.6 | 76.3 ||\n|| BiDAF | 62.8 | 73.5 | 67.1 | 76.8 ||",
    "claim": "The BiDAF model achieves higher EM and F1 scores than the TPRN model on the test dataset.",
    "label": "support"
  },
  {
    "id": "training_718_support",
    "table_caption": "Table: Comparison of different approaches on the SQuAD 2.0 test set, extracted on Aug 28, 2018: Levy et al. Levy171, Clark et al. Clark182, Liu et al. liu2017stochastic3, Huang et al. Huang17b4 and Wang et al. wang2018multi5. † indicates unpublished works.",
    "table_content": "|| Model | Dev EM | Dev F1 | Test EM | Test F1 ||\n|| BNA1 | 59.8 | 62.6 | 59.2 | 62.1 ||\n|| DocQA2 | 61.9 | 64.8 | 59.3 | 62.3 ||\n|| DocQA + ELMo | 65.1 | 67.6 | 63.4 | 66.3 ||\n|| ARRR† | - | - | 68.6 | 71.1 ||\n|| VS3−Net† | - | - | 68.4 | 71.3 ||\n|| SAN3 | - | - | 68.6 | 71.4 ||\n|| FusionNet++(ensemble)4 | - | - | 70.3 | 72.6 ||\n|| SLQA+5 | - | - | 71.5 | [BOLD] 74.4 ||\n|| RMR + ELMo + Verifier | [BOLD] 72.3 | [BOLD] 74.8 | [BOLD] 71.7 | 74.2 ||\n|| Human | 86.3 | 89.0 | 86.9 | 89.5 ||",
    "claim": "The system achieves an EM score of 71.7 and an F1 score of 74.2 on the test set, with SLQA+ having comparable results.",
    "label": "support"
  },
  {
    "id": "training_646_support",
    "table_caption": "Table: Intra-Dataset classification results, comparing the baseline and augmented-baseline training sets",
    "table_content": "|| Dataset | Accuracy Baseline | Accuracy Augmented | Accuracy (%) | Precision Baseline | Precision Augmented | Precision (%) | Recall Baseline | Recall Augmented | Recall (%) | F1 Baseline | F1 Augmented | F1 (%) ||\n|| WS | 0.967 | 0.977 | +1.03 | 0.968 | 0.989 | +2.17 | 0.936 | 0.943 | +0.75 | 0.952 | 0.966 | +1.47 ||\n|| WH | 0.891 | 0.872 | -2.13 | 0.862 | 0.600 | -30.40 | 0.375 | 0.582 | +55.20 | 0.523 | 0.591 | +13.00 ||\n|| SE | 0.715 | 0.764 | +6.85 | 0.901 | 0.767 | -14.87 | 0.367 | 0.635 | +73.02 | 0.522 | 0.695 | +33.14 ||\n|| DV | 0.922 | 0.935 | +1.41 | 0.929 | 0.923 | -0.65 | 0.753 | 0.814 | +8.10 | 0.832 | 0.865 | +3.97 ||\n|| FN | 0.956 | 0.942 | -1.46 | 0.874 | 0.644 | -26.32 | 0.337 | 0.515 | +52.82 | 0.486 | 0.573 | +17.90 ||\n|| Combined | 0.904 | 0.905 | +0.11 | 0.895 | 0.718 | -19.77 | 0.433 | 0.642 | +48.26 | 0.584 | 0.678 | +16.10 ||",
    "claim": "Augmenting the training sets leads to improvements in Recall and F1 in most cases, with the SE dataset showing a peak increase of +73.02% in Recall and +33.14% in F1, while Precision decreases for most datasets.",
    "label": "support"
  },
  {
    "id": "training_720_support",
    "table_caption": "Table: Comparison of different architectures for the answer verifier.",
    "table_content": "|| Configuration | NoAns ACC ||\n|| Model-I | 74.5 ||\n|| Model-II | 74.6 ||\n|| Model-II + ELMo | 75.3 ||\n|| Model-III | [BOLD] 76.2 ||\n|| Model-III + ELMo | 76.1 ||",
    "claim": "Model-III achieves the highest no-answer accuracy of 76.2, and adding ELMo embeddings does not improve performance.",
    "label": "support"
  },
  {
    "id": "training_706_support",
    "table_caption": "Table: Performance of our generative models on phrase matching metrics.",
    "table_content": "|| Models | METEOR | ROUGE-L ||\n|| Multiple References | Multiple References | Multiple References ||\n|| Seq2seq + Atten. (C) | 2.59 | 8.44 ||\n|| Seq2seq + Atten. (V) | 2.66 | 8.34 ||\n|| Seq2seq + Atten. (C+V) ⊗ | 3.03 | 8.84 ||\n|| ⊗ + BiDAF (C+V) | 3.70 | 9.82 ||",
    "claim": "The BiDAF model achieves higher scores than non-BiDAF models on both METEOR and ROUGE-L metrics.",
    "label": "support"
  },
  {
    "id": "training_717_support",
    "table_caption": "Table: Comparison of different readers with fixed answer verifier.",
    "table_content": "|| Configuration | All EM | All F1 | NoAns ACC ||\n|| DocQA | 61.9 | 64.8 | 69.1 ||\n|| + Model-III | [BOLD] 66.5 | [BOLD] 69.2 | [BOLD] 75.2 ||\n|| DocQA + ELMo | 65.1 | 67.6 | 70.6 ||\n|| + Model-III | [BOLD] 68.0 | [BOLD] 70.7 | [BOLD] 76.1 ||",
    "claim": "The no-answer accuracy increases by 6 points when adding Model-III to DocQA and by 5.5 points when adding Model-III to DocQA + ELMo.",
    "label": "support"
  },
  {
    "id": "training_708_support",
    "table_caption": "Table: Conpono improves the previous state-of-the-art on four DiscoEval tasks. The average accuracy across all tasks is also a new state-of-the-art, despite a small drop in accuracy for PDTB-E. BERT-Base and BERT-Large numbers are reported from Chen et al. (2019), while the rest were collected for this paper. We report standard deviations by running the evaluations 10 times with different seeds for the same Conpono model weights.",
    "table_content": "|| Model | SP | BSO | DC | SSP | PDTB-E | PDTB-I | RST-DT | avg. ||\n|| BERT-Base | 53.1 | 68.5 | 58.9 | 80.3 | 41.9 | 42.4 | 58.8 | 57.7 ||\n|| BERT-Large | 53.8 | 69.3 | 59.6 | [BOLD] 80.4 | [BOLD] 44.3 | 43.6 | 59.1 | 58.6 ||\n|| RoBERTa-Base | 38.7 | 58.7 | 58.4 | 79.7 | 39.4 | 40.6 | 44.1 | 51.4 ||\n|| BERT-Base BSO | 53.7 | 72.0 | 71.9 | 80.0 | 42.7 | 40.5 | [BOLD] 63.8 | 60.6 ||\n|| Conpono  [ITALIC] isolated | 50.2 | 57.9 | 63.2 | 79.9 | 35.8 | 39.6 | 48.7 | 53.6 ||\n|| Conpono  [ITALIC] uni-encoder | 59.9 | 74.6 | 72.0 | 79.6 | 40.0 | 43.9 | 61.9 | 61.7 ||\n|| Conpono (k=2) | [BOLD] 60.7 | [BOLD] 76.8 | [BOLD] 72.9 | [BOLD] 80.4 | 42.9 | [BOLD] 44.9 | 63.1 | [BOLD] 63.0 ||\n|| Conpono std. | ±.3 | ±.1 | ±.3 | ±.1 | ±.7 | ±.6 | ±.2 | - ||",
    "claim": "Conpono (k=2) achieves the highest performance on the sentence ordering and coherence tasks (SP, BSO, and DC) compared to other models.",
    "label": "support"
  },
  {
    "id": "training_707_support",
    "table_caption": "Table: Ablation of classification vs. max-margin loss on our TriDAF discriminative model (on dev set).",
    "table_content": "|| Models | recall@1 | recall@2 | recall@5 ||\n|| Classification loss | 19.32 | 33.72 | 66.60 ||\n|| Max-margin loss | 22.20 | 35.90 | 68.09 ||",
    "claim": "Max-margin loss has higher recall values than classification loss across recall@1, recall@2, and recall@5.",
    "label": "support"
  },
  {
    "id": "training_711_support",
    "table_caption": "Table: The ablation analysis shows the effects of different k values (ie. window sizes) in our objective, removing the MLM objective during pretraining and training with a small transformer encoder.",
    "table_content": "|| Model | SP | BSO | DC | SSP | PDTB-E | PDTB-I | RST-DT | avg. ||\n|| k=4 | 59.84 | 76.05 | [BOLD] 73.62 | [BOLD] 80.65 | 42.28 | 44.25 | 63.00 | 62.81 ||\n|| k=3 | 60.47 | 76.68 | 72.74 | 80.30 | [BOLD] 43.40 | 44.28 | 62.56 | 62.92 ||\n|| k=2 | [BOLD] 60.67 | [BOLD] 76.75 | 72.85 | 80.38 | 42.87 | [BOLD] 44.87 | [BOLD] 63.13 | [BOLD] 63.07 ||\n|| k=1 | 47.56 | 66.03 | 72.62 | 80.15 | 42.79 | 43.55 | 62.31 | 59.29 ||\n|| - MLM | 54.92 | 75.37 | 68.35 | 80.2 | 41.67 | 43.88 | 61.27 | 60.81 ||\n|| Small | 45.41 | 61.70 | 67.71 | 75.58 | 35.26 | 36.18 | 46.58 | 52.63 ||",
    "claim": "Using a window size larger than 1 improves performance, and increasing the window size beyond 2 results in similar performance.",
    "label": "support"
  },
  {
    "id": "training_705_support",
    "table_caption": "Table: Performance of our baselines, discriminative models, and generative models for recall@k metrics on our Twitch-FIFA test set. C and V represent chat and video context, respectively.",
    "table_content": "|| Models | r@1 | r@2 | r@5 ||\n|| Baselines | Baselines | Baselines | Baselines ||\n|| Most-Frequent-Response | 10.0 | 16.0 | 20.9 ||\n|| Naive Bayes | 9.6 | 20.9 | 51.5 ||\n|| Logistic Regression | 10.8 | 21.8 | 52.5 ||\n|| Nearest Neighbor | 11.4 | 22.6 | 53.2 ||\n|| Chat-Response-Cosine | 11.4 | 22.0 | 53.2 ||\n|| Discriminative Model | Discriminative Model | Discriminative Model | Discriminative Model ||\n|| Dual Encoder (C) | 17.1 | 30.3 | 61.9 ||\n|| Dual Encoder (V) | 16.3 | 30.5 | 61.1 ||\n|| Triple Encoder (C+V) | 18.1 | 33.6 | 68.5 ||\n|| TriDAF+Self Attn (C+V) | 20.7 | 35.3 | 69.4 ||\n|| Generative Model | Generative Model | Generative Model | Generative Model ||\n|| Seq2seq +Attn (C) | 14.8 | 27.3 | 56.6 ||\n|| Seq2seq +Attn (V) | 14.8 | 27.2 | 56.7 ||\n|| Seq2seq + Attn (C+V) | 15.7 | 28.0 | 57.0 ||\n|| Seq2seq + Attn + BiDAF (C+V) | 16.5 | 28.5 | 57.7 ||",
    "claim": "The TriDAF+Self Attn (C+V) model achieves the highest recall scores across all metrics, and dual and triple encoder models outperform the baselines.",
    "label": "support"
  },
  {
    "id": "training_578_support",
    "table_caption": "Table: Sentence selection evaluation and average label accuracy of GEAR with different thresholds on dev set (%).",
    "table_content": "|| [ITALIC] τ | [BOLD] OFEVER | [BOLD] Precision | [BOLD] Recall | [BOLD] F1 | [BOLD] GEAR LA ||\n|| 0 | [BOLD] 91.10 | 24.08 | [BOLD] 86.72 | 37.69 | 74.84 ||\n|| 10−4 | 91.04 | 30.88 | 86.63 | 45.53 | 74.86 ||\n|| 10−3 | 90.86 | 40.60 | 86.36 | 55.23 | [BOLD] 74.91 ||\n|| 10−2 | 90.27 | 53.12 | 85.47 | 65.52 | 74.89 ||\n|| 10−1 | 87.70 | [BOLD] 70.61 | 81.64 | [BOLD] 75.72 | 74.81 ||",
    "claim": "The model with threshold 0 achieves the highest recall and OFEVER score, while increasing the threshold results in higher precision and F1 scores.",
    "label": "support"
  },
  {
    "id": "training_777_support",
    "table_caption": "Table: Audio datasets",
    "table_content": "|| [BOLD] Set | [BOLD] Language | [BOLD] Corpus | [BOLD] Hours | [BOLD] Speakers | [BOLD] Sentences | [BOLD] Foreign words | [BOLD] Style ||\n|| Training | English | Librispeech-360hr | 363.6 | 921 | 104,014 | [EMPTY] | Reading ||\n|| Training | English | Mozilla CommonVoice | 780 | 31,858 | 644,120 | - | Reading ||\n|| Training | English | TED-LIUM | 452 | 2,351 | 268,263 | - | Spontaneous ||\n|| Training | Vietnamese | VinBDI-set | 2,500 | 18,000 | 3,666,892 | - | Reading ||\n|| Testing | English | Test-clean | 5.4 | 87 | 2,620 | - | Reading ||\n|| Testing | English | Test-other | 5.3 | 90 | 2,939 | - | Reading ||\n|| Testing | Vietnamese | Reading-test | 9.9 | 23 | 5,358 | - | Reading ||\n|| Testing | Vietnamese | Conversation-test | 10.8 | 1,892 | 11,533 | - | Spontaneous ||\n|| Testing | Vietnamese | YouTube-test | 9.9 | unknown | 5,432 | 24,8% | Spontaneous ||\n|| Testing | Vietnamese | VLSP2018 | 2.1 | unknown | 796 | - | Reading ||",
    "claim": "The YouTube-test set contains 24.8% foreign words.",
    "label": "support"
  },
  {
    "id": "training_774_support",
    "table_caption": "Table: Tagging accuracies of adaptation of our baseline SVM tagger from the PTB to the PPCEME in ablation experiments.",
    "table_content": "|| Feature set | IV | OOV | All ||\n|| All features | 81.68 | 48.96 | 74.15 ||\n|| – word context | 79.69 | 38.62 | 70.23 ||\n|| – prefix | 81.61 | 46.11 | 73.43 ||\n|| – suffix | 81.36 | 38.13 | 71.40 ||\n|| – affix | 81.22 | 34.40 | 70.44 ||\n|| – orthographic | 81.68 | 48.92 | 74.14 ||",
    "claim": "Removing word context features significantly reduces accuracy on OOV tokens, while removing orthographic features has little impact on overall accuracy.",
    "label": "support"
  },
  {
    "id": "training_700_support",
    "table_caption": "Table: Evaluation of diversity of 1000 generated sentences on self-BLEU scores (B-n), unique n-gram percentages (ngr), 2-gram entropy score.",
    "table_content": "|| [BOLD] Model | [BOLD] Yelp  [BOLD] B-2 | [BOLD] Yelp  [BOLD] B-3 | [BOLD] Yelp  [BOLD] B-4 | [BOLD] Yelp  [BOLD] 2gr | [BOLD] Yelp  [BOLD] 3gr | [BOLD] Yelp  [BOLD] 4gr | [BOLD] Yelp  [BOLD] Etp-2 ||\n|| ARAE | 0.725 | 0.544 | 0.402 | 36.2 | 59.7 | 75.8 | 7.551 ||\n|| AAE | 0.831 | 0.672 | 0.483 | 33.2 | 57.5 | 71.4 | 6.767 ||\n|| [ITALIC] flat-VAE | 0.872 | 0.755 | 0.617 | 23.7 | 48.2 | 69.0 | 6.793 ||\n|| [ITALIC] ml-VAE-S | 0.865 | 0.734 | 0.591 | 28.7 | 50.4 | 70.7 | 6.843 ||\n|| [ITALIC] ml-VAE-D | 0.851 | 0.723 | 0.579 | 30.5 | 53.2 | 72.6 | 6.926 ||",
    "claim": "ml-VAE-D shows the smallest BLEU scores among the VAE variants.",
    "label": "support"
  },
  {
    "id": "training_685_support",
    "table_caption": "Table: Test accuracy on the text classification datasets. For each dataset, we conduct significance test against the best reproducible model, and * means that the improvement is significant at 0.05 significance level.",
    "table_content": "|| Model | AG | Sogou | DBP | Yelp-B | Yelp | Yahoo | Amz | Amz-B ||\n|| Zhang ET AL., 2015 | 92.36 | [BOLD] 97.19 | 98.69 | 95.64 | 62.05 | 71.20 | 59.57 | 95.07 ||\n|| Joulin ET AL., 2016 | [BOLD] 92.50 | 96.80 | 98.60 | 95.70 | 63.90 | 72.30 | 60.20 | 94.60 ||\n|| Conneau ET AL., 2016 | 91.33 | 96.82 | 98.71 | [BOLD] 95.72 | [BOLD] 64.72 | [BOLD] 73.43 | [BOLD] 63.00 | [BOLD] 95.72 ||\n|| 24-Layers Transformer | 92.17 | 94.65 | [BOLD] 98.77 | 94.07 | 61.22 | 72.67 | 62.65 | 95.59 ||\n|| ENAS-macro | 92.39 | 96.79 | [BOLD] 99.01 | [BOLD] 96.07 | 64.60 | 73.16 | 62.64 | [BOLD] 95.80 ||\n|| ENAS-micro | 92.27 | [BOLD] 97.24 | 99.00 | 96.01 | 64.72 | 70.63 | 58.27 | 94.89 ||\n|| DARTS | 92.24 | 97.18 | 98.90 | 95.84 | 65.12 | 73.12 | 62.06 | 95.48 ||\n|| SMASH | 90.88 | 96.72 | 98.86 | 95.62 | [BOLD] 65.26 | [BOLD] 73.63 | [BOLD] 62.72 | 95.58 ||\n|| One-Shot | 92.06 | 96.92 | 98.89 | 95.78 | 64.78 | 73.20 | 61.30 | 95.20 ||\n|| Random Search | [BOLD] 92.54 | 97.13 | 98.98 | 96.00 | 65.23 | 72.47 | 60.91 | 94.87 ||\n|| textnas | [BOLD] 93.14 | [BOLD] 96.76 | [BOLD] 99.01 | [BOLD] 96.41∗ | [BOLD] 66.56∗ | [BOLD] 73.97∗ | [BOLD] 63.14∗ | [BOLD] 95.94∗ ||",
    "claim": "The TextNAS model outperforms other models on all text classification datasets except Sogou.",
    "label": "support"
  },
  {
    "id": "training_688_support",
    "table_caption": "Table: Detailed settings for experiments of natural language inference.",
    "table_content": "|| Exp | lr | training epoch | [ITALIC] l2 | dropout rate | penalization ||\n|| SNLI | 2×10−4 | 8 | 2×10−2 | 0.2 | 0 ||\n|| MNLI | 1×10−4 | 20 | 1×10−2 | 0.2 | 0 ||",
    "claim": "For the SNLI experiment, the learning rate is 2×10−4, training epoch is 8, l2 regularization is 2×10−2, dropout rate is 0.2, and penalization is 0. For the MNLI experiment, the learning rate is 1×10−4, training epoch is 20, l2 regularization is 1×10−2, dropout rate is 0.2, and penalization is 0.",
    "label": "support"
  },
  {
    "id": "training_752_support",
    "table_caption": "Table: Results on WSJ test set.",
    "table_content": "|| [BOLD] System | [BOLD] Accuracy ||\n|| Sogaard:2011:SCN | 97.50 ||\n|| DBLP:journals/corr/HuangXY15 | 97.55 ||\n|| choi:16a | 97.64 ||\n|| andor2016globally. | 97.44 ||\n|| DBLP:conf/conll/DozatQM17 | 97.41 ||\n|| ours | [BOLD] 97.96 ||",
    "claim": "The \"ours\" system achieves the highest accuracy of 97.96% compared to other systems.",
    "label": "support"
  },
  {
    "id": "training_779_support",
    "table_caption": "Table: Intrinsic evaluation on crowdsourced semantic equivalence vs. divergence testsets. We report overall F-score, as well as precision (P), recall (R) and F-score (F) for the equivalent (+) and divergent (-) classes separately. Semantic similarity yields better results across the board, with larger improvements on the divergent class.",
    "table_content": "|| [BOLD] Divergence Detection  [BOLD] Approach | [BOLD] OpenSubtitles +P | [BOLD] OpenSubtitles +R | [BOLD] OpenSubtitles +F | [BOLD] OpenSubtitles -P | [BOLD] OpenSubtitles -R | [BOLD] OpenSubtitles -F | [BOLD] OpenSubtitles Overall F | [BOLD] Common Crawl +P | [BOLD] Common Crawl +R | [BOLD] Common Crawl +F | [BOLD] Common Crawl -P | [BOLD] Common Crawl -R | [BOLD] Common Crawl -F | [BOLD] Common Crawl Overall F ||\n|| Sentence Embeddings | 65 | 60 | 62 | 56 | 61 | 58 | 60 | 78 | 58 | 66 | 52 | [BOLD] 74 | 61 | 64 ||\n|| MT Scores (1 epoch) | 67 | 53 | 59 | 54 | 68 | 60 | 60 | 54 | 65 | 59 | 17 | 11 | 14 | 42 ||\n|| Non-entailment | 58 | 78 | 66 | 53 | 30 | 38 | 54 | 73 | 49 | 58 | 48 | 72 | 57 | 58 ||\n|| Non-parallel | 70 | 83 | 76 | 61 | 42 | 50 | 66 | 70 | 83 | 76 | 61 | 42 | 49 | 67 ||\n|| Semantic Dissimilarity | [BOLD] 76 | [BOLD] 80 | [BOLD] 78 | [BOLD] 75 | [BOLD] 70 | [BOLD] 72 | [BOLD] 77 | [BOLD] 82 | [BOLD] 88 | [BOLD] 85 | [BOLD] 78 | 69 | [BOLD] 73 | [BOLD] 80 ||",
    "claim": "The Semantic Dissimilarity approach achieves the highest F-scores for divergent examples across both OpenSubtitles and Common Crawl datasets.",
    "label": "support"
  },
  {
    "id": "training_721_support",
    "table_caption": "Table: Comparison of readers with different answer verifiers.",
    "table_content": "|| Configuration | All EM | All F1 | NoAns ACC ||\n|| RMR | 66.9 | 69.1 | 73.1 ||\n|| + Model-I | 68.3 | 71.1 | 76.2 ||\n|| + Model-II | 68.1 | 70.8 | 75.6 ||\n|| + Model-II + ELMo | 68.2 | 70.9 | 75.9 ||\n|| + Model-III | [BOLD] 68.5 | [BOLD] 71.5 | [BOLD] 77.1 ||\n|| + Model-III + ELMo | 68.5 | 71.2 | 76.5 ||\n|| RMR + ELMo | 71.4 | 73.7 | 77.0 ||\n|| + Model-I | 71.8 | 74.4 | 77.3 ||\n|| + Model-II | 71.8 | 74.2 | 78.1 ||\n|| + Model-II + ELMo | 72.0 | 74.3 | 78.2 ||\n|| + Model-III | [BOLD] 72.3 | [BOLD] 74.8 | [BOLD] 78.6 ||\n|| + Model-III + ELMo | 71.8 | 74.3 | 78.3 ||",
    "claim": "Adding Model-III to RMR increases no-answer accuracy from 73.1 to 77.1, and similar gains are observed with ELMo embeddings.",
    "label": "support"
  },
  {
    "id": "training_756_support",
    "table_caption": "Table: The results on the standard BLI task and BLI controlled for lexeme for the original Ruder et al. (2018)’s model (✗) and the same model trained with a morphological constraint (✓) (discussed in §4.6).",
    "table_content": "|| [EMPTY] | Normal In vocab | Normal In vocab | Normal +OOVs | Normal +OOVs | Lexeme In vocab | Lexeme In vocab | Lexeme +OOVs | Lexeme +OOVs | Dictionary Sizes In vocab | Dictionary Sizes +OOVs ||\n|| Constraint | ✗ | ✓ | ✗ | ✓ | ✗ | ✓ | ✗ | ✓ | In vocab | +OOVs ||\n|| Ukrainian–Russian | [BOLD] 68.4 | 61.1 | [BOLD] 63.7 | 56.1 | [BOLD] 89.9 | 89.1 | [BOLD] 88.6 | 87.6 | 786 | 933 ||\n|| Russian–Slovak | [BOLD] 25.7 | 21.1 | [BOLD] 20.9 | 17.0 | [BOLD] 79.3 | 76.8 | [BOLD] 76.0 | 74.2 | 1610 | 2150 ||\n|| Polish–Czech | 42.0 | [BOLD] 44.4 | 34.8 | [BOLD] 36.7 | 80.6 | [BOLD] 81.1 | 75.3 | [BOLD] 75.9 | 4043 | 5332 ||\n|| Russian–Polish | 39.8 | [BOLD] 41.2 | 34.8 | [BOLD] 36.1 | 80.8 | [BOLD] 82.6 | 77.7 | [BOLD] 80.2 | 9183 | 11697 ||\n|| Catalan–Portuguese | 62.8 | [BOLD] 64.2 | 41.1 | [BOLD] 42.4 | 83.1 | [BOLD] 84.3 | 57.7 | [BOLD] 59.0 | 5418 | 10759 ||\n|| French–Spanish | 47.8 | [BOLD] 50.2 | 26.7 | [BOLD] 28.9 | 78.0 | [BOLD] 81.4 | 47.9 | [BOLD] 52.2 | 9770 | 21087 ||\n|| Portuguese–Spanish | 60.2 | [BOLD] 61.1 | 36.8 | [BOLD] 37.6 | 84.7 | [BOLD] 85.4 | 57.1 | [BOLD] 58.2 | 9275 | 22638 ||\n|| Italian–Spanish | 42.7 | [BOLD] 43.8 | 21.1 | [BOLD] 22.1 | 76.4 | [BOLD] 77.6 | 47.6 | [BOLD] 49.6 | 11685 | 30686 ||\n|| Polish–Spanish | [BOLD] 36.1 | 32.1 | [BOLD] 28.0 | 25.0 | [BOLD] 78.1 | 77.7 | [BOLD] 68.6 | 68.4 | 8964 | 12759 ||\n|| Spanish–Polish | 28.1 | [BOLD] 30.9 | 21.0 | [BOLD] 23.2 | 81.2 | [BOLD] 82.0 | 64.2 | [BOLD] 65.8 | 4270 | 6095 ||",
    "claim": "The accuracy for Spanish–Polish in-vocabulary pairs is 30.9%, which is particularly low compared to other language pairs.",
    "label": "support"
  },
  {
    "id": "training_729_support",
    "table_caption": "Table: Effect of different dispersion functions, content coverage, and dissimilarity metrics on our system. [Left] JSD values for different combinations on Yahoo! data, using LDA with 100 topics. All systems are significantly different from each other at significance level α=0.05. Systems using summation of distances for dispersion function (hsum) uniformly outperform the ones using minimum distance (hmin). [Right] ROUGE scores of different choices for TAC 2008 data. All systems use LDA with 40 topics. The parameters of our systems are adopted from the ones tuned on Yahoo! Answers.",
    "table_content": "|| [BOLD] TAC 2008 | [BOLD] TAC 2008 Dispersion [ITALIC] sum | [BOLD] TAC 2008 Dispersion [ITALIC] sum | [BOLD] TAC 2008 Dispersion [ITALIC] min | [BOLD] TAC 2008 Dispersion [ITALIC] min ||\n|| Dissimi | Cont [ITALIC] tfidf | Cont [ITALIC] sem | Cont [ITALIC] tfidf | Cont [ITALIC] sem ||\n|| [ITALIC] Semantic | 0.2216 | 0.2169 | 0.2772 | 0.2579 ||\n|| [ITALIC] Topical | 0.2128 | 0.2090 | [BOLD] 0.3234 | 0.3056 ||\n|| [ITALIC] Lexical | 0.2167 | 0.2129 | 0.3117 | 0.3160 ||",
    "claim": "Cosine using TFIDF is better at measuring content coverage than Semantic measurement, and Topical-based dissimilarity outperforms Semantic and Lexical metrics in dispersion [ITALIC] min.",
    "label": "support"
  },
  {
    "id": "training_746_support",
    "table_caption": "Table: The table shows how commonly a term appears in disruptive or constructive posts. A bold font indicates that the term appears more frequently in that class.",
    "table_content": "|| [BOLD] term | [BOLD] share of words from disruptive posts (‰) | [BOLD] share of words from constructive posts (‰) ||\n|| fucking | [BOLD] 0.06 | 0.00 ||\n|| fuck | [BOLD] 0.06 | 0.01 ||\n|| shit | [BOLD] 0.09 | 0.01 ||\n|| i | 6.40 | [BOLD] 10.70 ||\n|| you | [BOLD] 10.64 | 4.52 ||\n|| me | [BOLD] 2.43 | 1.20 ||\n|| my | [BOLD] 3.00 | 1.68 ||\n|| your | [BOLD] 3.05 | 1.25 ||\n|| myself | [BOLD] 0.22 | 0.13 ||\n|| yourself | [BOLD] 0.20 | 0.10 ||",
    "claim": "The terms \"fucking\", \"fuck\", and \"shit\" have a higher share of words in disruptive posts compared to constructive posts, while the term \"I\" has a higher share in constructive posts compared to disruptive posts.",
    "label": "support"
  },
  {
    "id": "training_754_support",
    "table_caption": "Table: F1 score for selected languages on sentence vs. word level character models for the prediction of morphology using late integration.",
    "table_content": "|| [BOLD] dev. set | word char model | sentence char model ||\n|| el | 89.05 | 93.41 ||\n|| la_ittb | 93.22 | 95.69 ||\n|| ru | 88.94 | 92.31 ||\n|| tr | 87.78 | 90.77 ||",
    "claim": "The sentence-based character model achieves higher accuracy than the word-based character model for all languages listed (el, la_ittb, ru, tr).",
    "label": "support"
  },
  {
    "id": "training_747_support",
    "table_caption": "Table: This table shows the performance of the SVM, NB and LM classifiers using the independent posts approach. Function words classifications are marked with “FW”. All others were full text classifications. The plus and minus symbols indicate whether the performance metric was calculated for disruptive (+) or constructive contributions (−).",
    "table_content": "|| [BOLD] Classifier | [BOLD] Recall+ | [BOLD] Recall− | [BOLD] Precision+ | [BOLD] Precision− | [BOLD] F1+ | [BOLD] F1− | [BOLD] Accuracy | [BOLD] AUC ||\n|| SVM | 59.91 | 75.56 | 71.03 | 65.33 | 65.00 | 70.07 | 67.73 | 0.750 ||\n|| SVM (FW) | 31.11 | 85.58 | 68.33 | 55.40 | 42.75 | 67.26 | 58.34 | 0.620 ||\n|| NB | 42.42 | 80.78 | 68.82 | 58.38 | 52.49 | 67.78 | 61.60 | 0.390 ||\n|| NB (FW) | 26.04 | 85.43 | 64.12 | 53.60 | 37.04 | 65.87 | 55.73 | 0.600 ||\n|| LM | 44.56 | 72.35 | 61.71 | 56.61 | 51.75 | 63.52 | 58.45 | 0.518 ||\n|| LM (FW) | 44.28 | 67.34 | 57.55 | 54.72 | 50.05 | 60.38 | 55.81 | 0.542 ||",
    "claim": "The support vector machine (SVM) performs best overall, with the highest positive recall and AUC, while the naïve Bayes (NB) classifier is only better in negative recall.",
    "label": "support"
  },
  {
    "id": "training_647_support",
    "table_caption": "Table: Performance of our model when using one and two layers",
    "table_content": "|| Entity | Bi-LSTM Pre. | Bi-LSTM Rec. | Bi-LSTM  [ITALIC] F1 | LSTM Pre. | LSTM Rec. | LSTM  [ITALIC] F1 ||\n|| LOC | 83.63 | 82.48 | 83.05 | 74.60 | 77.38 | 75.96 ||\n|| MISC | 84.14 | 78.37 | 81.07 | 2.15 | 2.04 | 2.09 ||\n|| ORG | 49.85 | 50.51 | 50.07 | 32.22 | 34.60 | 33.60 ||\n|| PER | 72.77 | 65.73 | 69.06 | 67.95 | 60.73 | 64.12 ||\n|| ALL | 75.88 | 72.26 | [BOLD] 74.02 | 66.61 | 65.04 | 65.80 ||",
    "claim": "Bi-LSTM outperforms LSTM in precision, recall, and F1 score across all entity types.",
    "label": "support"
  },
  {
    "id": "training_694_support",
    "table_caption": "Table: Quantitative comparison of the proposed PRN model against the baselines.",
    "table_content": "|| Model | Single-task Training Cloze | Single-task Training Coherence | Single-task Training Ordering | Single-task Training Average | Multi-task Training Cloze | Multi-task Training Coherence | Multi-task Training Ordering | Multi-task Training All ||\n|| Human∗ | 77.60 | 81.60 | 64.00 | 74.40 | – | – | – | – ||\n|| Hasty Student | 27.35 | [BOLD] 65.80 | 40.88 | 44.68 | – | – | – | – ||\n|| Impatient Reader | 27.36 | 28.08 | 26.74 | 27.39 | – | – | – | – ||\n|| BIDAF | 53.95 | 48.82 | 62.42 | 55.06 | 44.62 | 36.00 | [BOLD] 63.93 | 48.67 ||\n|| BIDAF w/ static memory | 51.82 | 45.88 | 60.90 | 52.87 | [BOLD] 47.81 | 40.23 | 62.94 | [BOLD] 50.59 ||\n|| PRN | [BOLD] 56.31 | 53.64 | [BOLD] 62.77 | [BOLD] 57.57 | 46.45 | [BOLD] 40.58 | 62.67 | 50.17 ||\n|| ∗ Taken from the RecipeQA project website, based on 100 questions sampled randomly from the validation set. | ∗ Taken from the RecipeQA project website, based on 100 questions sampled randomly from the validation set. | ∗ Taken from the RecipeQA project website, based on 100 questions sampled randomly from the validation set. | ∗ Taken from the RecipeQA project website, based on 100 questions sampled randomly from the validation set. | ∗ Taken from the RecipeQA project website, based on 100 questions sampled randomly from the validation set. | ∗ Taken from the RecipeQA project website, based on 100 questions sampled randomly from the validation set. | ∗ Taken from the RecipeQA project website, based on 100 questions sampled randomly from the validation set. | ∗ Taken from the RecipeQA project website, based on 100 questions sampled randomly from the validation set. | [EMPTY] ||",
    "claim": "In single-task training, PRN achieves the best performance on average compared to other neural models. In multi-task training, PRN and BIDAF w/ static memory perform comparably and better than BIDAF. Multi-task performances are generally worse than single-task performances.",
    "label": "support"
  },
  {
    "id": "training_618_support",
    "table_caption": "Table: Comparing the results of three different versions of ConVec (trained on Wikipedia 2.1B tokens) with Google Freebase pre-trained vectors over Google 100B tokens news dataset in the Phrase Analogy task. The Accuracy (All), shows the coverage and performance of each approach for answering questions. The accuracy for common questions (Accuracy (Commons)), is for fair comparison of each approach. #phrases shows the number of top frequent words of each approach that are used to calculate the accuracy. #found is the number of questions that all 4 words of them are present in the approach dictionary.",
    "table_content": "|| Embedding Name | #phrases | Accuracy (All) #found | Accuracy (All) Accuracy | Accuracy (Commons) #found | Accuracy (Commons) Accuracy ||\n|| Google Freebase | Top 30,000 | 1048 | 55.7% | 89 | 52.8% ||\n|| Google Freebase | Top 300,000 | 1536 | 47.0% | 800 | 48.5% ||\n|| Google Freebase | Top 3,000,000 | 1838 | 42.1% | 1203 | 42.7% ||\n|| ConVec | Top 30,000 | 202 | 81.7% | 89 | 82.0% ||\n|| ConVec | Top 300,000 | 1702 | 68.0% | 800 | 72.1% ||\n|| ConVec | Top 3,000,000 | 2238 | 56.4% | 1203 | 61.1% ||\n|| ConVec (Fine Tuned) | Top 30,000 | 202 | 80.7% | 89 | 79.8% ||\n|| ConVec (Fine Tuned) | Top 300,000 | 1702 | 68.3% | 800 | 73.0% ||\n|| ConVec (Fine Tuned) | Top 3,000,000 | 2238 | 56.8% | 1203 | 63.6% ||\n|| ConVec (Heuristic) | Top 30,000 | 242 | 81.4% | 89 | 80.9% ||\n|| ConVec (Heuristic) | Top 300,000 | 1804 | 65.6% | 800 | 68.9% ||\n|| ConVec (Heuristic) | Top 3,000,000 | 2960 | 46.6% | 1203 | 58.7% ||",
    "claim": "ConVec achieves higher accuracy than Google Freebase across all phrase counts for both all questions and common questions.",
    "label": "support"
  },
  {
    "id": "training_731_support",
    "table_caption": "Table: [Left] Summaries evaluated by Jensen-Shannon divergence (JSD) on Yahoo Answer for summaries of 100 words and 200 words. The average length of the best answer is 102.70. [Right] Value addition of each component in the objective function. The JSD on each line is statistically significantly lower than the JSD on the previous (α=0.05).",
    "table_content": "|| [EMPTY] | [BOLD] JSD100 | [BOLD] JSD200 ||\n|| Rel(evance) | 0.3424 | 0.2053 ||\n|| Rel + Aut(hor) | 0.3375 | 0.2040 ||\n|| Rel + Aut + TM (Topic Models) | 0.3366 | 0.2033 ||\n|| Rel + Aut + TM + Pol(arity) | 0.3309 | 0.1983 ||\n|| Rel + Aut + TM + Pol + Cont(ent Coverage) | 0.3102 | 0.1851 ||\n|| Rel + Aut + TM + Pol + Cont + Disp(ersion) | [BOLD] 0.3017 | [BOLD] 0.1758 ||",
    "claim": "JSD scores decrease as more features are added, with the lowest scores achieved when all features, including dispersion, are used.",
    "label": "support"
  },
  {
    "id": "training_697_support",
    "table_caption": "Table: Human evaluations on Yelp Reviews dataset. Each block is a head-to-head comparison of two models on grammatically, consistency, and non-redundancy.",
    "table_content": "|| [BOLD] Model | [BOLD] Grammar. | [BOLD] Cons. | [BOLD] Non-Red. | [BOLD] Overall ||\n|| [ITALIC] ml-VAE | 52.0 | 55.0 | 53.7 | 60.0 ||\n|| [ITALIC] flat-VAE | 30.0 | 33.0 | 27.7 | 32.3 ||\n|| [ITALIC] ml-VAE | 75.3 | 86.0 | 76.7 | 86.0 ||\n|| AAE | 13.3 | 10.3 | 15.0 | 12.0 ||\n|| [ITALIC] flat-VAE | 19.7 | 18.7 | 14.3 | 19.0 ||\n|| Real data | 61.7 | 74.7 | 74.3 | 77.7 ||\n|| [ITALIC] ml-VAE | 28.0 | 26.3 | 25.0 | 30.3 ||\n|| Real data | 48.6 | 58.7 | 49.0 | 61.3 ||",
    "claim": "ml-VAE is rated higher than flat-VAE in all evaluation criteria and produces more grammatically correct and semantically coherent samples than AAE.",
    "label": "support"
  },
  {
    "id": "training_758_support",
    "table_caption": "Table: Performance Comparison of Fake News Detection on Two Datasets.",
    "table_content": "|| Dataset | Methods | Accuracy | Precision | Recall | F1-Score ||\n|| Weibo | DTC | 0.756 | 0.754 | 0.758 | 0.756 ||\n|| Weibo | ML-GRU | 0.799 | 0.810 | 0.790 | 0.800 ||\n|| Weibo | Basic-GRU | 0.835 | 0.830 | 0.850 | 0.840 ||\n|| Weibo | CSI | 0.835 | 0.735 | [BOLD] 0.996 | 0.858 ||\n|| Weibo | HSA-BLSTM | 0.843 | [BOLD] 0.860 | 0.810 | 0.834 ||\n|| Weibo | SAME | 0.776 | 0.770 | 0.780 | 0.775 ||\n|| [EMPTY] | [BOLD] DEAN | [BOLD] 0.872 | [BOLD] 0.860 | 0.890 | [BOLD] 0.874 ||\n|| Twitter | DTC | 0.613 | 0.608 | 0.570 | 0.588 ||\n|| Twitter | ML-GRU | 0.684 | 0.663 | 0.740 | 0.692 ||\n|| Twitter | Basic-GRU | 0.695 | 0.674 | 0.721 | 0.697 ||\n|| Twitter | CSI | 0.696 | 0.706 | 0.649 | 0.671 ||\n|| Twitter | HSA-BLSTM | 0.718 | [BOLD] 0.731 | 0.663 | 0.695 ||\n|| Twitter | SAME | 0.667 | 0.613 | 0.849 | 0.712 ||\n|| [EMPTY] | [BOLD] DEAN | [BOLD] 0.751 | 0.698 | [BOLD] 0.860 | [BOLD] 0.771 ||",
    "claim": "The DEAN model achieves the highest accuracy and F1-score on both the Weibo and Twitter datasets compared to all other models.",
    "label": "support"
  },
  {
    "id": "training_763_support",
    "table_caption": "Table: Ablation study on modeling context on TED Zh-En development set. ”Doc” means using a entire document as a sequence for input or output. BLEU{}_{\\rm doc} indicates the document-level BLEU score calculated on the concatenation of all output sentences.",
    "table_content": "|| [BOLD] Model | BLEU (BLEU{}_{\\rm doc}) ||\n|| SentNmt [Vaswani2017Attention] | 11.4 (21.0) ||\n|| DocNmt (documents as input/output) | n/a (17.0) ||\n|| [ITALIC] Modeling source context | [ITALIC] Modeling source context ||\n|| Doc2Sent | 6.8 ||\n|| + reset word positions for each sentence | 10.0 ||\n|| + segment embedding | 10.5 ||\n|| + segment-level relative attention | 12.2 ||\n|| + context fusion gate | 12.4 ||\n|| [ITALIC] Modeling target context | [ITALIC] Modeling target context ||\n|| Transformer-XL decoder [Sent2Doc] | 12.4 ||\n|| Final | 12.9 (24.4) ||",
    "claim": "The final model achieves the highest BLEU score of 12.9 (24.4 for document-level BLEU) by jointly modeling both source and target contexts.",
    "label": "support"
  },
  {
    "id": "training_794_support",
    "table_caption": "Table: Fine-grained F1 scores on the AMR 2.0 test set. vN’17 is van Noord and Bos (2017b); L’18 is Lyu and Titov (2018); N’19 is Naseem et al. (2019).",
    "table_content": "|| Metric | vN’18 | L’18 | N’19 | Ours ||\n|| Smatch | 71.0 | 74.4 | 75.5 | [BOLD] 76.3±0.1 ||\n|| Unlabeled | 74 | 77 | [BOLD] 80 | 79.0±0.1 ||\n|| No WSD | 72 | 76 | 76 | [BOLD] 76.8±0.1 ||\n|| Reentrancies | 52 | 52 | 56 | [BOLD] 60.0±0.1 ||\n|| Concepts | 82 | [BOLD] 86 | [BOLD] 86 | 84.8±0.1 ||\n|| Named Ent. | 79 | [BOLD] 86 | 83 | 77.9±0.2 ||\n|| Wikification | 65 | 76 | 80 | [BOLD] 85.8±0.3 ||\n|| Negation | 62 | 58 | 67 | [BOLD] 75.2±0.2 ||\n|| SRL | 66 | 70 | [BOLD] 72 | 69.7±0.2 ||",
    "claim": "\"Ours\" achieves the highest scores in reentrancies, wikification, and negation compared to other systems.",
    "label": "support"
  },
  {
    "id": "training_693_support",
    "table_caption": "Table: Comparison of performance of systems with different stack capacities",
    "table_content": "|| [EMPTY] | F [ITALIC] rm | F [ITALIC] s | DA | EO | PO | TD [ITALIC] rp | TD [ITALIC] rm ||\n|| 1-best  [ITALIC] rmstart | 0.745 | 0.707 | 0.699 | 3.780 | 1.650 | 1.0 | 2.6 ||\n|| 2-best  [ITALIC] rmstart | 0.758 | 0.721 | 0.701 | 4.319 | 1.665 | 1.1 | 2.7 ||",
    "claim": "Moving to the 2-stack condition results in gains in Frm and Fs but at the cost of EO and time-to-detection scores TDrp and TDrm.",
    "label": "support"
  },
  {
    "id": "training_781_support",
    "table_caption": "Table: Performances on low-resource translations. As done by flores, the from-English pairs are measured in tokenized BLEU, while to-English are measured in detokenized SacreBLEU.",
    "table_content": "|| [BOLD] Method | [BOLD] En-Ne | [BOLD] Ne-En | [BOLD] En-Si | [BOLD] Si-En ||\n|| flores | 4.3 | 7.6 | 1.0 | 6.7 ||\n|| Data Diversification | [BOLD] 5.7 | [BOLD] 8.9 | [BOLD] 2.2 | [BOLD] 8.2 ||",
    "claim": "The Data Diversification method achieves higher BLEU scores than the baseline model (flores) in all four translation tasks.",
    "label": "support"
  },
  {
    "id": "training_784_support",
    "table_caption": "Table: Improvements of data diversification under conditions with- and without- dropout in the IWSLT’14 English-German and German-English.",
    "table_content": "|| [BOLD] Task | [BOLD] Baseline | [BOLD] Ours | [BOLD] Gain ||\n|| [BOLD] Dropout=0.3 | [BOLD] Dropout=0.3 | [BOLD] Dropout=0.3 | [BOLD] Dropout=0.3 ||\n|| En-De | 28.6 | 30.1 | +1.5 (5%) ||\n|| De-En | 34.7 | 36.5 | +1.8 (5%) ||\n|| [BOLD] Dropout=0 | [BOLD] Dropout=0 | [BOLD] Dropout=0 | [BOLD] Dropout=0 ||\n|| En-De | 25.7 | 27.5 | +1.8 (6%) ||\n|| De-En | 30.7 | 32.5 | +1.8 (5%) ||",
    "claim": "The gains made by our data diversification method are similar for both dropout=0.3 and dropout=0 conditions.",
    "label": "support"
  },
  {
    "id": "training_669_support",
    "table_caption": "Table: Link prediction results on the FB122 dataset.",
    "table_content": "|| [EMPTY] | [BOLD] Model | hits@3 | hits@5 | hits@10 | MRR ||\n|| With Rules | KALE-Pre guo2016jointly | 0.358 | 0.419 | 0.498 | 0.291 ||\n|| With Rules | KALE-Joint guo2016jointly | 0.384 | [BOLD] 0.447 | [BOLD] 0.522 | 0.325 ||\n|| With Rules | [ITALIC] ASR-DistMult minervini2017adversarial | 0.363 | 0.403 | 0.449 | 0.330 ||\n|| With Rules | [ITALIC] ASR-ComplEx minervini2017adversarial | 0.373 | 0.410 | 0.459 | 0.338 ||\n|| Without Rules | TransE bordes2013translating | 0.360 | 0.415 | 0.481 | 0.296 ||\n|| Without Rules | DistMult yang2017differentiable | 0.360 | 0.403 | 0.453 | 0.313 ||\n|| Without Rules | ComplEx trouillon2016complex | 0.370 | 0.413 | 0.462 | 0.329 ||\n|| Without Rules | GNTPs minervini2019differentiable | 0.337 | 0.369 | 0.412 | 0.313 ||\n|| Without Rules | CBR (Ours) | [BOLD] 0.400 | [BOLD] 0.445 | [BOLD] 0.488 | [BOLD] 0.359 ||",
    "claim": "CBR outperforms GNTPs and most models with access to hand-coded rules on the FB122 dataset.",
    "label": "support"
  },
  {
    "id": "training_676_support",
    "table_caption": "Table: Evaluation of captions generated using the proposed method. G means greedy decoding, and T1−2 means using constrained beam search [2] with 1−2 top detected concepts. ∗ is the result using VGG-16 [41] and † is the result using ResNet-101.",
    "table_content": "|| Method | Out-of-Domain Test Data bottle | Out-of-Domain Test Data bus | Out-of-Domain Test Data couch | Out-of-Domain Test Data microwave | Out-of-Domain Test Data pizza | Out-of-Domain Test Data racket | Out-of-Domain Test Data suitcase | Out-of-Domain Test Data zebra | Out-of-Domain Test Data Avg | Out-of-Domain Test Data SPICE | Out-of-Domain Test Data METEOR | Out-of-Domain Test Data CIDEr | In-Domain Test Data SPICE | In-Domain Test Data METEOR | In-Domain Test Data CIDER ||\n|| DCC  | 4.6 | 29.8 | 45.9 | 28.1 | 64.6 | 52.2 | 13.2 | 79.9 | 39.8 | 13.4 | 21.0 | 59.1 | 15.9 | 23.0 | 77.2 ||\n|| NOC  | 17.8 | 68.8 | 25.6 | 24.7 | 69.3 | 68.1 | 39.9 | 89.0 | 49.1 | - | 21.4 | - | - | - | - ||\n|| C-LSTM  | 29.7 | 74.4 | 38.8 | 27.8 | 68.2 | 70.3 | 44.8 | 91.4 | 55.7 | - | 23.0 | - | - | - | - ||\n|| Base+T4  | 16.3 | 67.8 | 48.2 | 29.7 | 77.2 | 57.1 | 49.9 | 85.7 | 54.0 | 15.9 | 23.3 | 77.9 | 18.0 | 24.5 | 86.3 ||\n|| NBT∗+G | 7.1 | 73.7 | 34.4 | 61.9 | 59.9 | 20.2 | 42.3 | 88.5 | 48.5 | 15.7 | 22.8 | 77.0 | 17.5 | 24.3 | 87.4 ||\n|| NBT†+G | 14.0 | 74.8 | 42.8 | 63.7 | 74.4 | 19.0 | 44.5 | 92.0 | 53.2 | 16.6 | 23.9 | 84.0 | [BOLD] 18.4 | 25.3 | 94.0 ||\n|| NBT†+T1 | 36.2 | 77.7 | 43.9 | 65.8 | 70.3 | 19.8 | 51.2 | 93.7 | 57.3 | 16.7 | 23.9 | 85.7 | [BOLD] 18.4 | [BOLD] 25.5 | [BOLD] 95.2 ||\n|| NBT†+T2 | [BOLD] 38.3 | [BOLD] 80.0 | [BOLD] 54.0 | [BOLD] 70.3 | [BOLD] 81.1 | [BOLD] 74.8 | [BOLD] 67.8 | [BOLD] 96.6 | [BOLD] 70.3 | [BOLD] 17.4 | [BOLD] 24.1 | [BOLD] 86.0 | 18.0 | 25.0 | 92.1 ||",
    "claim": "NBT†+T2 achieves the highest scores across all out-of-domain test data categories and average scores.",
    "label": "support"
  },
  {
    "id": "training_826_support",
    "table_caption": "Table: Test Results for event sequencing. The Oracle Cluster+Temporal system is using Caevo’s result on the Oracle Clusters.",
    "table_content": "|| [EMPTY] | Prec. | Recall | F-Score ||\n|| Oracle Cluster+Temporal | [BOLD] 46.21 | 8.72 | 14.68 ||\n|| Our Model | 18.28 | [BOLD] 16.91 | [BOLD] 17.57 ||",
    "claim": "Oracle Cluster+Temporal achieves higher precision, while Our Model achieves higher recall and F-score.",
    "label": "support"
  },
  {
    "id": "training_795_support",
    "table_caption": "Table: Ablation studies on components of our model. (Scores are sorted by the delta from the full model.)",
    "table_content": "|| Ablation | AMR 1.0 | AMR 2.0 ||\n|| Full model | 70.2 | 76.3 ||\n|| no source-side copy | 62.7 | 70.9 ||\n|| no target-side copy | 66.2 | 71.6 ||\n|| no coverage loss | 68.5 | 74.5 ||\n|| no BERT embeddings | 68.8 | 74.6 ||\n|| no index embeddings | 68.5 | 75.5 ||\n|| no anonym. indicator embed. | 68.9 | 75.6 ||\n|| no beam search | 69.2 | 75.3 ||\n|| no POS tag embeddings | 69.2 | 75.7 ||\n|| no CharCNN features | 70.0 | 75.8 ||\n|| only edge prediction | 88.4 | 90.9 ||",
    "claim": "Removing target-side copy leads to a large drop in performance on both AMR 1.0 and AMR 2.0.",
    "label": "support"
  },
  {
    "id": "training_713_support",
    "table_caption": "Table: ROUGE-1/2/L F1 scores of our model and versions thereof with less synthetic data (second block), using only one noising method (third block), and without some modules (fourth block).",
    "table_content": "|| [EMPTY] | Rotten Tomatoes | Rotten Tomatoes | Rotten Tomatoes | Yelp | Yelp | Yelp ||\n|| Model | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE-1 | ROUGE-2 | ROUGE-L ||\n|| DenoiseSum | [BOLD] 21.26 | 4.61 | [BOLD] 16.27 | [BOLD] 30.14 | [BOLD] 4.99 | [BOLD] 17.65 ||\n|| 10% synthetic dataset | 20.16 | 3.14 | 15.39 | 28.54 | 3.63 | 16.22 ||\n|| 50% synthetic dataset | 20.76 | 3.91 | 15.76 | 29.16 | 4.40 | 17.54 ||\n|| no segment noising | 20.64 | 4.39 | 16.03 | 28.93 | 4.31 | 16.88 ||\n|| no document noising | 21.23 | 4.38 | 16.22 | 28.75 | 4.06 | 16.67 ||\n|| no explicit denoising | 21.17 | 4.18 | 16.06 | 28.60 | 4.10 | 17.06 ||\n|| no partial copy | 20.76 | 4.01 | 15.89 | 28.03 | 4.58 | 16.31 ||\n|| no discriminator | 20.77 | 4.48 | 15.84 | 29.09 | 4.22 | 16.64 ||\n|| using human categories | 20.67 | [BOLD] 4.69 | 15.87 | 28.54 | 4.02 | 15.86 ||",
    "claim": "The final model, DenoiseSum, achieves the highest ROUGE-1 and ROUGE-L scores on both Rotten Tomatoes and Yelp datasets compared to other model variations.",
    "label": "support"
  },
  {
    "id": "training_722_support",
    "table_caption": "Table: Document-level F1-score calculated by comparing document attention from ELDAN and human coders on 20 CPT codes. #enc is the number of encounters that contains the code. #doc is the number of documents within those encounters. #source is the number of documents being labeled by human coders as the source documents for the code. Attention (from ELDAN) and Chance both report document-level F1-score, and Diff is the difference between them.",
    "table_content": "|| CPT Codes | #enc | #doc | #source | Attention | Chance | Diff ||\n|| 43239 | 8 | 19 | 9 | 88.89 | 59.22 | 29.67 ||\n|| 45380 | 5 | 11 | 5 | 90.91 | 56.47 | 34.44 ||\n|| 45385 | 6 | 13 | 8 | 85.71 | 67.52 | 18.20 ||\n|| 66984 | 7 | 13 | 7 | 100.00 | 68.65 | 31.35 ||\n|| 45378 | 10 | 20 | 11 | 90.91 | 67.44 | 23.47 ||\n|| 12001 | 1 | 3 | 1 | 100.00 | 45.63 | 54.37 ||\n|| 12011 | 3 | 8 | 3 | 57.14 | 54.30 | 2.85 ||\n|| 29125 | 2 | 9 | 4 | 72.73 | 50.91 | 21.81 ||\n|| 10060 | 4 | 9 | 6 | 100.00 | 71.65 | 28.35 ||\n|| 69436 | 7 | 18 | 8 | 87.50 | 60.54 | 26.96 ||",
    "claim": "ELDAN's document attention scores are consistently higher than the chance baseline for all listed CPT codes.",
    "label": "support"
  },
  {
    "id": "training_803_support",
    "table_caption": "Table: DistilBERT yields to comparable performance on downstream tasks. Comparison on downstream tasks: IMDb (test accuracy) and SQuAD 1.1 (EM/F1 on dev set). D: with a second step of distillation during fine-tuning.",
    "table_content": "|| Model | IMDb | SQuAD ||\n|| [EMPTY] | (acc.) | (EM/F1) ||\n|| BERT-base | 93.46 | 81.2/88.5 ||\n|| DistilBERT | 92.82 | 77.7/85.8 ||\n|| DistilBERT (D) | - | 79.1/86.9 ||",
    "claim": "On SQuAD, DistilBERT achieves an EM/F1 score of 77.7/85.8, while BERT-base achieves 81.2/88.5.",
    "label": "support"
  },
  {
    "id": "training_719_support",
    "table_caption": "Table: Comparison of readers with different auxiliary losses.",
    "table_content": "|| Configuration | HasAns EM | HasAns F1 | All EM | All F1 | NoAns ACC ||\n|| RMR | 72.6 | 81.6 | 66.9 | 69.1 | 73.1 ||\n|| - indep-I | 71.3 | 80.4 | 66.0 | 68.6 | 72.8 ||\n|| - indep-II | 72.4 | 81.4 | 64.0 | 66.1 | 69.8 ||\n|| - both | 71.9 | 80.9 | 65.2 | 67.5 | 71.4 ||\n|| RMR + ELMo | 79.4 | 86.8 | 71.4 | 73.7 | 77.0 ||\n|| - indep-I | 78.9 | 86.5 | 71.2 | 73.5 | 76.7 ||\n|| - indep-II | 79.5 | 86.6 | 69.4 | 71.4 | 75.1 ||\n|| - both | 78.7 | 86.2 | 70.0 | 71.9 | 75.3 ||",
    "claim": "Removing the independent span loss (indep-I) results in a performance drop for HasAns EM and HasAns F1, while removing the independent no-answer loss (indep-II) leads to a decline in NoAns ACC.",
    "label": "support"
  },
  {
    "id": "training_836_support",
    "table_caption": "Table: Comparison on German→English.",
    "table_content": "|| toolkit | BLEU [%] 2015 | BLEU [%] 2017 ||\n|| RETURNN | [BOLD] 31.2 | [BOLD] 31.3 ||\n|| Sockeye | 29.7 | 30.2 ||",
    "claim": "RETURNN achieves higher BLEU scores than Sockeye in both 2015 and 2017.",
    "label": "support"
  },
  {
    "id": "training_845_support",
    "table_caption": "Table: Quality filtering test results after tuning quality hyperparameters on development data (averaged over languages and data sources for the NMT rows). Results are on STS datasets (Pearson’s r×100).",
    "table_content": "|| Filtering Method | GRAN | Avg ||\n|| None (Random) | 66.9 | 65.5 ||\n|| Translation Cost | 66.6 | 65.4 ||\n|| Language Model | 66.7 | 65.5 ||\n|| Reference Classification | 67.0 | 65.5 ||",
    "claim": "Reference Classification slightly outperforms Random selection with the GRAN model.",
    "label": "support"
  },
  {
    "id": "training_775_support",
    "table_caption": "Table: Tagging accuracies of domain adaptation models from the PTB to the PPCEME.",
    "table_content": "|| System | IV | OOV | All ||\n|| SVM | 81.68 | 48.96 | 74.15 ||\n|| SCL | 82.01 | 55.45 | 75.89 ||\n|| Brown | 81.81 | 56.76 | 76.04 ||\n|| word2vec | 81.79 | 56.00 | 75.85 ||\n|| Fema-single | 82.30 | 62.63 | 77.77 ||\n|| Fema-attribute | 82.34 | 63.16 | 77.92 ||",
    "claim": "Fema-attribute achieves higher accuracy on OOV tokens compared to Fema-single, and SCL performs better than Brown clustering and word2vec on IV tokens but worse on OOV tokens.",
    "label": "support"
  },
  {
    "id": "training_762_support",
    "table_caption": "Table: Experiment results of our model in comparison with several baselines, including increments of the number of parameters over Transformer baseline (\\Delta|{\\bm{\\theta}}|), training/testing speeds (v_{\\rm{train}}/v_{\\rm{test}}, some of them are derived from maruf2019selective [maruf2019selective]), and translation results of the testsets in BLEU score.",
    "table_content": "|| [BOLD] Model | \\Delta|{\\bm{\\theta}}| | v_{\\rm{train}} | v_{\\rm{test}} | Zh-En TED | En-De TED | En-De News | En-De Europarl | En-De avg. ||\n|| SentNmt [Vaswani2017Attention] | 0.0m | 1.0\\times | 1.0\\times | 17.0 | 23.10 | 22.40 | 29.40 | 24.96 ||\n|| DocT [zhang2018improving] | 9.5m | 0.65\\times | 0.98\\times | [EMPTY] | 24.00 | 23.08 | 29.32 | 25.46 ||\n|| HAN [miculicich2018document] | 4.8m | 0.32\\times | 0.89\\times | 17.9 | 24.58 | [BOLD] 25.03 | 28.60 | 26.07 ||\n|| SAN [maruf2019selective] | 4.2m | 0.51\\times | 0.86\\times | [EMPTY] | 24.42 | 24.84 | 29.75 | 26.33 ||\n|| QCN [yang2019enhancing] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [BOLD] 25.19 | 22.37 | 29.82 | 25.79 ||\n|| Final | 4.7m | 0.22\\times | 1.08\\times | [BOLD] 19.1 | 25.10 | 24.91 | [BOLD] 30.40 | [BOLD] 26.80 ||",
    "claim": "The \"Final\" model achieves the highest scores on TED Zh-En and Europarl tasks and has the best average performance on English-German benchmarks.",
    "label": "support"
  },
  {
    "id": "training_787_support",
    "table_caption": "Table: Accuracy of LAMBADA vs. other generative approaches over all datasets and classifiers. LAMBADA is statistically (* McNemar, p−value<0.01) superior to all models on each classifier and each dataset (on par to EDA with SVM on TREC).",
    "table_content": "|| Dataset | [EMPTY] | BERT | SVM | LSTM ||\n|| ATIS | Baseline | 53.3 | 35.6 | 29.0 ||\n|| ATIS | EDA | 62.8 | 35.7 | 27.3 ||\n|| ATIS | CVAE | 60.6 | 27.6 | 14.9 ||\n|| ATIS | CBERT | 51.4 | 34.8 | 23.2 ||\n|| ATIS | LAMBADA | [BOLD] 75.7* | [BOLD] 56.5* | [BOLD] 33.7* ||\n|| TREC | Baseline | 60.3 | 42.7 | 17.7 ||\n|| TREC | EDA | 62.6 | [BOLD] 44.8* | 23.1 ||\n|| TREC | CVAE | 61.1 | 40.9 | 25.4* ||\n|| TREC | CBERT | 61.4 | 43.8 | 24.2 ||\n|| TREC | LAMBADA | [BOLD] 64.3* | 43.9* | [BOLD] 25.8 * ||\n|| WVA | Baseline | 67.2 | 60.2 | 26.0 ||\n|| WVA | EDA | 67.0 | 60.7 | 28.2 ||\n|| WVA | CVAE | 65.4 | 54.8 | 22.9 ||\n|| WVA | CBERT | 67.4 | 60.7 | 28.4 ||\n|| WVA | LAMBADA | [BOLD] 68.6* | [BOLD] 62.9* | [BOLD] 32.0* ||",
    "claim": "On the TREC dataset with the SVM classifier, the method is on par with EDA.",
    "label": "support"
  },
  {
    "id": "training_772_support",
    "table_caption": "Table: Accuracy results for temporal adaptation in the PPCMBE and the PPCEME of historical English. Percentage error reduction is shown for the best-performing method, Fema-attribute.",
    "table_content": "|| Task | baseline SVM | baseline MEMM (Stanford) | SCL | Brown | word2vec | Fema single embedding | Fema attribute embeddings (error reduction) ||\n|| [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [EMPTY] ||\n|| → 1770-1839 | 96.30 | 96.57 | 96.42 | 96.45 | 96.44 | 96.80 | [BOLD] 96.84 (15%) ||\n|| → 1700-1769 | 94.57 | 94.83 | 95.07 | 95.15 | 94.85 | 95.65 | [BOLD] 95.75 (22%) ||\n|| average | 95.43 | 95.70 | 95.74 | 95.80 | 95.64 | 96.23 | [BOLD] 96.30 (19%) ||\n|| [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [EMPTY] ||\n|| → 1570-1639 | 93.62 | 93.98 | 94.23 | 94.36 | 94.18 | 95.01 | [BOLD] 95.20 (25%) ||\n|| → 1500-1569 | 87.59 | 87.47 | 89.39 | 89.73 | 89.30 | 91.40 | [BOLD] 91.63 (33%) ||\n|| average | 90.61 | 90.73 | 91.81 | 92.05 | 91.74 | 93.20 | [BOLD] 93.41 (30%) ||",
    "claim": "Fema attribute embeddings achieve the highest accuracy across all tasks compared to other models.",
    "label": "support"
  },
  {
    "id": "training_769_support",
    "table_caption": "Table: Results on the test set using all subject’s history as a single document, i.e. timeless classification.",
    "table_content": "|| [EMPTY] | [ITALIC] F1 | [ITALIC] π | [ITALIC] ρ ||\n|| SS3 | [BOLD] 0.61 | [BOLD] 0.63 | 0.60 ||\n|| LOGREG | 0.59 | 0.56 | 0.63 ||\n|| SVM | 0.55 | 0.5 | 0.62 ||\n|| MNB | 0.39 | 0.25 | [BOLD] 0.96 ||\n|| KNN | 0.54 | 0.5 | 0.58 ||",
    "claim": "SS3 obtained the highest values for F1 (0.61) and Precision (0.63), while MNB had the highest Recall (0.96).",
    "label": "support"
  },
  {
    "id": "training_770_support",
    "table_caption": "Table: Effect of dimensionlity: additional results for the test performance of GLOSS-BoW and GLOSS-Pos models with different dimensionality of the latent vectors on unsupervised STS-(12-16, B) and supervised tasks. (∗) Following SentEval, STS-13 does not include the SMT dataset due to licensing issues. Best results for each task are in bold.",
    "table_content": "|| Model | Config. #Tok | Config. Dim | Unsupervised STS-() tasks 12 | Unsupervised STS-() tasks 13* | Unsupervised STS-() tasks 14 | Unsupervised STS-() tasks 15 | Unsupervised STS-() tasks 16 | Unsupervised STS-() tasks B | Unsupervised STS-() tasks Avg | Supervised tasks MR | Supervised tasks CR | Supervised tasks SUBJ | Supervised tasks MPQA | Supervised tasks TREC | Supervised tasks Avg ||\n|| GLOSS-BoW | 27M | 100 | 54.8 | 51.8 | 68.4 | 71.2 | [BOLD] 71.8 | [BOLD] 72.4 | 65.1 | 67.4 | 72.0 | 86.4 | 79.5 | 71.0 | 75.3 ||\n|| GLOSS-BoW | 27M | 300 | [BOLD] 55.9 | 55.6 | [BOLD] 69.2 | 73.4 | 71.2 | 72.1 | [BOLD] 66.2 | 69.5 | 74.7 | 88.6 | 82.3 | 78.0 | 78.6 ||\n|| GLOSS-BoW | 27M | 700 | 54.9 | [BOLD] 55.8 | 68.8 | [BOLD] 73.7 | 71.0 | 71.4 | 65.9 | 72.4 | 76.7 | 90.2 | 83.7 | [BOLD] 82.2 | 81.0 ||\n|| GLOSS-POS | 27M | 100 | 54.6 | 54.8 | 68.3 | 71.7 | 71.4 | 69.7 | 65.1 | 68.8 | 73.9 | 87.0 | 83.3 | 74.8 | 77.6 ||\n|| GLOSS-POS | 27M | 300 | 54.2 | 52.7 | 68.1 | 73.4 | 70.5 | 69.0 | 64.7 | 71.8 | 75.5 | 89.3 | 84.7 | 80.2 | 80.3 ||\n|| GLOSS-POS | 27M | 700 | 53.6 | 53.3 | 67.8 | 73.3 | 70.1 | 68.1 | 64.4 | 72.7 | 77.4 | 89.9 | 85.4 | 81.4 | 81.4 ||\n|| GLOSS-POS | 27M | 1K | 53.0 | 52.9 | 67.5 | 72.0 | 69.8 | 68.0 | 63.9 | [BOLD] 73.4 | [BOLD] 78.1 | [BOLD] 91.0 | [BOLD] 86.3 | 82.1 | [BOLD] 82.2 ||",
    "claim": "GLOSS-BoW is better than GLOSS-POS for unsupervised tasks, and increasing dimensionality does not improve accuracy on unsupervised tasks.",
    "label": "support"
  },
  {
    "id": "training_827_support",
    "table_caption": "Table: Ablation Study for Event Sequencing.",
    "table_content": "|| [EMPTY] | Prec. | Recall | F-Score | Δ ||\n|| Full | 37.92 | 36.79 | 36.36 | [EMPTY] ||\n|| - Mention Type | 32.78 | 29.81 | 30.07 | 6.29 ||\n|| - Sentence | 33.90 | 30.75 | 31.00 | 5.36 ||\n|| - Temporal | 37.21 | 36.53 | 35.81 | 0.55 ||\n|| - Dependency | 38.18 | 36.44 | 36.23 | 0.13 ||\n|| - Function words | 38.08 | 36.51 | 36.18 | 0.18 ||",
    "claim": "Removing the \"Mention Type\" and \"Sentence\" features results in a significant drop in both precision and recall.",
    "label": "support"
  },
  {
    "id": "training_712_support",
    "table_caption": "Table: ROUGE-L of our model and versions thereof with less synthetic data (second block), using only one noising method (third block), and without some modules (fourth block). A more comprehensive table and discussion can be found in the Appendix.",
    "table_content": "|| Model | RT | Yelp ||\n|| DenoiseSum | 16.27 | 17.65 ||\n|| 10% synthetic dataset | 15.39 | 16.22 ||\n|| 50% synthetic dataset | 15.76 | 17.54 ||\n|| no segment noising | 16.03 | 16.88 ||\n|| no document noising | 16.22 | 16.67 ||\n|| no explicit denoising | 16.06 | 17.06 ||\n|| no partial copy | 15.89 | 16.31 ||\n|| no discriminator | 15.84 | 16.64 ||\n|| using human categories | 15.87 | 15.86 ||",
    "claim": "Increasing the size of the synthetic dataset improves performance, and both segment and document noising contribute to better results. Human-labeled categories decrease model performance compared to other methods.",
    "label": "support"
  },
  {
    "id": "training_782_support",
    "table_caption": "Table: BLEU scores for models with and without back-translation (BT) on the IWSLT’14 English-German (En-De), German-English (De-En) and WMT’14 En-De tasks. Column |D| shows the total data used in back-translation compared to the original parallel data.",
    "table_content": "|| [BOLD] Task | [BOLD] No back-translation  [BOLD] Baseline | [BOLD] No back-translation  [BOLD] Ours | [BOLD] With back-translation | [ITALIC] D| | [BOLD] With back-translation  [BOLD] Baseline | [BOLD] With back-translation  [BOLD] Ours ||\n|| IWSLT’14 En-De | 28.6 | 30.6 | 29× | 30.0 | [BOLD] 31.8 ||\n|| IWSLT’14 De-En | 34.7 | 37.0 | 29× | 37.1 | [BOLD] 38.5 ||\n|| WMT’14 En-De | 29.3 | 30.7 | 2.4× | 30.8 | [BOLD] 31.8 ||",
    "claim": "With back-translation, the proposed method achieves higher BLEU scores than the baseline in all tasks.",
    "label": "support"
  },
  {
    "id": "training_798_support",
    "table_caption": "Table: Inter-annotator agreement for our English (goldEN) and German (goldDE) gold standard, as well as the lexicon by Warriner13 for comparision; Averaged standard deviation of ratings for each VAD dimension and mean over all dimensions.",
    "table_content": "|| [EMPTY] | Valence | Arousal | Dominance | Mean ||\n|| goldEN | 1.20 | 1.08 | 1.41 | 1.23 ||\n|| goldDE | 1.72 | 1.56 | 2.31 | 1.86 ||\n|| Warriner | 1.68 | 2.30 | 2.16 | 2.05 ||",
    "claim": "The gold standard lexicons display higher rating consistency with IAA scores of 1.23 for English and 1.86 for German, compared to 2.05 for the Warriner lexicon.",
    "label": "support"
  },
  {
    "id": "training_843_support",
    "table_caption": "Table: Test correlations for our models when trained on sentences with particular length ranges (averaged over languages and data sources for the NMT rows). Results are on STS datasets (Pearson’s r×100).",
    "table_content": "|| Data | Model | Length Range 0-10 | Length Range 10-20 | Length Range 20-30 | Length Range 30-100 ||\n|| SimpWiki | GRAN | 67.4 | 67.7 | 67.1 | 67.3 ||\n|| SimpWiki | Avg | 65.9 | 65.7 | 65.6 | 65.9 ||\n|| NMT | GRAN | 66.6 | 66.5 | 66.0 | 64.8 ||\n|| NMT | Avg | 65.7 | 65.6 | 65.3 | 65.0 ||",
    "claim": "Performance for NMT data decreases as sentence length increases for both GRAN and Avg models.",
    "label": "support"
  },
  {
    "id": "training_835_support",
    "table_caption": "Table: Training speed and memory consumption on WMT 2017 German→English. Train time is for seeing the full train dataset once. Batch size is in words, such that it almost maximizes the GPU memory consumption. The BLEU score is for the converged models, reported for newstest2015 (dev) and newstest2017. The encoder has one bidirectional LSTM layer and either 3 or 5 unidirectional LSTM layers.",
    "table_content": "|| toolkit | encoder n. layers | time [h] | batch size | BLEU [%] 2015 | BLEU [%] 2017 ||\n|| RETURNN | 4 | [BOLD] 11.25 | 8500 | 28.0 | 28.4 ||\n|| Sockeye | [EMPTY] | 11.45 | 3000 | [BOLD] 28.9 | [BOLD] 29.2 ||\n|| RETURNN | 6 | [BOLD] 12.87 | 7500 | 28.7 | 28.7 ||\n|| Sockeye | [EMPTY] | 14.76 | 2500 | [BOLD] 29.4 | [BOLD] 29.1 ||",
    "claim": "Sockeye achieves higher BLEU scores than RETURNN for both 2015 and 2017, but RETURNN has shorter training times.",
    "label": "support"
  },
  {
    "id": "training_730_support",
    "table_caption": "Table: [Left] Summaries evaluated by Jensen-Shannon divergence (JSD) on Yahoo Answer for summaries of 100 words and 200 words. The average length of the best answer is 102.70. [Right] Value addition of each component in the objective function. The JSD on each line is statistically significantly lower than the JSD on the previous (α=0.05).",
    "table_content": "|| [EMPTY] | [BOLD] Length 100 | [BOLD] Length 200 ||\n|| Best answer | 0.3858 | - ||\n|| Lin:2011:CSF:2002472.2002537 | 0.3398 | 0.2008 ||\n|| Lin:2011:CSF:2002472.2002537 + q | 0.3379 | 0.1988 ||\n|| dasgupta-kumar-ravi:2013:ACL2013 | 0.3316 | 0.1939 ||\n|| Our system | [BOLD] 0.3017 | [BOLD] 0.1758 ||",
    "claim": "Both \"our system\" and \"dasgupta-kumar-ravi:2013:ACL2013\" produce better JSD scores than the two variants of the \"Lin:2011:CSF:2002472.2002537\" system at length 100.",
    "label": "support"
  },
  {
    "id": "training_765_support",
    "table_caption": "Table: Comparison between UR-FUNNY and notable humor detection datasets in the NLP community. Here, ‘pos’, ’neg’ , ‘mod’ and ‘spk’ denote positive, negative, modalities and speaker respectively.",
    "table_content": "|| Dataset | #Pos | #Neg | Mod | type | #spk ||\n|| 16000 One-Liners | 16000 | 16000 | { [ITALIC] l} | joke | - ||\n|| Pun of the Day | 2423 | 2423 | { [ITALIC] l} | pun | - ||\n|| PTT Jokes | 1425 | 2551 | { [ITALIC] l} | political | - ||\n|| Ted Laughter | 4726 | 4726 | { [ITALIC] l} | speech | 1192 ||\n|| Big Bang Theory | 18691 | 24981 | { [ITALIC] l,a} | tv show | <50 ||\n|| [BOLD] UR-Funny | 8257 | 8257 | { [ITALIC] l,a,v} | speech | 1741 ||",
    "claim": "UR-FUNNY is the only humor detection dataset that incorporates all three modalities of text, vision, and audio.",
    "label": "support"
  },
  {
    "id": "training_761_support",
    "table_caption": "Table: Accuracy (\\%) of discourse phenomena. * different data and system conditions, only for reference.",
    "table_content": "|| [BOLD] Model | deixis | lex.c. | ell.infl. | ell.VP ||\n|| SentNmt | 50.0 | 45.9 | 52.2 | 24.2 ||\n|| Ours | 61.3 | 46.1 | 61.0 | 35.6 ||\n|| voita2018context [voita2018context]* | 81.6 | 58.1 | 72.2 | 80.0 ||",
    "claim": "The \"Ours\" model outperforms the SentNmt model across all discourse phenomena test sets.",
    "label": "support"
  },
  {
    "id": "training_850_support",
    "table_caption": "Table: Comparison between machine translation and human interpretation. The interpretation reference consists of a collection of interpretations from S, A and B. Our model is trained on the large-scale corpus.",
    "table_content": "|| Models | Translation Reference BLEU | Translation Reference Brevity Penalty | Interpretation Reference (3-references) BLEU | Interpretation Reference (3-references) Brevity Penalty ||\n|| [ITALIC] Our Model | 20.93 | 1.000 | 28.08 | 1.000 ||\n|| [ITALIC] S | 16.02 | 0.845 | - | - ||\n|| [ITALIC] A | 16.38 | 0.887 | - | - ||\n|| [ITALIC] B | 12.08 | 0.893 | - | - ||",
    "claim": "Our Model achieves the highest Translation Reference BLEU score and Interpretation Reference BLEU score compared to models S, A, and B.",
    "label": "support"
  },
  {
    "id": "training_844_support",
    "table_caption": "Table: Length filtering test results after tuning length ranges on development data (averaged over languages and data sources for the NMT rows). Results are on STS datasets (Pearson’s r×100).",
    "table_content": "|| Filtering Method | NMT GRAN | NMT Avg | SimpWiki GRAN | SimpWiki Avg ||\n|| None (Random) | 66.9 | 65.5 | 67.2 | 65.8 ||\n|| Length | 67.3 | 66.0 | 67.4 | 66.2 ||\n|| Tuned Len. Range | [0,10] | [0,10] | [0,10] | [0,15] ||",
    "claim": "Length-based filtering leads to higher performance than the random baseline for both NMT and SimpWiki.",
    "label": "support"
  },
  {
    "id": "training_858_support",
    "table_caption": "Table: ROUGE F1 and METEOR scores of sanity check ablations, evaluated on CNN/DM validation set.",
    "table_content": "|| [EMPTY] | ROUGE 1 | ROUGE 2 | ROUGE L ||\n|| pg baseline | 37.73 | 16.52 | 34.49 ||\n|| pg + ptrdec | 37.66 | 16.50 | 34.47 ||\n|| pg-2layer | 37.92 | 16.48 | 34.62 ||\n|| pg-big | 38.03 | 16.71 | 34.84 ||\n|| pg + cbdec | [BOLD] 38.87 | [BOLD] 16.93 | [BOLD] 35.38 ||",
    "claim": "The \"pg + cbdec\" model achieves the highest scores across all ROUGE metrics.",
    "label": "support"
  },
  {
    "id": "training_821_support",
    "table_caption": "Table: Percentage of sentences in the correct language computed with langdetect.",
    "table_content": "|| [EMPTY] | De | Nl | Es | Fr | It | Pt ||\n|| M-Pre | 95.7 | 98.5 | 97.2 | 94.6 | 95.3 | 96.6 ||\n|| M-Pre + ASR | 96.1 | 98.7 | 97.9 | 95.3 | 95.4 | 95.2 ||",
    "claim": "The percentage of correct language identification increases slightly in all languages except for Portuguese when using ASR data.",
    "label": "support"
  },
  {
    "id": "training_829_support",
    "table_caption": "Table: Analysis of errors by a human annotator of the top ten retrievals on development data. Percentages (%) indicate the absolute drop in P@10 due to that error type.",
    "table_content": "|| Error type | XVisionSpeech Count | XVisionSpeech % | XBoWCNN Count | XBoWCNN % ||\n|| (1) Correct (exact) | 032 | 08.2 | 45 | 11.5 ||\n|| (2) Semantically related | 086 | 22.1 | 13 | 03.3 ||\n|| (3) Incorrect retrieval | 035 | 09.0 | 19 | 04.9 ||\n|| Total | 153 | 39.3 | 77 | 19.7 ||",
    "claim": "The bulk of errors from XVisionSpeechCNN is due to semantically related retrievals, with 22.1% of errors falling into this category.",
    "label": "support"
  },
  {
    "id": "training_776_support",
    "table_caption": "Table: WER results",
    "table_content": "|| [EMPTY] | 1st decoding with LM0 | Rescoring with LM2 | Rescoring with LM1 | Proposed method ||\n|| Test-clean | 18.8 | - | 11.50 | 11.50 ||\n|| Test-other | 33.35 | - | 22.59 | 22.59 ||\n|| Reading-test | 3.84 | 2.42 | - | 2.42 ||\n|| Conversation-test | 20.54 | 19.00 | - | 19.2 ||\n|| YouTube-test | 24.55 | 21.57 | - | 21.83 ||\n|| VLSP2018 | 10.12 | 8.16 | - | 8.16 ||",
    "claim": "The proposed method achieves a WER of 11.50 on Test-clean, 22.59 on Test-other, 2.42 on Reading-test, 19.2 on Conversation-test, 21.83 on YouTube-test, and 8.16 on VLSP2018.",
    "label": "support"
  },
  {
    "id": "training_766_support",
    "table_caption": "Table: Comparison of validation and test set perplexity for r-RNTNs with f mapping (K=100 for PTB, K=376 for text8) versus s-RNNs and m-RNN. r-RNTNs with the same H as corresponding s-RNNs significantly increase model capacity and performance with no computational cost. The RNTN was not run on text8 due to the number of parameters required.",
    "table_content": "|| Method | [ITALIC] H | PTB # Params | PTB Test PPL | text8 # Params | text8 Test PPL | Method | [ITALIC] H | PTB # Params | PTB Test PPL ||\n|| s-RNN | 100 | 2M | 146.7 | 7.6M | 236.4 | GRU | 244 | 9.6M | 92.2 ||\n|| r-RNTN  [ITALIC] f | 100 | 3M | 131.2 | 11.4M | 190.1 | GRU | 650 | 15.5M | 90.3 ||\n|| RNTN | 100 | 103M | 128.8 | 388M | - | r-GRU  [ITALIC] f | 244 | 15.5M | [BOLD] 87.5 ||\n|| m-RNN | 100 | 3M | 164.2 | 11.4M | 895.0 | LSTM | 254 | 10M | 88.8 ||\n|| s-RNN | 150 | 3M | 133.7 | 11.4M | 207.9 | LSTM | 650 | 16.4M | [BOLD] 84.6 ||\n|| r-RNTN  [ITALIC] f | 150 | 5.3M | [BOLD] 126.4 | 19.8M | [BOLD] 171.7 | r-LSTM  [ITALIC] f | 254 | 16.4M | 87.1 ||",
    "claim": "The r-RNTN with H=100 approaches the performance of the RNTN with a smaller number of parameters.",
    "label": "support"
  },
  {
    "id": "training_864_support",
    "table_caption": "Table: GEC results with W&I+LOCNESS test data.",
    "table_content": "|| Team | TP | FP | FN | P | R | F0.5 ||\n|| UEDIN-MS | 2,312 | 982 | 2,506 | 70.19 | 47.99 | 64.24 ||\n|| Kakao&Brain | 2,412 | 1,413 | 2,797 | 63.06 | 46.30 | 58.80 ||\n|| LAIX | 1,443 | 884 | 3,175 | 62.01 | 31.25 | 51.81 ||\n|| CAMB-CUED | 1,814 | 1,450 | 2,956 | 55.58 | 38.03 | 50.88 ||\n|| UFAL, Charles University, Prague | 1,245 | 1,222 | 2,993 | 50.47 | 29.38 | 44.13 ||\n|| Siteimprove | 1,299 | 1,619 | 3,199 | 44.52 | 28.88 | 40.17 ||\n|| WebSpellChecker.com | 2,363 | 3,719 | 3,031 | 38.85 | 43.81 | 39.75 ||\n|| TMU | 1,638 | 4,314 | 3,486 | 27.52 | 31.97 | 28.31 ||\n|| Buffalo | 446 | 1,243 | 3,556 | 26.41 | 11.14 | 20.73 ||",
    "claim": "The F0.5 score for TMU is 28.31, ranking eighth among the nine teams, and TMU has the highest number of false positives with 4,314.",
    "label": "support"
  },
  {
    "id": "training_790_support",
    "table_caption": "Table: Classification accuracies and F1-Scores for news arcticle classifications for different source and target domains. The first row corresponds to the baseline performance trained on only the target dataset. The following two rows shows the performance of instance-infusion method with and without the usage of penalty function. In all the three cases, our approach outperforms the baselines by a significant margin.",
    "table_content": "|| [BOLD] METHOD | Target Source | News20 BBC | News20 BBC | BBC News20 | BBC News20 | BBC Sports BBC | BBC Sports BBC ||\n|| [BOLD] METHOD | [EMPTY] | Accuracy | F1-Score | Accuracy | F1-Score | Accuracy | F1-Score ||\n|| Bi-LSTM (Target Only) | [EMPTY] | 65.17 | 0.6328 | 91.33 | 0.9122 | 84.22 | 0.8395 ||\n|| Instance-Infused Bi-LSTM | [EMPTY] | 76.44 | 0.7586 | 95.35 | 0.9531 | 88.78 | 0.8855 ||\n|| Instance-Infused Bi-LSTM (with penalty function) | [EMPTY] | [BOLD] 78.29 | [BOLD] 0.7773 | [BOLD] 96.09 | [BOLD] 0.9619 | [BOLD] 91.56 | [BOLD] 0.9100 ||",
    "claim": "The Instance-Infused Bi-LSTM (with penalty function) achieves an improvement of 12% in accuracy over the baseline model on the News20 BBC dataset and around 5% on the BBC News20 and BBC Sports BBC datasets.",
    "label": "support"
  },
  {
    "id": "training_778_support",
    "table_caption": "Table: Language model evaluation",
    "table_content": "|| [EMPTY] | [BOLD] Multilinguage LM0 | [BOLD] English LM1 | [BOLD] Vietnamese LM2 ||\n|| [BOLD] Test-clean | 569.3 | 187.1 | - ||\n|| [BOLD] Test-other | 522.2 | 174.3 | - ||\n|| [BOLD] Reading-test | 136.6 | - | 87.2 ||\n|| [BOLD] Conversation-test | 95.2 | - | 62.7 ||\n|| [BOLD] YouTube-test | 199.5 | - | 111.4 ||\n|| [BOLD] VLSP2018 | 75.7 | - | 47.5 ||",
    "claim": "LM0 has a perplexity of 569.3 on Test-clean and 522.2 on Test-other, while LM1 has a perplexity of 187.1 on Test-clean and 174.3 on Test-other. LM2 has a perplexity of 87.2 on Reading-test, 62.7 on Conversation-test, 111.4 on YouTube-test, and 47.5 on VLSP2018.",
    "label": "support"
  },
  {
    "id": "training_750_support",
    "table_caption": "Table: This table shows the performance of the SVM and NB classifier using the sliding window approach with stratified sampling. Function words classifications are marked with “FW”. All others were full text classifications. The plus and minus symbols indicate whether the performance metric was calculated for disruptive (+) or constructive contributions (−).",
    "table_content": "|| [BOLD] Classifier | [BOLD] Recall+ | [BOLD] Recall− | [BOLD] Precision+ | [BOLD] Precision− | [BOLD] F1+ | [BOLD] F1− | [BOLD] Accuracy | [BOLD] AUC ||\n|| SVM | 88.96 | 84.60 | 85.25 | 88.46 | 87.07 | 86.49 | 86.78 | 0.950 ||\n|| SVM (FW) | 78.73 | 50.75 | 61.52 | 70.47 | 69.07 | 59.01 | 64.74 | 0.710 ||\n|| NB | 69.57 | 93.43 | 91.37 | 75.43 | 78.99 | 83.47 | 81.50 | 0.670 ||\n|| NB (FW) | 42.27 | 79.33 | 67.16 | 57.88 | 51.88 | 66.93 | 60.80 | 0.670 ||",
    "claim": "SVM outperforms SVM (FW) across all performance metrics, including Recall+, Recall−, Precision+, Precision−, F1+, F1−, Accuracy, and AUC.",
    "label": "support"
  },
  {
    "id": "training_764_support",
    "table_caption": "Table: Binary accuracy for different variants of C-MFN and training scenarios outlined in Section 5. The best performance is achieved using all three modalities of text (T), vision (V) and acoustic (A).",
    "table_content": "|| Modality | T | A+V | T+A | T+V | T+A+V ||\n|| C-MFN (P) | 62.85 | 53.3 | 63.28 | 63.22 | 64.47 ||\n|| C-MFN (C) | 57.96 | 50.23 | 57.78 | 57.99 | 58.45 ||\n|| C-MFN | 64.44 | 57.99 | 64.47 | 64.22 | 65.23 ||",
    "claim": "C-MFN outperforms C-MFN (P) and C-MFN (C) across all modalities, and C-MFN (P) performs better than C-MFN (C).",
    "label": "support"
  },
  {
    "id": "training_773_support",
    "table_caption": "Table: Accuracy results for adapting from the PTB to the PPCMBE and the PPCEME of historical English. ∗Error reduction for the normalized PPCEME is computed against the unnormalized SVM accuracy, showing total error reduction.",
    "table_content": "|| Target | Normalized | baseline SVM | baseline MEMM (Stanford) | SCL | Brown | word2vec | Fema single embedding | Fema attribute embeddings (error reduction) ||\n|| ppcmbe | No | 81.12 | 81.35 | 81.66 | 81.65 | 81.75 | 82.34 | [BOLD] 82.46 (7%) ||\n|| ppceme | No | 74.15 | 74.34 | 75.89 | 76.04 | 75.85 | 77.77 | [BOLD] 77.92 (15%) ||\n|| ppceme | Yes | 76.73 | 76.87 | 77.61 | 77.65 | 77.76 | 78.85 | [BOLD] 79.05 (19%∗) ||",
    "claim": "Fema attribute embeddings improve performance by 1.34% on PPCMBE data and by 3.77% on unnormalized PPCEME data. Spelling normalization improves the baseline systems by more than 2.5%.",
    "label": "support"
  },
  {
    "id": "training_841_support",
    "table_caption": "Table: Pearson’s r and Mean Absolute Error (MAE) for prediction of MET scores (test set, 57 participants) and TOEFL scores (leave-one-out cross validation, all 53 participants) from eye movement patterns in reading. We consider two baselines which do not use eyetracking information: (1) the average proficiency score in the training set, which yields 4.82 MAE on MET and 8.29 MAE on TOEFL, and (2) the reading speed of the participant.",
    "table_content": "|| [EMPTY] | [BOLD] MET Fixed | [BOLD] MET Fixed | [BOLD] MET Any | [BOLD] MET Any | [BOLD] TOEFL Fixed | [BOLD] TOEFL Fixed | [BOLD] TOEFL Any | [BOLD] TOEFL Any ||\n|| [BOLD] Features | [ITALIC] r | MAE | [ITALIC] r | MAE | [ITALIC] r | MAE | [ITALIC] r | MAE ||\n|| Reading Speed | 0.27 | 4.58 | 0.24 | 4.62 | 0.09 | 7.92 | 0.06 | 7.96 ||\n|| WP-Coefficients | 0.43 | 4.11 | 0.44 | 4.14 | 0.34 | 7.76 | 0.31 | [BOLD] 7.49 ||\n|| S-Clusters | 0.56 | 3.87 | [BOLD] 0.49 | [BOLD] 4.11 | [BOLD] 0.55 | 7.45 | [BOLD] 0.50 | 7.76 ||\n|| Transitions | 0.52 | 3.93 | [EMPTY] | [EMPTY] | 0.38 | 7.11 | [EMPTY] | [EMPTY] ||\n|| WFC | [BOLD] 0.70 | [BOLD] 3.31 | [EMPTY] | [EMPTY] | 0.50 | [BOLD] 6.68 | [EMPTY] | [EMPTY] ||",
    "claim": "Eyetracking-based features outperform the average score and reading speed baselines on both MET and TOEFL in terms of MAE.",
    "label": "support"
  },
  {
    "id": "training_768_support",
    "table_caption": "Table: Comparison on dialog response generation. Reconstruction perplexity (Rec-P) and BLEU (B) scores are used for evaluation.",
    "table_content": "|| [BOLD] Model  [BOLD] Schedule | CVAE  [BOLD] M | CVAE  [BOLD] C | CVAE+BoW  [BOLD] M | CVAE+BoW  [BOLD] C ||\n|| Rec-P ↓ | 36.16 | [BOLD] 29.77 | 18.44 | [BOLD] 16.74 ||\n|| KL Loss ↑ | 0.265 | [BOLD] 4.104 | 14.06 | [BOLD] 15.55 ||\n|| B4 prec | 0.185 | [BOLD] 0.234 | 0.211 | [BOLD] 0.219 ||\n|| B4 recall | 0.122 | [BOLD] 0.220 | 0.210 | [BOLD] 0.219 ||\n|| A-bow prec | 0.957 | [BOLD] 0.961 | 0.958 | [BOLD] 0.961 ||\n|| A-bow recall | 0.911 | [BOLD] 0.941 | 0.938 | [BOLD] 0.940 ||\n|| E-bow prec | 0.867 | 0.833 | 0.830 | 0.828 ||\n|| E-bow recall | 0.784 | [BOLD] 0.808 | 0.808 | 0.805 ||",
    "claim": "The cyclical schedule results in lower reconstruction perplexity, higher KL loss, and higher BLEU scores for both CVAE and CVAE+BoW models.",
    "label": "support"
  },
  {
    "id": "training_791_support",
    "table_caption": "Table: Test Accuracy for proposed model using instances from multiple source datasets with 50% target dataset",
    "table_content": "|| Dataset | Single Source Accuracy | Single Source F1-Score | Multiple Sources Accuracy | Multiple Sources F1-Score ||\n|| News20 | 61.72 | 0.6133 | 67.32 | 0.6650 ||\n|| BBC | 91.01 | 0.9108 | 91.41 | 0.9120 ||\n|| BBC Sports | 81.72 | 0.7990 | 82.81 | 0.8027 ||",
    "claim": "Multiple sources improve both accuracy and F1-score compared to a single source for News20, BBC, and BBC Sports datasets.",
    "label": "support"
  },
  {
    "id": "training_834_support",
    "table_caption": "Table: Automatic Evaluation on the QA dataset",
    "table_content": "|| Models | BLEU | Precision | Recall ||\n|| S2SA | 0.05 | 0.08±0.125 | 0.07±0.13 ||\n|| GenQA | 0.12 | 0.06±0.11 | 0.04±0.09 ||\n|| GenQAD | 0.13 | 0.25±0.2 | 0.34±0.235 ||\n|| GenDS-Single | 0.226 | 0.76±0.205 | [BOLD] 0.77±0.21 ||\n|| GenDS-Static | 0.19 | 0.64±0.23 | 0.66±0.235 ||\n|| GenDS | [BOLD] 0.227 | [BOLD] 0.77±0.205 | 0.76±0.215 ||",
    "claim": "GenDS achieves the highest BLEU score and precision, while GenDS-Single achieves the highest recall.",
    "label": "support"
  },
  {
    "id": "training_816_support",
    "table_caption": "Table: Effect of layers fine-tuning",
    "table_content": "|| ASR | NLU | Accuracy on Test, % | Accuracy on Test, % | Accuracy on Test, % | Validation ||\n|| layers | layers | SwBD | MRDA | FSC | loss ||\n|| 0 | 0 | 43.76 | 56.08 | 68.07 | 0.26 ||\n|| 0 | 1 | 37.61 | 56.47 | 85.53 | 0.19 ||\n|| 1 | 0 | 52.37 | [BOLD] 60.21 | 86.42 | 0.16 ||\n|| 1 | 1 | 52.05 | 58.32 | [BOLD] 86.82 | 0.17 ||\n|| 2 | 0 | 52.93 | 59.42 | 85.76 | [BOLD] 0.15 ||\n|| 3 | 0 | [BOLD] 53.81 | 58.90 | 85.53 | 0.16 ||",
    "claim": "Fine-tuning former ASR encoder layers is more beneficial than former NLU layers.",
    "label": "support"
  },
  {
    "id": "training_727_support",
    "table_caption": "Table: Q-WAAE : Impact on the METEOR metric of the reconstruction and adversarial loss coefficient on the ambiguous COCO data-set",
    "table_content": "|| [EMPTY] | [EMPTY] | [ITALIC] λr 0.2 | [ITALIC] λr 0.5 | [ITALIC] λr 0.8 ||\n|| [origin=c]90 [ITALIC] λa | 0.2 | [BOLD] 50.95 | 50.08 | 49.33 ||\n|| [origin=c]90 [ITALIC] λa | 0.5 | 49.79 | 49.62 | 49.16 ||\n|| [origin=c]90 [ITALIC] λa | 0.8 | 49.70 | 49.16 | 48.02 ||",
    "claim": "Translation quality decreases as the importance of the auxiliary loss increases relative to the translation loss.",
    "label": "support"
  },
  {
    "id": "training_745_support",
    "table_caption": "Table: This table shows the average performance of the classifiers with the various timeframes. The values are the arithmetic mean of the classifiers’ results. These are an SVM, a NB classifier and an LM classifier. A bold font highlights the best result considering this metric. The plus and minus symbols indicate whether the performance metric was calculated for disruptive (+) or constructive contributions (−).",
    "table_content": "|| [BOLD] time | [BOLD] recall+ | [BOLD] recall− | [BOLD] precision+ | [BOLD] precision− | [BOLD] F1+ | [BOLD] F1− | [BOLD] accuracy | [BOLD] AUC ||\n|| 13 hours | [BOLD] 49.81 | 75.35 | 66.68 | 60.26 | 56.8 | 66.87 | 62.58 | 0.551 ||\n|| 1 day | 49.29 | [BOLD] 76.63 | [BOLD] 67.83 | [BOLD] 60.27 | [BOLD] 56.93 | [BOLD] 67.42 | [BOLD] 62.96 | [BOLD] 0.562 ||\n|| 1.5 days | 47.45 | 76.03 | 66.46 | 59.22 | 55.18 | 66.52 | 61.74 | 0.542 ||\n|| 2 days | 48.14 | 75.0 | 65.85 | 59.24 | 55.38 | 66.11 | 61.57 | 0.542 ||\n|| 2.5 days | 46.57 | 75.77 | 65.88 | 58.72 | 54.35 | 66.09 | 61.17 | 0.536 ||\n|| 3 days | 46.49 | 74.04 | 64.33 | 58.12 | 53.71 | 65.03 | 60.26 | 0.529 ||\n|| 4 days | 45.35 | 74.03 | 63.72 | 57.59 | 52.76 | 64.7 | 59.69 | 0.521 ||\n|| 5 days | 45.13 | 74.51 | 64.07 | 57.64 | 52.72 | 64.92 | 59.82 | 0.522 ||\n|| 6 days | 42.95 | 74.6 | 63.02 | 56.7 | 50.85 | 64.35 | 58.77 | 0.512 ||",
    "claim": "Most performance metrics peak at the 1-day timeframe, including recall−, precision−, F1−, accuracy, and AUC.",
    "label": "support"
  },
  {
    "id": "training_808_support",
    "table_caption": "Table: Results of different methods on NYT and WebNLG datasets. Our re-implementation is marked by *.",
    "table_content": "|| Method | NYT  [ITALIC] Prec. | NYT  [ITALIC] Rec. | NYT  [ITALIC] F1 | WebNLG  [ITALIC] Prec. | WebNLG  [ITALIC] Rec. | WebNLG  [ITALIC] F1 ||\n|| NovelTagging  zheng2017Joint | 62.4 | 31.7 | 42.0 | 52.5 | 19.3 | 28.3 ||\n|| CopyR [ITALIC] OneDecoder  zeng2018Extracting | 59.4 | 53.1 | 56.0 | 32.2 | 28.9 | 30.5 ||\n|| CopyR [ITALIC] MultiDecoder  zeng2018Extracting | 61.0 | 56.6 | 58.7 | 37.7 | 36.4 | 37.1 ||\n|| GraphRel1 [ITALIC] p  fu2019GraphRel | 62.9 | 57.3 | 60.0 | 42.3 | 39.2 | 40.7 ||\n|| GraphRel2 [ITALIC] p  fu2019GraphRel | 63.9 | 60.0 | 61.9 | 44.7 | 41.1 | 42.9 ||\n|| CopyR [ITALIC] RL  zeng2019Learning | 77.9 | 67.2 | 72.1 | 63.3 | 59.9 | 61.6 ||\n|| CopyR∗ [ITALIC] RL | 72.8 | 69.4 | 71.1 | 60.9 | 61.1 | 61.0 ||\n|| CasRel [ITALIC] random | 81.5 | 75.7 | 78.5 | 84.7 | 79.5 | 82.0 ||\n|| CasRel [ITALIC] LSTM | 84.2 | 83.0 | 83.6 | 86.9 | 80.6 | 83.7 ||\n|| CasRel | [BOLD] 89.7 | [BOLD] 89.5 | [BOLD] 89.6 | [BOLD] 93.4 | [BOLD] 90.1 | [BOLD] 91.8 ||",
    "claim": "The CasRel model achieves the highest precision, recall, and F1-score on both the NYT and WebNLG datasets compared to all other methods.",
    "label": "support"
  },
  {
    "id": "training_812_support",
    "table_caption": "Table: Sentiment attitudes Granger-causality on the FT I dataset.",
    "table_content": "|| Stock | Model | Lag | Attitude | Price⇒ ||\n|| Stock | Model | Lag | ⇒Price | Attitude ||\n|| S&P 500 | Standard | 1 | 0.1929 | 0.1105 ||\n|| S&P 500 | Standard | 2 | 0.2611 | [BOLD] 0.0780 ||\n|| S&P 500 | Temporal | 1 | 0.2689 | [BOLD] 0.0495 ||\n|| S&P 500 | Temporal | 2 | 0.1692 | [BOLD] 0.0940 ||\n|| APPL | Standard | 1 | 0.7351 | 0.4253 ||\n|| APPL | Standard | 2 | 0.9117 | 0.6426 ||\n|| APPL | Temporal | 1 | 0.9478 | 0.6725 ||\n|| APPL | Temporal | 2 | 0.9715 | 0.8245 ||\n|| GOOGL | Standard | 1 | 0.5285 | 0.4035 ||\n|| GOOGL | Standard | 2 | 0.8075 | [BOLD] 0.0418 ||\n|| GOOGL | Temporal | 1 | 0.6920 | 0.5388 ||\n|| GOOGL | Temporal | 2 | 0.8516 | [BOLD] 0.0422 ||\n|| HPQ | Standard | 1 | 0.1534 | 0.3996 ||\n|| HPQ | Standard | 2 | 0.1877 | 0.5322 ||\n|| HPQ | Temporal | 1 | 0.4069 | [BOLD] 0.0836 ||\n|| HPQ | Temporal | 2 | 0.5097 | 0.1180 ||\n|| JPM | Standard | 1 | 0.8991 | [BOLD] 0.0461 ||\n|| JPM | Standard | 2 | 0.9963 | [BOLD] 0.0435 ||\n|| JPM | Temporal | 1 | 0.9437 | 0.1204 ||\n|| JPM | Temporal | 2 | 0.7722 | 0.2720 ||",
    "claim": "Stock price changes Granger-cause sentiment attitudes, with the strongest causality found using the temporal sentiment analysis model. For Google, causality is detected with a two-day lag, and for HP, with a one-day lag using temporal sentiment.",
    "label": "support"
  },
  {
    "id": "training_833_support",
    "table_caption": "Table: Automatic Evaluation on the Music dataset",
    "table_content": "|| Models | BLEU | Precision | Recall ||\n|| S2SA | 0.11 | 0.01±0.01 | 0.004±0.02 ||\n|| GenQA | 0.05 | 0.1134±0.14 | 0.05±0.1 ||\n|| GenQAD | 0.06 | 0.15±0.16 | 0.05±0.1 ||\n|| GenDS-Single | 0.108 | 0.28±0.19 | [BOLD] 0.19±0.18 ||\n|| GenDS-Static | 0.108 | 0.14±0.15 | 0.10±0.14 ||\n|| GenDS | [BOLD] 0.122 | [BOLD] 0.40 ±0.25 | 0.14±0.16 ||",
    "claim": "GenDS achieves the highest BLEU and precision scores among all models, while GenDS-Single achieves the highest recall.",
    "label": "support"
  },
  {
    "id": "training_799_support",
    "table_caption": "Table: Aspect and opinion term extraction performance.",
    "table_content": "|| Approach | SE14-R Aspect | SE14-R Opinion | SE14-L Aspect | SE14-L Opinion | SE15-R Aspect | SE15-R Opinion ||\n|| BiLSTM-CRF + word2vec | 84.06 | 84.59 | 73.47 | 75.41 | 66.17 | 68.16 ||\n|| BERT fine-tuning | 84.36 | 85.50 | 75.67 | 79.75 | 65.84 | 74.21 ||\n|| BERT feature-based | 85.14 | 85.74 | 76.81 | 81.41 | 66.84 | 73.92 ||\n|| RINANTE+BERT | [BOLD] 85.51 | [BOLD] 86.82 | [BOLD] 79.93 | [BOLD] 82.09 | [BOLD] 68.50 | [BOLD] 74.54 ||",
    "claim": "RINANTE+BERT achieves the highest performance across all tasks compared to other approaches.",
    "label": "support"
  },
  {
    "id": "training_797_support",
    "table_caption": "Table: Results of the synchronic evaluation in Pearson’s r averaged over all three VAD dimensions. The best system for each seed lexicon and those with statistically non-significant differences (p ≥ 0.05) are in bold.",
    "table_content": "|| Induction Method | Seed Selection | SVDPPMI | SGNS ||\n|| kNN | full | [BOLD] 0.548 | 0.487 ||\n|| ParaSimNum | full | [BOLD] 0.557 | 0.489 ||\n|| RandomWalkNum | full | [BOLD] 0.544 | 0.436 ||\n|| kNN | limited | 0.181 | 0.166 ||\n|| ParaSimNum | limited | 0.249 | 0.191 ||\n|| RandomWalkNum | limited | [BOLD] 0.330 | 0.181 ||",
    "claim": "SVDPPMI embeddings perform better than SGNS embeddings for both full and limited seed lexicons, and results with the limited seed set are worse than those with the full seed lexicon.",
    "label": "support"
  },
  {
    "id": "training_879_support",
    "table_caption": "Table: Performance on each class of UMD suicidality dataset",
    "table_content": "|| Label | Metrics | BiLSTM | SSA | RN ||\n|| -1 | Precision | 0.62 | 0.57 | 0.69 ||\n|| -1 | Recall | 0.77 | [BOLD] 0.92 | 0.70 ||\n|| -1 | F1-score | 0.69 | 0.70 | 0.69 ||\n|| [EMPTY] | Precision | 0.51 | 0.57 | 0.48 ||\n|| 1 | Recall | 0.55 | 0.31 | [BOLD] 0.62 ||\n|| [EMPTY] | F1-score | 0.53 | 0.41 | 0.54 ||\n|| [EMPTY] | Precision | 0.15 | 0.00 | 0.24 ||\n|| 0 | Recall | 0.02 | 0.00 | 0.09 ||\n|| [EMPTY] | F1-score | 0.04 | 0.00 | 0.13 ||",
    "claim": "The RN model has the highest recall for posts with high suicide risk (label 1), while all models perform poorly on posts with low suicide risk (label 0).",
    "label": "support"
  },
  {
    "id": "training_889_support",
    "table_caption": "Table: BLEU score vs context length on different models",
    "table_content": "|| Context length | 0 | 32 | 64 | 128 ||\n|| Transformer | 18.85 | 18.66 | 17.59 | 15.55 ||\n|| BPT (k=4) | 19.19 | 19.84 | 19.71 | 19.84 ||\n|| BPT (k=8) | 19.13 | 19.59 | 19.78 | 19.60 ||",
    "claim": "The performance of BPT models remains stable across different context lengths, while the performance of the Transformer model decreases as context length increases.",
    "label": "support"
  },
  {
    "id": "training_854_support",
    "table_caption": "Table: Results of image retrieval task on Flickr30k.",
    "table_content": "|| [EMPTY] | R@1 | R@5 | R@10 ||\n|| ViLBERT  | 58.2 | 84.9 | 91.5 ||\n|| ViLBERT + augmentation | [BOLD] 60.4 | [BOLD] 86.4 | [BOLD] 91.9 ||",
    "claim": "ViLBERT + augmentation achieves a higher R@1 score than ViLBERT.",
    "label": "support"
  },
  {
    "id": "training_767_support",
    "table_caption": "Table: Comparison on dialog response generation. BLEU (B) scores 1-4 are used for evaluation. Monotonic (M) and Cyclical (C) schedules are tested on two models.",
    "table_content": "|| [BOLD] Model  [BOLD] Schedule | CVAE  [BOLD] M | CVAE  [BOLD] C | CVAE+BoW  [BOLD] M | CVAE+BoW  [BOLD] C ||\n|| B1 prec | 0.326 | [BOLD] 0.423 | 0.384 | [BOLD] 0.397 ||\n|| B1 recall | 0.214 | [BOLD] 0.391 | 0.376 | [BOLD] 0.387 ||\n|| B2 prec | 0.278 | [BOLD] 0.354 | 0.320 | [BOLD] 0.331 ||\n|| B2 recall | 0.180 | [BOLD] 0.327 | 0.312 | [BOLD] 0.323 ||\n|| B3 prec | 0.237 | [BOLD] 0.299 | 0.269 | [BOLD] 0.279 ||\n|| B3 recall | 0.153 | [BOLD] 0.278 | 0.265 | [BOLD] 0.275 ||\n|| B4 prec | 0.185 | [BOLD] 0.234 | 0.211 | [BOLD] 0.219 ||\n|| B4 recall | 0.122 | [BOLD] 0.220 | 0.210 | [BOLD] 0.219 ||",
    "claim": "The cyclical schedule achieves higher BLEU precision and recall values than the monotonic schedule across all metrics (B1 to B4).",
    "label": "support"
  },
  {
    "id": "training_767_refute",
    "table_caption": "Table: Comparison on dialog response generation. BLEU (B) scores 1-4 are used for evaluation. Monotonic (M) and Cyclical (C) schedules are tested on two models.",
    "table_content": "|| [BOLD] Model  [BOLD] Schedule | CVAE  [BOLD] M | CVAE  [BOLD] C | CVAE+BoW  [BOLD] M | CVAE+BoW  [BOLD] C ||\n|| B1 prec | 0.326 | [BOLD] 0.423 | 0.384 | [BOLD] 0.397 ||\n|| B1 recall | 0.214 | [BOLD] 0.391 | 0.376 | [BOLD] 0.387 ||\n|| B2 prec | 0.278 | [BOLD] 0.354 | 0.320 | [BOLD] 0.331 ||\n|| B2 recall | 0.180 | [BOLD] 0.327 | 0.312 | [BOLD] 0.323 ||\n|| B3 prec | 0.237 | [BOLD] 0.299 | 0.269 | [BOLD] 0.279 ||\n|| B3 recall | 0.153 | [BOLD] 0.278 | 0.265 | [BOLD] 0.275 ||\n|| B4 prec | 0.185 | [BOLD] 0.234 | 0.211 | [BOLD] 0.219 ||\n|| B4 recall | 0.122 | [BOLD] 0.220 | 0.210 | [BOLD] 0.219 ||",
    "claim": "The monotonic schedule achieves higher BLEU precision and recall values than the cyclical schedule across all metrics (B1 to B4).",
    "label": "refute"
  },
  {
    "id": "training_684_refute",
    "table_caption": "Table: Performance for answer sentence selection on TREC-QA and WikiQA datasets.",
    "table_content": "|| Models | TREC-QA MAP | TREC-QA MRR | WikiQA MAP | WikiQA MRR ||\n||  | 0.695 | 0.763 | 0.652 | 0.665 ||\n||  | 0.728 | 0.832 | – | – ||\n|| Wang and Itty. wang2015faq | 0.746 | 0.820 | – | – ||\n||  | 0.753 | 0.851 | 0.689 | 0.696 ||\n||  | – | – | 0.692 | 0.711 ||\n||  | – | – | 0.689 | 0.707 ||\n||  | 0.771 | 0.845 | 0.706 | 0.723 ||\n||  | 0.777 | 0.836 | 0.709 | 0.723 ||\n||  | 0.801 | [BOLD] 0.877 | 0.701 | 0.718 ||\n||  | – | – | 0.734 | 0.742 ||\n||  | – | – | [BOLD] 0.743 | [BOLD] 0.755 ||\n|| BiMPM | [BOLD] 0.802 | 0.875 | 0.718 | 0.731 ||",
    "claim": "BiMPM achieves the highest TREC-QA MAP score of 0.802 and the highest TREC-QA MRR score of 0.875.",
    "label": "refute"
  },
  {
    "id": "training_777_refute",
    "table_caption": "Table: Audio datasets",
    "table_content": "|| [BOLD] Set | [BOLD] Language | [BOLD] Corpus | [BOLD] Hours | [BOLD] Speakers | [BOLD] Sentences | [BOLD] Foreign words | [BOLD] Style ||\n|| Training | English | Librispeech-360hr | 363.6 | 921 | 104,014 | [EMPTY] | Reading ||\n|| Training | English | Mozilla CommonVoice | 780 | 31,858 | 644,120 | - | Reading ||\n|| Training | English | TED-LIUM | 452 | 2,351 | 268,263 | - | Spontaneous ||\n|| Training | Vietnamese | VinBDI-set | 2,500 | 18,000 | 3,666,892 | - | Reading ||\n|| Testing | English | Test-clean | 5.4 | 87 | 2,620 | - | Reading ||\n|| Testing | English | Test-other | 5.3 | 90 | 2,939 | - | Reading ||\n|| Testing | Vietnamese | Reading-test | 9.9 | 23 | 5,358 | - | Reading ||\n|| Testing | Vietnamese | Conversation-test | 10.8 | 1,892 | 11,533 | - | Spontaneous ||\n|| Testing | Vietnamese | YouTube-test | 9.9 | unknown | 5,432 | 24,8% | Spontaneous ||\n|| Testing | Vietnamese | VLSP2018 | 2.1 | unknown | 796 | - | Reading ||",
    "claim": "The YouTube-test set contains 28% foreign words.",
    "label": "refute"
  },
  {
    "id": "training_716_refute",
    "table_caption": "Table: Performance of Each of the Sound-class Detectors Measured in Terms of Equal-error-rates (EERs) for the Development Data. Frequency of Observation in Development Utterances is also Shown for Each Class Type.",
    "table_content": "|| Class | S1 | S2 | S3 | S4 | S5 | All | Freq. ||\n|| Vowel | 3.52 | 13.30 | [BOLD] 0.65 | [BOLD] 0.74 | 7.26 | 6.35 | 0.542 ||\n|| Nasal | 8.90 | 20.82 | 5.09 | 5.86 | 13.79 | 11.62 | 0.156 ||\n|| Glide | 9.33 | 21.69 | 4.10 | 4.44 | 15.92 | 12.15 | 0.118 ||\n|| Stop | [BOLD] 2.24 | [BOLD] 4.78 | 0.70 | 0.78 | [BOLD] 6.77 | [BOLD] 3.68 | 0.112 ||\n|| Rest | 8.97 | 10.58 | 3.32 | 3.78 | 16.08 | 9.43 | 0.072 ||",
    "claim": "Stop sounds have the highest performance in scenarios S1, S3, S5, and overall (All).",
    "label": "refute"
  },
  {
    "id": "training_720_refute",
    "table_caption": "Table: Comparison of different architectures for the answer verifier.",
    "table_content": "|| Configuration | NoAns ACC ||\n|| Model-I | 74.5 ||\n|| Model-II | 74.6 ||\n|| Model-II + ELMo | 75.3 ||\n|| Model-III | [BOLD] 76.2 ||\n|| Model-III + ELMo | 76.1 ||",
    "claim": "Model-III achieves the highest no-answer accuracy of 76.2, and adding ELMo embeddings improves performance.",
    "label": "refute"
  },
  {
    "id": "training_712_refute",
    "table_caption": "Table: ROUGE-L of our model and versions thereof with less synthetic data (second block), using only one noising method (third block), and without some modules (fourth block). A more comprehensive table and discussion can be found in the Appendix.",
    "table_content": "|| Model | RT | Yelp ||\n|| DenoiseSum | 16.27 | 17.65 ||\n|| 10% synthetic dataset | 15.39 | 16.22 ||\n|| 50% synthetic dataset | 15.76 | 17.54 ||\n|| no segment noising | 16.03 | 16.88 ||\n|| no document noising | 16.22 | 16.67 ||\n|| no explicit denoising | 16.06 | 17.06 ||\n|| no partial copy | 15.89 | 16.31 ||\n|| no discriminator | 15.84 | 16.64 ||\n|| using human categories | 15.87 | 15.86 ||",
    "claim": "Increasing the size of the synthetic dataset does not improve performance, and both segment and document noising contribute to better results. Human-labeled categories decrease model performance compared to other methods.",
    "label": "refute"
  },
  {
    "id": "training_732_refute",
    "table_caption": "Table: Effect of different dispersion functions, content coverage, and dissimilarity metrics on our system. [Left] JSD values for different combinations on Yahoo! data, using LDA with 100 topics. All systems are significantly different from each other at significance level α=0.05. Systems using summation of distances for dispersion function (hsum) uniformly outperform the ones using minimum distance (hmin). [Right] ROUGE scores of different choices for TAC 2008 data. All systems use LDA with 40 topics. The parameters of our systems are adopted from the ones tuned on Yahoo! Answers.",
    "table_content": "|| [BOLD] Yahoo! Answer | [BOLD] Yahoo! Answer Dispersion [ITALIC] sum | [BOLD] Yahoo! Answer Dispersion [ITALIC] sum | [BOLD] Yahoo! Answer Dispersion [ITALIC] min | [BOLD] Yahoo! Answer Dispersion [ITALIC] min ||\n|| Dissimi | Cont [ITALIC] tfidf | Cont [ITALIC] sem | Cont [ITALIC] tfidf | Cont [ITALIC] sem ||\n|| [ITALIC] Semantic | 0.3143 | 0.324 3 | 0.3129 | 0.3232 ||\n|| [ITALIC] Topical | 0.3101 | 0.3202 | 0.3106 | 0.3209 ||\n|| [ITALIC] Lexical | [BOLD] 0.3017 | 0.3147 | 0.3071 | 0.3172 ||",
    "claim": "Semantic measurement is better at measuring content coverage than Cosine using TFIDF in Yahoo! Answer data.",
    "label": "refute"
  },
  {
    "id": "training_743_refute",
    "table_caption": "Table: Full-match scores of predicted keyphrases by various methods",
    "table_content": "|| [BOLD] Method | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [BOLD] F1@5 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [BOLD] F1@7 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [BOLD] F1@5 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [BOLD] F1@7 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [BOLD] F1@5 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [BOLD] F1@7 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [BOLD] F1@5 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [BOLD] F1@7 ||\n|| Yake! | 19.35 | 21.47 | 17.98 | 17.4 | 17.11 | 15.19 | 15.24 | 14.57 ||\n|| TopicRank | 16.5 | 20.44 | 6.93 | 6.92 | 11.93 | 11.72 | 11.9 | 12.08 ||\n|| Maui | 20.11 | 20.56 | 23.17 | 23.04 | 22.3 | 19.63 | 19.58 | 18.42 ||\n|| CopyRnn | [BOLD] 29.2 | [BOLD] 33.6 | [BOLD] 30.2 | [BOLD] 25.2 | [BOLD] 32.8 | [BOLD] 25.5 | [BOLD] 33.06 | [BOLD] 31.92 ||\n|| Merge | 6.85 | 6.86 | 4.92 | 4.93 | 8.75 | 8.76 | 11.12 | 13.39 ||\n|| Inject | 6.09 | 6.08 | 4.1 | 4.11 | 8.09 | 8.09 | 9.61 | 11.22 ||\n|| Abs | 14.75 | 14.82 | 10.24 | 10.29 | 12.17 | 12.09 | 14.54 | 14.57 ||\n|| PointCov | 22.19 | 21.55 | 19.87 | 20.03 | 20.45 | 20.89 | 22.72 | 21.49 ||",
    "claim": "TopicRank consistently outperforms Yake!, and CopyRnn is superior to Maui across the datasets.",
    "label": "refute"
  },
  {
    "id": "training_747_refute",
    "table_caption": "Table: This table shows the performance of the SVM, NB and LM classifiers using the independent posts approach. Function words classifications are marked with “FW”. All others were full text classifications. The plus and minus symbols indicate whether the performance metric was calculated for disruptive (+) or constructive contributions (−).",
    "table_content": "|| [BOLD] Classifier | [BOLD] Recall+ | [BOLD] Recall− | [BOLD] Precision+ | [BOLD] Precision− | [BOLD] F1+ | [BOLD] F1− | [BOLD] Accuracy | [BOLD] AUC ||\n|| SVM | 59.91 | 75.56 | 71.03 | 65.33 | 65.00 | 70.07 | 67.73 | 0.750 ||\n|| SVM (FW) | 31.11 | 85.58 | 68.33 | 55.40 | 42.75 | 67.26 | 58.34 | 0.620 ||\n|| NB | 42.42 | 80.78 | 68.82 | 58.38 | 52.49 | 67.78 | 61.60 | 0.390 ||\n|| NB (FW) | 26.04 | 85.43 | 64.12 | 53.60 | 37.04 | 65.87 | 55.73 | 0.600 ||\n|| LM | 44.56 | 72.35 | 61.71 | 56.61 | 51.75 | 63.52 | 58.45 | 0.518 ||\n|| LM (FW) | 44.28 | 67.34 | 57.55 | 54.72 | 50.05 | 60.38 | 55.81 | 0.542 ||",
    "claim": "The support vector machine (SVM) performs best overall, with the highest negative recall and AUC, while the naïve Bayes (NB) classifier is only better in negative recall.",
    "label": "refute"
  },
  {
    "id": "training_774_refute",
    "table_caption": "Table: Tagging accuracies of adaptation of our baseline SVM tagger from the PTB to the PPCEME in ablation experiments.",
    "table_content": "|| Feature set | IV | OOV | All ||\n|| All features | 81.68 | 48.96 | 74.15 ||\n|| – word context | 79.69 | 38.62 | 70.23 ||\n|| – prefix | 81.61 | 46.11 | 73.43 ||\n|| – suffix | 81.36 | 38.13 | 71.40 ||\n|| – affix | 81.22 | 34.40 | 70.44 ||\n|| – orthographic | 81.68 | 48.92 | 74.14 ||",
    "claim": "Removing word context features significantly reduces accuracy on OOV tokens, while removing orthographic features significantly impacts overall accuracy.",
    "label": "refute"
  },
  {
    "id": "training_735_refute",
    "table_caption": "Table: Test set performance. * denotes the models trained on separate datasets while ** indicates those trained on multiple datasets. †specifies the models in Rudinger et al. (2018) that are significantly improved with BERT.",
    "table_content": "|| [EMPTY] | FactBank MAE | FactBank  [ITALIC] r | UW MAE | UW  [ITALIC] r | Meantime MAE | Meantime  [ITALIC] r | UDS-IH2 MAE | UDS-IH2  [ITALIC] r ||\n|| Lee et al. ( 2015 )* | - | - | 0.511 | 0.708 | - | - | - | - ||\n|| Stanovsky et al. ( 2017 )* | 0.590 | 0.710 | [BOLD] 0.420 | 0.660 | 0.340 | 0.470 | - | - ||\n|| L-biLSTM(2)-S*† | 0.427 | 0.826 | 0.508 | 0.719 | 0.427 | 0.335 | 0.960 | 0.768 ||\n|| L-biLSTM(2)-MultiBal**† | 0.391 | 0.821 | 0.496 | 0.724 | 0.278 | 0.613 | - | - ||\n|| L-biLSTM(1)-MultiFoc**† | 0.314 | 0.846 | 0.502 | 0.710 | 0.305 | 0.377 | - | - ||\n|| L-biLSTM(2)-MultiSimp w/UDS-IH2**† | 0.377 | 0.828 | 0.508 | 0.722 | 0.367 | 0.469 | 0.965 | 0.771 ||\n|| H-biLSTM(1)-MultiSimp**† | 0.313 | 0.857 | 0.528 | 0.704 | 0.314 | 0.545 | - | - ||\n|| H-biLSTM(2)-MultiSimp w/UDS-IH2**† | 0.393 | 0.820 | 0.481 | 0.749 | 0.374 | 0.495 | 0.969 | 0.760 ||\n|| L-biLSTM(2)-S+BERT* | 0.381 | 0.85 | 0.475 | 0.752 | 0.389 | 0.394 | 0.895 | 0.804 ||\n|| L-biLSTM(2)-MultiSimp w/UDS-IH2+BERT** | 0.343 | 0.855 | 0.476 | 0.749 | 0.358 | 0.499 | 0.841 | 0.841 ||\n|| H-biLSTM(1)-MultiSimp+BERT** | 0.310 | 0.821 | 0.495 | 0.771 | 0.281 | 0.639 | 0.822 | 0.812 ||\n|| H-biLSTM(2)-MultiSimp w/UDS-IH2+BERT** | 0.330 | 0.871 | 0.460 | 0.798 | 0.339 | 0.571 | 0.835 | 0.802 ||\n|| Graph-based (Ours)* | 0.315 | 0.890 | 0.451 | 0.828 | 0.350 | 0.452 | 0.730 | 0.905 ||\n|| Ours with multiple datasets** | [BOLD] 0.310 | [BOLD] 0.903 | 0.438 | [BOLD] 0.830 | [BOLD] 0.204 | [BOLD] 0.702 | [BOLD] 0.726 | [BOLD] 0.909 ||",
    "claim": "The proposed model \"Ours with multiple datasets\" achieves the highest MAE and lowest Pearson Correlation (r) across most datasets compared to other models.",
    "label": "refute"
  },
  {
    "id": "training_752_refute",
    "table_caption": "Table: Results on WSJ test set.",
    "table_content": "|| [BOLD] System | [BOLD] Accuracy ||\n|| Sogaard:2011:SCN | 97.50 ||\n|| DBLP:journals/corr/HuangXY15 | 97.55 ||\n|| choi:16a | 97.64 ||\n|| andor2016globally. | 97.44 ||\n|| DBLP:conf/conll/DozatQM17 | 97.41 ||\n|| ours | [BOLD] 97.96 ||",
    "claim": "The \"choi:16a\" system achieves the highest accuracy of 97.64% compared to other systems.",
    "label": "refute"
  },
  {
    "id": "training_795_refute",
    "table_caption": "Table: Ablation studies on components of our model. (Scores are sorted by the delta from the full model.)",
    "table_content": "|| Ablation | AMR 1.0 | AMR 2.0 ||\n|| Full model | 70.2 | 76.3 ||\n|| no source-side copy | 62.7 | 70.9 ||\n|| no target-side copy | 66.2 | 71.6 ||\n|| no coverage loss | 68.5 | 74.5 ||\n|| no BERT embeddings | 68.8 | 74.6 ||\n|| no index embeddings | 68.5 | 75.5 ||\n|| no anonym. indicator embed. | 68.9 | 75.6 ||\n|| no beam search | 69.2 | 75.3 ||\n|| no POS tag embeddings | 69.2 | 75.7 ||\n|| no CharCNN features | 70.0 | 75.8 ||\n|| only edge prediction | 88.4 | 90.9 ||",
    "claim": "Removing target-side copy does not significantly affect performance on either AMR 1.0 or AMR 2.0.",
    "label": "refute"
  },
  {
    "id": "training_804_refute",
    "table_caption": "Table: DistilBERT yields to comparable performance on downstream tasks. Comparison on downstream tasks: IMDb (test accuracy) and SQuAD 1.1 (EM/F1 on dev set). D: with a second step of distillation during fine-tuning.",
    "table_content": "|| Model | # param. | Inf. time ||\n|| [EMPTY] | (Millions) | (seconds) ||\n|| ELMo | 180 | 895 ||\n|| BERT-base | 110 | 668 ||\n|| DistilBERT | 66 | 410 ||",
    "claim": "DistilBERT has 40% fewer parameters than BERT and is 60% slower than BERT.",
    "label": "refute"
  },
  {
    "id": "training_826_refute",
    "table_caption": "Table: Test Results for event sequencing. The Oracle Cluster+Temporal system is using Caevo’s result on the Oracle Clusters.",
    "table_content": "|| [EMPTY] | Prec. | Recall | F-Score ||\n|| Oracle Cluster+Temporal | [BOLD] 46.21 | 8.72 | 14.68 ||\n|| Our Model | 18.28 | [BOLD] 16.91 | [BOLD] 17.57 ||",
    "claim": "Our Model achieves higher precision, while it also achieves higher recall and F-score.",
    "label": "refute"
  },
  {
    "id": "training_695_refute",
    "table_caption": "Table: Speed comparison of our systems with other open-sourced systems.",
    "table_content": "|| [BOLD] System | [BOLD] Speed (Sents/s) | [BOLD] Speedup ||\n|| [BOLD] Only Segmenter | [EMPTY] | [EMPTY] ||\n|| CODRA Joty et al. ( 2015 ) | 3.06 | 1.0x ||\n|| WLY Wang et al. ( 2018 ) | 4.30 | 1.4x ||\n|| SPADE Soricut and Marcu ( 2003 ) | 5.24 | 1.7x ||\n|| Our (CPU) | 12.05 | 3.9x ||\n|| Our (GPU) | 35.54 | 11.6x ||\n|| [BOLD] Only Parser | [EMPTY] | [EMPTY] ||\n|| SPADE Soricut and Marcu ( 2003 ) | 5.07 | 1.0x ||\n|| CODRA Joty et al. ( 2015 ) | 7.77 | 1.5x ||\n|| Our (CPU) | 12.57 | 2.5x ||\n|| Our (GPU) | 30.45 | 6.0x ||\n|| [BOLD] End-to-End (Segmenter → Parser) | [EMPTY] | [EMPTY] ||\n|| CODRA Joty et al. ( 2015 ) | 3.05 | 1.0x ||\n|| SPADE Soricut and Marcu ( 2003 ) | 4.90 | 1.6x ||\n|| Our (CPU) | 11.99 | 3.9x ||\n|| Our (GPU) | 28.96 | 9.5x ||",
    "claim": "Our segmenter is 6.8x faster than SPADE. Our parser is 2.9x faster than CODRA. Our end-to-end system is 5.9x faster than SPADE. Even when tested only on CPU, our model is not faster than all the other models.",
    "label": "refute"
  },
  {
    "id": "training_746_refute",
    "table_caption": "Table: The table shows how commonly a term appears in disruptive or constructive posts. A bold font indicates that the term appears more frequently in that class.",
    "table_content": "|| [BOLD] term | [BOLD] share of words from disruptive posts (‰) | [BOLD] share of words from constructive posts (‰) ||\n|| fucking | [BOLD] 0.06 | 0.00 ||\n|| fuck | [BOLD] 0.06 | 0.01 ||\n|| shit | [BOLD] 0.09 | 0.01 ||\n|| i | 6.40 | [BOLD] 10.70 ||\n|| you | [BOLD] 10.64 | 4.52 ||\n|| me | [BOLD] 2.43 | 1.20 ||\n|| my | [BOLD] 3.00 | 1.68 ||\n|| your | [BOLD] 3.05 | 1.25 ||\n|| myself | [BOLD] 0.22 | 0.13 ||\n|| yourself | [BOLD] 0.20 | 0.10 ||",
    "claim": "The terms \"fucking\", \"fuck\", and \"shit\" have a higher share of words in disruptive posts compared to constructive posts, while the term \"I\" has a higher share in disruptive posts compared to constructive posts.",
    "label": "refute"
  },
  {
    "id": "training_768_refute",
    "table_caption": "Table: Comparison on dialog response generation. Reconstruction perplexity (Rec-P) and BLEU (B) scores are used for evaluation.",
    "table_content": "|| [BOLD] Model  [BOLD] Schedule | CVAE  [BOLD] M | CVAE  [BOLD] C | CVAE+BoW  [BOLD] M | CVAE+BoW  [BOLD] C ||\n|| Rec-P ↓ | 36.16 | [BOLD] 29.77 | 18.44 | [BOLD] 16.74 ||\n|| KL Loss ↑ | 0.265 | [BOLD] 4.104 | 14.06 | [BOLD] 15.55 ||\n|| B4 prec | 0.185 | [BOLD] 0.234 | 0.211 | [BOLD] 0.219 ||\n|| B4 recall | 0.122 | [BOLD] 0.220 | 0.210 | [BOLD] 0.219 ||\n|| A-bow prec | 0.957 | [BOLD] 0.961 | 0.958 | [BOLD] 0.961 ||\n|| A-bow recall | 0.911 | [BOLD] 0.941 | 0.938 | [BOLD] 0.940 ||\n|| E-bow prec | 0.867 | 0.833 | 0.830 | 0.828 ||\n|| E-bow recall | 0.784 | [BOLD] 0.808 | 0.808 | 0.805 ||",
    "claim": "The cyclical schedule results in higher reconstruction perplexity, higher KL loss, and higher BLEU scores for both CVAE and CVAE+BoW models.",
    "label": "refute"
  },
  {
    "id": "training_786_refute",
    "table_caption": "Table: Accuracy of LAMBADA with or without label vs. unlabeled data for ATIS dataset with 5 samples per class. Significant improvement for BERT and SVM classifiers (*McNemar, p−value<0.01).",
    "table_content": "|| Classifier | Base. | Unlab. Data | Unlab. GPT | LAMBADA ||\n|| BERT | 53.3 | 54.5 | 73.2 | [BOLD] 75.7* ||\n|| SVM | 35.6 | 23.5 | 47.2 | [BOLD] 56.5* ||\n|| LSTM | 29.0 | [BOLD] 40.1* | 23.2 | 33.7 ||",
    "claim": "For most classifiers, using unlabeled data achieves better accuracy compared to LAMBADA.",
    "label": "refute"
  },
  {
    "id": "training_713_refute",
    "table_caption": "Table: ROUGE-1/2/L F1 scores of our model and versions thereof with less synthetic data (second block), using only one noising method (third block), and without some modules (fourth block).",
    "table_content": "|| [EMPTY] | Rotten Tomatoes | Rotten Tomatoes | Rotten Tomatoes | Yelp | Yelp | Yelp ||\n|| Model | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE-1 | ROUGE-2 | ROUGE-L ||\n|| DenoiseSum | [BOLD] 21.26 | 4.61 | [BOLD] 16.27 | [BOLD] 30.14 | [BOLD] 4.99 | [BOLD] 17.65 ||\n|| 10% synthetic dataset | 20.16 | 3.14 | 15.39 | 28.54 | 3.63 | 16.22 ||\n|| 50% synthetic dataset | 20.76 | 3.91 | 15.76 | 29.16 | 4.40 | 17.54 ||\n|| no segment noising | 20.64 | 4.39 | 16.03 | 28.93 | 4.31 | 16.88 ||\n|| no document noising | 21.23 | 4.38 | 16.22 | 28.75 | 4.06 | 16.67 ||\n|| no explicit denoising | 21.17 | 4.18 | 16.06 | 28.60 | 4.10 | 17.06 ||\n|| no partial copy | 20.76 | 4.01 | 15.89 | 28.03 | 4.58 | 16.31 ||\n|| no discriminator | 20.77 | 4.48 | 15.84 | 29.09 | 4.22 | 16.64 ||\n|| using human categories | 20.67 | [BOLD] 4.69 | 15.87 | 28.54 | 4.02 | 15.86 ||",
    "claim": "The final model, DenoiseSum, achieves the highest ROUGE-2 scores on both Rotten Tomatoes and Yelp datasets compared to other model variations.",
    "label": "refute"
  },
  {
    "id": "training_827_refute",
    "table_caption": "Table: Ablation Study for Event Sequencing.",
    "table_content": "|| [EMPTY] | Prec. | Recall | F-Score | Δ ||\n|| Full | 37.92 | 36.79 | 36.36 | [EMPTY] ||\n|| - Mention Type | 32.78 | 29.81 | 30.07 | 6.29 ||\n|| - Sentence | 33.90 | 30.75 | 31.00 | 5.36 ||\n|| - Temporal | 37.21 | 36.53 | 35.81 | 0.55 ||\n|| - Dependency | 38.18 | 36.44 | 36.23 | 0.13 ||\n|| - Function words | 38.08 | 36.51 | 36.18 | 0.18 ||",
    "claim": "Removing the \"Mention Type\" and \"Sentence\" features results in no significant change in both precision and recall.",
    "label": "refute"
  },
  {
    "id": "training_729_refute",
    "table_caption": "Table: Effect of different dispersion functions, content coverage, and dissimilarity metrics on our system. [Left] JSD values for different combinations on Yahoo! data, using LDA with 100 topics. All systems are significantly different from each other at significance level α=0.05. Systems using summation of distances for dispersion function (hsum) uniformly outperform the ones using minimum distance (hmin). [Right] ROUGE scores of different choices for TAC 2008 data. All systems use LDA with 40 topics. The parameters of our systems are adopted from the ones tuned on Yahoo! Answers.",
    "table_content": "|| [BOLD] TAC 2008 | [BOLD] TAC 2008 Dispersion [ITALIC] sum | [BOLD] TAC 2008 Dispersion [ITALIC] sum | [BOLD] TAC 2008 Dispersion [ITALIC] min | [BOLD] TAC 2008 Dispersion [ITALIC] min ||\n|| Dissimi | Cont [ITALIC] tfidf | Cont [ITALIC] sem | Cont [ITALIC] tfidf | Cont [ITALIC] sem ||\n|| [ITALIC] Semantic | 0.2216 | 0.2169 | 0.2772 | 0.2579 ||\n|| [ITALIC] Topical | 0.2128 | 0.2090 | [BOLD] 0.3234 | 0.3056 ||\n|| [ITALIC] Lexical | 0.2167 | 0.2129 | 0.3117 | 0.3160 ||",
    "claim": "Cosine using TFIDF is better at measuring content coverage than Semantic measurement, and Topical-based dissimilarity does not outperform Semantic and Lexical metrics in dispersion [ITALIC] min.",
    "label": "refute"
  },
  {
    "id": "training_719_refute",
    "table_caption": "Table: Comparison of readers with different auxiliary losses.",
    "table_content": "|| Configuration | HasAns EM | HasAns F1 | All EM | All F1 | NoAns ACC ||\n|| RMR | 72.6 | 81.6 | 66.9 | 69.1 | 73.1 ||\n|| - indep-I | 71.3 | 80.4 | 66.0 | 68.6 | 72.8 ||\n|| - indep-II | 72.4 | 81.4 | 64.0 | 66.1 | 69.8 ||\n|| - both | 71.9 | 80.9 | 65.2 | 67.5 | 71.4 ||\n|| RMR + ELMo | 79.4 | 86.8 | 71.4 | 73.7 | 77.0 ||\n|| - indep-I | 78.9 | 86.5 | 71.2 | 73.5 | 76.7 ||\n|| - indep-II | 79.5 | 86.6 | 69.4 | 71.4 | 75.1 ||\n|| - both | 78.7 | 86.2 | 70.0 | 71.9 | 75.3 ||",
    "claim": "Removing the independent span loss (indep-I) results in a performance drop for HasAns EM and HasAns F1, while removing the independent no-answer loss (indep-II) leads to an improvement in NoAns ACC.",
    "label": "refute"
  },
  {
    "id": "training_558_refute",
    "table_caption": "Table: Results in F1 scores for FG-NER (We run each setting five times and report the average F1 scores.)",
    "table_content": "|| Model | FG-NER | +Chunk | +NER (CoNLL) | +POS | +NER (Ontonotes) ||\n|| Base Model (GloVe) | 81.51 | - | - | - | - ||\n|| RNN-Shared Model (GloVe) | - | 80.53 | 81.38 | 80.55 | 81.13 ||\n|| Embedding-Shared Model (GloVe) | - | 81.49 | 81.21 | 81.59 | 81.24 ||\n|| Hierarchical-Shared Model (GloVe) | - | 81.65 | [BOLD] 82.14 | 81.27 | 81.67 ||\n|| Base Model (ELMo) | 82.74 | - | - | - | - ||\n|| RNN-Shared Model (ELMo) | - | 82.60 | 82.09 | 81.77 | 82.12 ||\n|| Embedding-Shared Model (ELMo) | - | 82.75 | 82.45 | 82.34 | 81.94 ||\n|| Hierarchical-Shared Model (ELMo) | - | [BOLD] 83.04 | 82.72 | 82.76 | 82.96 ||\n|| Base Model (GloVe) + LM  | 81.77 | - | - | - | - ||\n|| RNN-Shared Model (GloVe) + Shared-LM | - | 80.83 | 81.34 | 80.69 | 81.45 ||\n|| Embedding-Shared Model (GloVe) + Shared-LM | - | 81.54 | 81.95 | 81.86 | 81.34 ||\n|| Hierarchical-Shared Model (GloVe) + Shared-LM | - | 81.69 | [BOLD] 81.96 | 81.42 | 81.78 ||\n|| Base Model (ELMo) + LM | 82.91 | - | - | - | - ||\n|| RNN-Shared Model (ELMo) + Shared-LM | - | 82.68 | 82.64 | 81.61 | 82.36 ||\n|| Embedding-Shared Model (ELMo) + Shared-LM | - | 82.61 | 82.32 | 82.46 | 82.45 ||\n|| Hierarchical-Shared Model (ELMo) + Shared-LM | - | 82.87 | 82.82 | 82.85 | [BOLD] 82.99 ||\n|| Hierarchical-Shared Model (GloVe) + Unshared-LM | - | 81.77 | 81.80 | 81.72 | 81.88 ||\n|| Hierarchical-Shared Model (ELMo) + Unshared-LM | - | [BOLD] 83.35 | 83.14 | 83.06 | 82.82 ||\n||  | 83.14 | - | - | - | - ||",
    "claim": "The hierarchical-shared model achieves an F1 score of 82.72% with NER (CoNLL) and 82.96% with NER (Ontonotes) using ELMo, compared to the base model's F1 scores of 81.51% with GloVe and 82.74% with ELMo.",
    "label": "refute"
  },
  {
    "id": "training_727_refute",
    "table_caption": "Table: Q-WAAE : Impact on the METEOR metric of the reconstruction and adversarial loss coefficient on the ambiguous COCO data-set",
    "table_content": "|| [EMPTY] | [EMPTY] | [ITALIC] λr 0.2 | [ITALIC] λr 0.5 | [ITALIC] λr 0.8 ||\n|| [origin=c]90 [ITALIC] λa | 0.2 | [BOLD] 50.95 | 50.08 | 49.33 ||\n|| [origin=c]90 [ITALIC] λa | 0.5 | 49.79 | 49.62 | 49.16 ||\n|| [origin=c]90 [ITALIC] λa | 0.8 | 49.70 | 49.16 | 48.02 ||",
    "claim": "Translation quality increases as the importance of the auxiliary loss increases relative to the translation loss.",
    "label": "refute"
  },
  {
    "id": "training_756_refute",
    "table_caption": "Table: The results on the standard BLI task and BLI controlled for lexeme for the original Ruder et al. (2018)’s model (✗) and the same model trained with a morphological constraint (✓) (discussed in §4.6).",
    "table_content": "|| [EMPTY] | Normal In vocab | Normal In vocab | Normal +OOVs | Normal +OOVs | Lexeme In vocab | Lexeme In vocab | Lexeme +OOVs | Lexeme +OOVs | Dictionary Sizes In vocab | Dictionary Sizes +OOVs ||\n|| Constraint | ✗ | ✓ | ✗ | ✓ | ✗ | ✓ | ✗ | ✓ | In vocab | +OOVs ||\n|| Ukrainian–Russian | [BOLD] 68.4 | 61.1 | [BOLD] 63.7 | 56.1 | [BOLD] 89.9 | 89.1 | [BOLD] 88.6 | 87.6 | 786 | 933 ||\n|| Russian–Slovak | [BOLD] 25.7 | 21.1 | [BOLD] 20.9 | 17.0 | [BOLD] 79.3 | 76.8 | [BOLD] 76.0 | 74.2 | 1610 | 2150 ||\n|| Polish–Czech | 42.0 | [BOLD] 44.4 | 34.8 | [BOLD] 36.7 | 80.6 | [BOLD] 81.1 | 75.3 | [BOLD] 75.9 | 4043 | 5332 ||\n|| Russian–Polish | 39.8 | [BOLD] 41.2 | 34.8 | [BOLD] 36.1 | 80.8 | [BOLD] 82.6 | 77.7 | [BOLD] 80.2 | 9183 | 11697 ||\n|| Catalan–Portuguese | 62.8 | [BOLD] 64.2 | 41.1 | [BOLD] 42.4 | 83.1 | [BOLD] 84.3 | 57.7 | [BOLD] 59.0 | 5418 | 10759 ||\n|| French–Spanish | 47.8 | [BOLD] 50.2 | 26.7 | [BOLD] 28.9 | 78.0 | [BOLD] 81.4 | 47.9 | [BOLD] 52.2 | 9770 | 21087 ||\n|| Portuguese–Spanish | 60.2 | [BOLD] 61.1 | 36.8 | [BOLD] 37.6 | 84.7 | [BOLD] 85.4 | 57.1 | [BOLD] 58.2 | 9275 | 22638 ||\n|| Italian–Spanish | 42.7 | [BOLD] 43.8 | 21.1 | [BOLD] 22.1 | 76.4 | [BOLD] 77.6 | 47.6 | [BOLD] 49.6 | 11685 | 30686 ||\n|| Polish–Spanish | [BOLD] 36.1 | 32.1 | [BOLD] 28.0 | 25.0 | [BOLD] 78.1 | 77.7 | [BOLD] 68.6 | 68.4 | 8964 | 12759 ||\n|| Spanish–Polish | 28.1 | [BOLD] 30.9 | 21.0 | [BOLD] 23.2 | 81.2 | [BOLD] 82.0 | 64.2 | [BOLD] 65.8 | 4270 | 6095 ||",
    "claim": "The accuracy for Spanish–Polish in-vocabulary pairs is 30.9%, which is particularly high compared to other language pairs.",
    "label": "refute"
  },
  {
    "id": "training_760_refute",
    "table_caption": "Table: Analysis of the Dual Emotion Modeling.",
    "table_content": "|| Module | Methods | Weibo Dataset Accuracy | Weibo Dataset F1-Score | Twitter Dataset Accuracy | Twitter Dataset F1-Score ||\n|| Content ( [ITALIC] Publisher Emotion) | WE | 0.790 | 0.801 | 0.678 | 0.627 ||\n|| Content ( [ITALIC] Publisher Emotion) | EE | 0.700 | 0.719 | 0.639 | 0.615 ||\n|| Content ( [ITALIC] Publisher Emotion) | WEE(c) | 0.813 | 0.810 | 0.690 | 0.709 ||\n|| Content ( [ITALIC] Publisher Emotion) | WEE(att) | 0.799 | 0.793 | 0.701 | 0.675 ||\n|| [EMPTY] | WEE(gn) | [BOLD] 0.851 | [BOLD] 0.854 | [BOLD] 0.725 | [BOLD] 0.735 ||\n|| Comment ( [ITALIC] Social Emotion) | WE | 0.667 | 0.550 | 0.667 | 0.634 ||\n|| Comment ( [ITALIC] Social Emotion) | EE | 0.619 | 0.553 | 0.655 | 0.667 ||\n|| Comment ( [ITALIC] Social Emotion) | WEE(c) | 0.669 | 0.560 | 0.689 | 0.693 ||\n|| [EMPTY] | WEE(gc) | [BOLD] 0.671 | [BOLD] 0.563 | [BOLD] 0.713 | [BOLD] 0.705 ||\n|| Content + Comment ( [ITALIC] Dual Emotion) | (WE+WE)(c) | 0.835 | 0.840 | 0.695 | 0.697 ||\n|| Content + Comment ( [ITALIC] Dual Emotion) | (WEE(gn)+WE)(c) | 0.863 | 0.860 | 0.736 | 0.754 ||\n|| Content + Comment ( [ITALIC] Dual Emotion) | (WEE(gn)+WEE(gc))(c) | 0.866 | 0.870 | 0.746 | [BOLD] 0.776 ||\n|| [EMPTY] | (WEE(gn)+WEE(gc))(gm) | [BOLD] 0.872 | [BOLD] 0.874 | [BOLD] 0.751 | 0.771 ||",
    "claim": "In the content module, using emotion embeddings improves performance, especially on the Twitter dataset, where the F1-score increases by nearly 6%. Social emotion is more important than publisher emotion on both datasets.",
    "label": "refute"
  },
  {
    "id": "training_792_refute",
    "table_caption": "Table: Comparison of results using other learning schemes on News20, BBC and BBC Sports datasets. The proposed model using a deep learning model as a baseline achieves competitive performance for all the three datasets.",
    "table_content": "|| Model | News20 Accuracy | News20 F1-Score | BBC Accuracy | BBC F1-Score | BBC Sports Accuracy | BBC Sports F1-Score ||\n|| kNN-ngrams | 35.25 | 0.3566 | 74.61 | 0.7376 | 94.59 | 0.9487 ||\n|| Multinomial NB-bigram | [BOLD] 79.21 | [BOLD] 0.7841 | 95.96 | 0.9575 | 95.95 | 0.9560 ||\n|| SVM-bigram | 75.04 | 0.7474 | 94.83 | 0.9456 | 93.92 | 0.9393 ||\n|| SVM-ngrams | 78.60 | 0.7789 | 95.06 | 0.9484 | 95.95 | 0.9594 ||\n|| Random Forests-bigram | 69.01 | 0.6906 | 87.19 | 0.8652 | 85.81 | 0.8604 ||\n|| Random Forests-ngrams | 78.36 | 0.7697 | 94.83 | 0.9478 | 94.59 | 0.9487 ||\n|| Random Forests- tf-idf | 78.6 | 0.7709 | 95.51 | 0.9547 | [BOLD] 96.62 | [BOLD] 0.9660 ||\n|| Bi-LSTM | 65.17 | 0.6328 | 91.33 | 0.9122 | 84.22 | 0.8395 ||\n|| Instance-Infused Bi-LSTM | 78.29 | 0.7773 | [BOLD] 96.09 | [BOLD] 0.9619 | 91.56 | 0.9100 ||",
    "claim": "The instance-infused Bi-LSTM model achieves the highest accuracy and F1-score on the BBC Sports dataset.",
    "label": "refute"
  },
  {
    "id": "training_744_refute",
    "table_caption": "Table: Rouge scores of predicted keyphrases by various methods",
    "table_content": "|| [BOLD] Method | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [ITALIC]  [BOLD] R [BOLD] 1 [ITALIC]  [BOLD] F [BOLD] 1 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [ITALIC]  [BOLD] R [BOLD] L [BOLD] F [BOLD] 1 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [ITALIC]  [BOLD] R [BOLD] 1 [ITALIC]  [BOLD] F [BOLD] 1 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [ITALIC]  [BOLD] R [BOLD] L [BOLD] F [BOLD] 1 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [ITALIC]  [BOLD] R [BOLD] 1 [ITALIC]  [BOLD] F [BOLD] 1 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [ITALIC]  [BOLD] R [BOLD] L [BOLD] F [BOLD] 1 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [ITALIC]  [BOLD] R [BOLD] 1 [ITALIC]  [BOLD] F [BOLD] 1 | [BOLD]  Hulth (500) Krapivin (400) Meng (20K) OAGK (100K)  [ITALIC]  [BOLD] R [BOLD] L [BOLD] F [BOLD] 1 ||\n|| Yake! | 37.48 | 24.83 | 26.19 | 18.57 | 26.47 | 17.36 | 20.38 | 14.54 ||\n|| TopicRank | 32.0 | 20.36 | 14.08 | 11.47 | 21.68 | 15.94 | 17.46 | 13.28 ||\n|| Maui | 36.88 | 27.16 | 28.29 | 23.74 | 34.33 | 28.12 | 32.16 | 25.09 ||\n|| CopyRnn | [BOLD] 44.58 | [BOLD] 35.24 | [BOLD] 39.73 | [BOLD] 30.29 | [BOLD] 42.93 | 34.62 | [BOLD] 43.54 | [BOLD] 36.09 ||\n|| Merge | 15.19 | 9.45 | 9.66 | 7.14 | 16.53 | 12.31 | 17.3 | 14.43 ||\n|| Inject | 14.15 | 8.81 | 9.58 | 6.79 | 15.6 | 11.21 | 14.3 | 11.08 ||\n|| Abs | 27.54 | 19.48 | 25.59 | 18.2 | 28.31 | 22.16 | 29.05 | 25.77 ||\n|| PointCov | 37.16 | 33.69 | 35.81 | 29.52 | 38.47 | [BOLD] 35.06 | 38.66 | 34.04 ||",
    "claim": "CopyRnn achieves the highest scores across all datasets, with Maui having the second-highest scores. Abs, PointCov, and Yake! have comparable scores, while TopicRank, Merge, and Inject have the lowest scores.",
    "label": "refute"
  },
  {
    "id": "training_816_refute",
    "table_caption": "Table: Effect of layers fine-tuning",
    "table_content": "|| ASR | NLU | Accuracy on Test, % | Accuracy on Test, % | Accuracy on Test, % | Validation ||\n|| layers | layers | SwBD | MRDA | FSC | loss ||\n|| 0 | 0 | 43.76 | 56.08 | 68.07 | 0.26 ||\n|| 0 | 1 | 37.61 | 56.47 | 85.53 | 0.19 ||\n|| 1 | 0 | 52.37 | [BOLD] 60.21 | 86.42 | 0.16 ||\n|| 1 | 1 | 52.05 | 58.32 | [BOLD] 86.82 | 0.17 ||\n|| 2 | 0 | 52.93 | 59.42 | 85.76 | [BOLD] 0.15 ||\n|| 3 | 0 | [BOLD] 53.81 | 58.90 | 85.53 | 0.16 ||",
    "claim": "Fine-tuning former NLU layers is more beneficial than former ASR encoder layers.",
    "label": "refute"
  },
  {
    "id": "training_814_refute",
    "table_caption": "Table: Market trend prediction using financial tweets from Twitter (01/04/2014 – 01/04/2015).",
    "table_content": "|| Type | baseline Acc | baseline  [ITALIC] Fup1 | baseline  [ITALIC] Fdown1 | all+attitude+emotion Acc | all+attitude+emotion  [ITALIC] Fup1 | all+attitude+emotion  [ITALIC] Fdown1 | all+emotion Acc | all+emotion  [ITALIC] Fup1 | all+emotion  [ITALIC] Fdown1 | filtering+emotion Acc | filtering+emotion  [ITALIC] Fup1 | filtering+emotion  [ITALIC] Fdown1 ||\n|| DJIA | [BOLD] 0.810 | [BOLD] 0.854 | 0.727 | 0.810 | 0.846 | [BOLD] 0.750 | 0.778 | 0.829 | 0.682 | - | - | - ||\n|| AAPL | [BOLD] 0.889 | [BOLD] 0.918 | [BOLD] 0.829 | 0.810 | 0.860 | 0.700 | 0.794 | 0.847 | 0.683 | 0.794 | 0.831 | 0.735 ||\n|| JPM | 0.746 | 0.800 | 0.652 | 0.730 | 0.779 | 0.653 | 0.746 | 0.789 | 0.680 | [BOLD] 0.778 | [BOLD] 0.829 | [BOLD] 0.682 ||\n|| GBP/USD | [BOLD] 0.708 | [BOLD] 0.387 | [BOLD] 0.808 | 0.662 | 0.389 | 0.766 | 0.631 | 0.294 | 0.750 | - | - | - ||\n|| EUR/USD | 0.685 | [BOLD] 0.627 | 0.727 | 0.685 | [BOLD] 0.627 | 0.727 | [BOLD] 0.692 | 0.626 | [BOLD] 0.739 | - | - | - ||",
    "claim": "Noticeable performance improvements for the JPM stock were observed in the \"all+emotion\" setting.",
    "label": "refute"
  },
  {
    "id": "training_715_refute",
    "table_caption": "Table: Inflection accuracy on the test languages; best results in bold. Languages listed by language family; from top to bottom: Romance, Slavic, Uralic.",
    "table_content": "|| [EMPTY] | [BOLD] PG  [BOLD] MAML-PG | [BOLD] PG  [BOLD] MulPG+FT | [BOLD] PG  [BOLD] MulPG | [BOLD] PG | [BOLD] MED  [BOLD] MAML-MED | [BOLD] MED  [BOLD] MulMED+FT | [BOLD] MED  [BOLD] MulMED | [BOLD] UZH ||\n|| Asturian | 72.08 | [BOLD] 73.28 | 66.98 | 69.82 | 68.94 | 65.54 | 58.62 | 71.56 ||\n|| French | [BOLD] 64.16 | 61.54 | 49.10 | 53.98 | 46.18 | 43.64 | 24.68 | 64.04 ||\n|| Friulian | [BOLD] 80.00 | 77.00 | 66.00 | 69.40 | 63.40 | 59.20 | 35.40 | 78.20 ||\n|| Italian | 53.12 | [BOLD] 54.30 | 40.12 | 45.06 | 40.88 | 35.22 | 20.22 | 53.12 ||\n|| Ladin | 58.80 | 58.60 | 40.80 | 66.20 | 50.20 | 47.80 | 31.80 | [BOLD] 68.60 ||\n|| Latin | 14.04 | 13.90 | 7.90 | 13.36 | 9.10 | 8.28 | 2.52 | [BOLD] 15.98 ||\n|| Middle French | 83.18 | 80.68 | 65.76 | 81.70 | 71.18 | 66.24 | 40.36 | [BOLD] 85.16 ||\n|| Neapolitan | 81.00 | 82.20 | 72.60 | 77.00 | 78.00 | 75.20 | 47.40 | [BOLD] 84.20 ||\n|| Norman | 53.20 | 54.80 | 34.40 | [BOLD] 58.40 | [BOLD] 58.40 | 50.00 | 13.20 | 50.40 ||\n|| Occitan | [BOLD] 77.20 | 72.40 | 66.00 | 70.60 | 70.20 | 70.80 | 46.40 | 74.80 ||\n|| Old French | [BOLD] 44.48 | 42.86 | 27.82 | 36.16 | 34.58 | 31.78 | 16.84 | 42.14 ||\n|| Romanian | 40.94 | 39.40 | 31.44 | 33.70 | 29.32 | 26.30 | 5.12 | [BOLD] 42.28 ||\n|| Spanish | 71.62 | [BOLD] 72.20 | 66.84 | 55.58 | 65.42 | 59.08 | 52.02 | 65.08 ||\n|| Venetian | 74.72 | 73.46 | 68.06 | 75.14 | 71.08 | 67.86 | 40.32 | [BOLD] 76.40 ||\n|| Belarusian | 26.32 | 24.74 | 17.24 | 22.12 | 14.68 | 11.84 | 1.12 | [BOLD] 27.56 ||\n|| Czech | [BOLD] 50.36 | 49.08 | 42.86 | 35.46 | 34.46 | 33.06 | 26.14 | 41.56 ||\n|| Kashubian | 59.20 | 58.40 | 56.00 | 57.20 | 58.00 | 59.20 | 19.20 | [BOLD] 63.60 ||\n|| Lower Sorbian | [BOLD] 53.52 | 51.54 | 47.22 | 40.72 | 40.36 | 36.88 | 15.36 | 41.88 ||\n|| Old Church Slavonic | [BOLD] 50.00 | [BOLD] 50.00 | 35.80 | 47.00 | 38.00 | 29.40 | 7.40 | 42.40 ||\n|| Polish | [BOLD] 43.40 | 42.60 | 35.38 | 29.62 | 33.40 | 31.56 | 14.36 | 41.82 ||\n|| Russian | [BOLD] 52.48 | 51.20 | 41.74 | 40.66 | 29.34 | 23.80 | 11.68 | 48.22 ||\n|| Serbo-Croatian | [BOLD] 39.88 | 37.04 | 23.02 | 35.88 | 26.50 | 21.62 | 9.32 | 38.38 ||\n|| Slovene | [BOLD] 59.68 | 58.44 | 45.94 | 48.42 | 43.62 | 42.74 | 27.92 | 53.18 ||\n|| Ukrainian | 48.30 | [BOLD] 48.72 | 43.68 | 36.38 | 35.66 | 28.12 | 18.04 | 46.78 ||\n|| Hungarian | 27.64 | 23.22 | 12.46 | 37.78 | 20.02 | 17.60 | 4.04 | [BOLD] 38.04 ||\n|| Ingrian | [BOLD] 50.40 | 46.40 | 31.60 | 44.00 | 34.40 | 40.00 | 16.40 | 32.80 ||\n|| Karelian | 85.20 | 84.80 | 62.40 | [BOLD] 90.80 | 79.60 | 71.20 | 29.20 | 79.20 ||\n|| Livonian | 28.00 | 27.00 | 22.20 | [BOLD] 32.80 | 22.80 | 22.60 | 2.60 | 29.80 ||\n|| Votic | [BOLD] 26.60 | 24.60 | 11.20 | 22.60 | 25.60 | 25.20 | 11.40 | 23.00 ||\n|| [BOLD] average | [BOLD] 54.12 | 52.91 | 42.50 | 49.23 | 44.60 | 41.44 | 22.38 | 52.42 ||",
    "claim": "MAML-PG achieves a 1.21% higher average accuracy than MulPG+FT, and MulPG+FT achieves a lower average accuracy than MulPG.",
    "label": "refute"
  },
  {
    "id": "training_763_refute",
    "table_caption": "Table: Ablation study on modeling context on TED Zh-En development set. ”Doc” means using a entire document as a sequence for input or output. BLEU{}_{\\rm doc} indicates the document-level BLEU score calculated on the concatenation of all output sentences.",
    "table_content": "|| [BOLD] Model | BLEU (BLEU{}_{\\rm doc}) ||\n|| SentNmt [Vaswani2017Attention] | 11.4 (21.0) ||\n|| DocNmt (documents as input/output) | n/a (17.0) ||\n|| [ITALIC] Modeling source context | [ITALIC] Modeling source context ||\n|| Doc2Sent | 6.8 ||\n|| + reset word positions for each sentence | 10.0 ||\n|| + segment embedding | 10.5 ||\n|| + segment-level relative attention | 12.2 ||\n|| + context fusion gate | 12.4 ||\n|| [ITALIC] Modeling target context | [ITALIC] Modeling target context ||\n|| Transformer-XL decoder [Sent2Doc] | 12.4 ||\n|| Final | 12.9 (24.4) ||",
    "claim": "The final model achieves the highest BLEU score of 12.9 (24.4 for document-level BLEU) by modeling only the source context.",
    "label": "refute"
  },
  {
    "id": "training_782_refute",
    "table_caption": "Table: BLEU scores for models with and without back-translation (BT) on the IWSLT’14 English-German (En-De), German-English (De-En) and WMT’14 En-De tasks. Column |D| shows the total data used in back-translation compared to the original parallel data.",
    "table_content": "|| [BOLD] Task | [BOLD] No back-translation  [BOLD] Baseline | [BOLD] No back-translation  [BOLD] Ours | [BOLD] With back-translation | [ITALIC] D| | [BOLD] With back-translation  [BOLD] Baseline | [BOLD] With back-translation  [BOLD] Ours ||\n|| IWSLT’14 En-De | 28.6 | 30.6 | 29× | 30.0 | [BOLD] 31.8 ||\n|| IWSLT’14 De-En | 34.7 | 37.0 | 29× | 37.1 | [BOLD] 38.5 ||\n|| WMT’14 En-De | 29.3 | 30.7 | 2.4× | 30.8 | [BOLD] 31.8 ||",
    "claim": "With back-translation, the baseline achieves higher BLEU scores than the proposed method in all tasks.",
    "label": "refute"
  },
  {
    "id": "training_794_refute",
    "table_caption": "Table: Fine-grained F1 scores on the AMR 2.0 test set. vN’17 is van Noord and Bos (2017b); L’18 is Lyu and Titov (2018); N’19 is Naseem et al. (2019).",
    "table_content": "|| Metric | vN’18 | L’18 | N’19 | Ours ||\n|| Smatch | 71.0 | 74.4 | 75.5 | [BOLD] 76.3±0.1 ||\n|| Unlabeled | 74 | 77 | [BOLD] 80 | 79.0±0.1 ||\n|| No WSD | 72 | 76 | 76 | [BOLD] 76.8±0.1 ||\n|| Reentrancies | 52 | 52 | 56 | [BOLD] 60.0±0.1 ||\n|| Concepts | 82 | [BOLD] 86 | [BOLD] 86 | 84.8±0.1 ||\n|| Named Ent. | 79 | [BOLD] 86 | 83 | 77.9±0.2 ||\n|| Wikification | 65 | 76 | 80 | [BOLD] 85.8±0.3 ||\n|| Negation | 62 | 58 | 67 | [BOLD] 75.2±0.2 ||\n|| SRL | 66 | 70 | [BOLD] 72 | 69.7±0.2 ||",
    "claim": "\"Ours\" achieves the highest scores in reentrancies, wikification, and concepts compared to other systems.",
    "label": "refute"
  },
  {
    "id": "training_761_refute",
    "table_caption": "Table: Accuracy (\\%) of discourse phenomena. * different data and system conditions, only for reference.",
    "table_content": "|| [BOLD] Model | deixis | lex.c. | ell.infl. | ell.VP ||\n|| SentNmt | 50.0 | 45.9 | 52.2 | 24.2 ||\n|| Ours | 61.3 | 46.1 | 61.0 | 35.6 ||\n|| voita2018context [voita2018context]* | 81.6 | 58.1 | 72.2 | 80.0 ||",
    "claim": "The \"Ours\" model outperforms the SentNmt model in most discourse phenomena test sets, but not in deixis.",
    "label": "refute"
  },
  {
    "id": "training_765_refute",
    "table_caption": "Table: Comparison between UR-FUNNY and notable humor detection datasets in the NLP community. Here, ‘pos’, ’neg’ , ‘mod’ and ‘spk’ denote positive, negative, modalities and speaker respectively.",
    "table_content": "|| Dataset | #Pos | #Neg | Mod | type | #spk ||\n|| 16000 One-Liners | 16000 | 16000 | { [ITALIC] l} | joke | - ||\n|| Pun of the Day | 2423 | 2423 | { [ITALIC] l} | pun | - ||\n|| PTT Jokes | 1425 | 2551 | { [ITALIC] l} | political | - ||\n|| Ted Laughter | 4726 | 4726 | { [ITALIC] l} | speech | 1192 ||\n|| Big Bang Theory | 18691 | 24981 | { [ITALIC] l,a} | tv show | <50 ||\n|| [BOLD] UR-Funny | 8257 | 8257 | { [ITALIC] l,a,v} | speech | 1741 ||",
    "claim": "UR-FUNNY and Big Bang Theory are the only humor detection datasets that incorporate all three modalities of text, vision, and audio.",
    "label": "refute"
  },
  {
    "id": "training_844_refute",
    "table_caption": "Table: Length filtering test results after tuning length ranges on development data (averaged over languages and data sources for the NMT rows). Results are on STS datasets (Pearson’s r×100).",
    "table_content": "|| Filtering Method | NMT GRAN | NMT Avg | SimpWiki GRAN | SimpWiki Avg ||\n|| None (Random) | 66.9 | 65.5 | 67.2 | 65.8 ||\n|| Length | 67.3 | 66.0 | 67.4 | 66.2 ||\n|| Tuned Len. Range | [0,10] | [0,10] | [0,10] | [0,15] ||",
    "claim": "Length-based filtering leads to lower performance than the random baseline for both NMT and SimpWiki.",
    "label": "refute"
  },
  {
    "id": "training_845_refute",
    "table_caption": "Table: Quality filtering test results after tuning quality hyperparameters on development data (averaged over languages and data sources for the NMT rows). Results are on STS datasets (Pearson’s r×100).",
    "table_content": "|| Filtering Method | GRAN | Avg ||\n|| None (Random) | 66.9 | 65.5 ||\n|| Translation Cost | 66.6 | 65.4 ||\n|| Language Model | 66.7 | 65.5 ||\n|| Reference Classification | 67.0 | 65.5 ||",
    "claim": "Random selection slightly outperforms Reference Classification with the GRAN model.",
    "label": "refute"
  },
  {
    "id": "training_811_refute",
    "table_caption": "Table: Market trend prediction using main technical indicators — the baseline model.",
    "table_content": "|| Type | Method | 3-day ahead Acc | 3-day ahead  [ITALIC] Fup1 | 3-day ahead  [ITALIC] Fdown1 | 5-day ahead Acc | 5-day ahead  [ITALIC] Fup1 | 5-day ahead  [ITALIC] Fdown1 ||\n|| DJIA | SVM | 0.616 | 0.738 | 0.282 | [BOLD] 0.700 | [BOLD] 0.754 | [BOLD] 0.615 ||\n|| DJIA | LSTM | 0.559 | 0.706 | 0.120 | 0.585 | 0.728 | 0.127 ||\n|| AAPL | SVM | 0.577 | 0.676 | 0.391 | [BOLD] 0.685 | [BOLD] 0.723 | [BOLD] 0.634 ||\n|| AAPL | LSTM | 0.547 | 0.693 | 0.138 | 0.521 | 0.641 | 0.282 ||\n|| JPM | SVM | 0.677 | 0.747 | 0.552 | [BOLD] 0.673 | [BOLD] 0.733 | [BOLD] 0.578 ||\n|| JPM | LSTM | 0.541 | 0.665 | 0.269 | 0.573 | 0.676 | 0.373 ||\n|| EUR/USD | SVM | 0.642 | 0.607 | 0.672 | [BOLD] 0.671 | [BOLD] 0.620 | [BOLD] 0.710 ||\n|| EUR/USD | LSTM | 0.509 | 0.423 | 0.572 | 0.563 | 0.370 | 0.665 ||\n|| GBP/USD | SVM | 0.610 | 0.589 | 0.630 | [BOLD] 0.714 | [BOLD] 0.705 | [BOLD] 0.723 ||\n|| GBP/USD | LSTM | 0.500 | 0.604 | 0.323 | 0.633 | 0.646 | 0.618 ||",
    "claim": "LSTM consistently achieves higher accuracy and more balanced F1 scores compared to SVM across all assets and prediction horizons.",
    "label": "refute"
  },
  {
    "id": "training_829_refute",
    "table_caption": "Table: Analysis of errors by a human annotator of the top ten retrievals on development data. Percentages (%) indicate the absolute drop in P@10 due to that error type.",
    "table_content": "|| Error type | XVisionSpeech Count | XVisionSpeech % | XBoWCNN Count | XBoWCNN % ||\n|| (1) Correct (exact) | 032 | 08.2 | 45 | 11.5 ||\n|| (2) Semantically related | 086 | 22.1 | 13 | 03.3 ||\n|| (3) Incorrect retrieval | 035 | 09.0 | 19 | 04.9 ||\n|| Total | 153 | 39.3 | 77 | 19.7 ||",
    "claim": "The bulk of errors from XVisionSpeechCNN is due to incorrect retrievals, with 22.1% of errors falling into this category.",
    "label": "refute"
  },
  {
    "id": "training_775_refute",
    "table_caption": "Table: Tagging accuracies of domain adaptation models from the PTB to the PPCEME.",
    "table_content": "|| System | IV | OOV | All ||\n|| SVM | 81.68 | 48.96 | 74.15 ||\n|| SCL | 82.01 | 55.45 | 75.89 ||\n|| Brown | 81.81 | 56.76 | 76.04 ||\n|| word2vec | 81.79 | 56.00 | 75.85 ||\n|| Fema-single | 82.30 | 62.63 | 77.77 ||\n|| Fema-attribute | 82.34 | 63.16 | 77.92 ||",
    "claim": "Fema-attribute achieves higher accuracy on OOV tokens compared to Fema-single, and SCL performs worse than Brown clustering and word2vec on IV tokens but worse on OOV tokens.",
    "label": "refute"
  },
  {
    "id": "training_769_refute",
    "table_caption": "Table: Results on the test set using all subject’s history as a single document, i.e. timeless classification.",
    "table_content": "|| [EMPTY] | [ITALIC] F1 | [ITALIC] π | [ITALIC] ρ ||\n|| SS3 | [BOLD] 0.61 | [BOLD] 0.63 | 0.60 ||\n|| LOGREG | 0.59 | 0.56 | 0.63 ||\n|| SVM | 0.55 | 0.5 | 0.62 ||\n|| MNB | 0.39 | 0.25 | [BOLD] 0.96 ||\n|| KNN | 0.54 | 0.5 | 0.58 ||",
    "claim": "SS3 obtained the highest value for F1 (0.61) but not for Precision, while MNB had the highest Recall (0.96).",
    "label": "refute"
  },
  {
    "id": "training_841_refute",
    "table_caption": "Table: Pearson’s r and Mean Absolute Error (MAE) for prediction of MET scores (test set, 57 participants) and TOEFL scores (leave-one-out cross validation, all 53 participants) from eye movement patterns in reading. We consider two baselines which do not use eyetracking information: (1) the average proficiency score in the training set, which yields 4.82 MAE on MET and 8.29 MAE on TOEFL, and (2) the reading speed of the participant.",
    "table_content": "|| [EMPTY] | [BOLD] MET Fixed | [BOLD] MET Fixed | [BOLD] MET Any | [BOLD] MET Any | [BOLD] TOEFL Fixed | [BOLD] TOEFL Fixed | [BOLD] TOEFL Any | [BOLD] TOEFL Any ||\n|| [BOLD] Features | [ITALIC] r | MAE | [ITALIC] r | MAE | [ITALIC] r | MAE | [ITALIC] r | MAE ||\n|| Reading Speed | 0.27 | 4.58 | 0.24 | 4.62 | 0.09 | 7.92 | 0.06 | 7.96 ||\n|| WP-Coefficients | 0.43 | 4.11 | 0.44 | 4.14 | 0.34 | 7.76 | 0.31 | [BOLD] 7.49 ||\n|| S-Clusters | 0.56 | 3.87 | [BOLD] 0.49 | [BOLD] 4.11 | [BOLD] 0.55 | 7.45 | [BOLD] 0.50 | 7.76 ||\n|| Transitions | 0.52 | 3.93 | [EMPTY] | [EMPTY] | 0.38 | 7.11 | [EMPTY] | [EMPTY] ||\n|| WFC | [BOLD] 0.70 | [BOLD] 3.31 | [EMPTY] | [EMPTY] | 0.50 | [BOLD] 6.68 | [EMPTY] | [EMPTY] ||",
    "claim": "Reading speed outperforms eyetracking-based features on both MET and TOEFL in terms of MAE.",
    "label": "refute"
  },
  {
    "id": "training_758_refute",
    "table_caption": "Table: Performance Comparison of Fake News Detection on Two Datasets.",
    "table_content": "|| Dataset | Methods | Accuracy | Precision | Recall | F1-Score ||\n|| Weibo | DTC | 0.756 | 0.754 | 0.758 | 0.756 ||\n|| Weibo | ML-GRU | 0.799 | 0.810 | 0.790 | 0.800 ||\n|| Weibo | Basic-GRU | 0.835 | 0.830 | 0.850 | 0.840 ||\n|| Weibo | CSI | 0.835 | 0.735 | [BOLD] 0.996 | 0.858 ||\n|| Weibo | HSA-BLSTM | 0.843 | [BOLD] 0.860 | 0.810 | 0.834 ||\n|| Weibo | SAME | 0.776 | 0.770 | 0.780 | 0.775 ||\n|| [EMPTY] | [BOLD] DEAN | [BOLD] 0.872 | [BOLD] 0.860 | 0.890 | [BOLD] 0.874 ||\n|| Twitter | DTC | 0.613 | 0.608 | 0.570 | 0.588 ||\n|| Twitter | ML-GRU | 0.684 | 0.663 | 0.740 | 0.692 ||\n|| Twitter | Basic-GRU | 0.695 | 0.674 | 0.721 | 0.697 ||\n|| Twitter | CSI | 0.696 | 0.706 | 0.649 | 0.671 ||\n|| Twitter | HSA-BLSTM | 0.718 | [BOLD] 0.731 | 0.663 | 0.695 ||\n|| Twitter | SAME | 0.667 | 0.613 | 0.849 | 0.712 ||\n|| [EMPTY] | [BOLD] DEAN | [BOLD] 0.751 | 0.698 | [BOLD] 0.860 | [BOLD] 0.771 ||",
    "claim": "The DEAN model achieves the highest precision and F1-score on both the Weibo and Twitter datasets compared to all other models.",
    "label": "refute"
  },
  {
    "id": "training_798_refute",
    "table_caption": "Table: Inter-annotator agreement for our English (goldEN) and German (goldDE) gold standard, as well as the lexicon by Warriner13 for comparision; Averaged standard deviation of ratings for each VAD dimension and mean over all dimensions.",
    "table_content": "|| [EMPTY] | Valence | Arousal | Dominance | Mean ||\n|| goldEN | 1.20 | 1.08 | 1.41 | 1.23 ||\n|| goldDE | 1.72 | 1.56 | 2.31 | 1.86 ||\n|| Warriner | 1.68 | 2.30 | 2.16 | 2.05 ||",
    "claim": "The Warriner lexicon displays higher rating consistency with an IAA score of 2.05, compared to 1.23 for English and 1.86 for German in the gold standard lexicons.",
    "label": "refute"
  },
  {
    "id": "training_787_refute",
    "table_caption": "Table: Accuracy of LAMBADA vs. other generative approaches over all datasets and classifiers. LAMBADA is statistically (* McNemar, p−value<0.01) superior to all models on each classifier and each dataset (on par to EDA with SVM on TREC).",
    "table_content": "|| Dataset | [EMPTY] | BERT | SVM | LSTM ||\n|| ATIS | Baseline | 53.3 | 35.6 | 29.0 ||\n|| ATIS | EDA | 62.8 | 35.7 | 27.3 ||\n|| ATIS | CVAE | 60.6 | 27.6 | 14.9 ||\n|| ATIS | CBERT | 51.4 | 34.8 | 23.2 ||\n|| ATIS | LAMBADA | [BOLD] 75.7* | [BOLD] 56.5* | [BOLD] 33.7* ||\n|| TREC | Baseline | 60.3 | 42.7 | 17.7 ||\n|| TREC | EDA | 62.6 | [BOLD] 44.8* | 23.1 ||\n|| TREC | CVAE | 61.1 | 40.9 | 25.4* ||\n|| TREC | CBERT | 61.4 | 43.8 | 24.2 ||\n|| TREC | LAMBADA | [BOLD] 64.3* | 43.9* | [BOLD] 25.8 * ||\n|| WVA | Baseline | 67.2 | 60.2 | 26.0 ||\n|| WVA | EDA | 67.0 | 60.7 | 28.2 ||\n|| WVA | CVAE | 65.4 | 54.8 | 22.9 ||\n|| WVA | CBERT | 67.4 | 60.7 | 28.4 ||\n|| WVA | LAMBADA | [BOLD] 68.6* | [BOLD] 62.9* | [BOLD] 32.0* ||",
    "claim": "On the TREC dataset with the SVM classifier, the method is on par with CVAE.",
    "label": "refute"
  },
  {
    "id": "training_773_refute",
    "table_caption": "Table: Accuracy results for adapting from the PTB to the PPCMBE and the PPCEME of historical English. ∗Error reduction for the normalized PPCEME is computed against the unnormalized SVM accuracy, showing total error reduction.",
    "table_content": "|| Target | Normalized | baseline SVM | baseline MEMM (Stanford) | SCL | Brown | word2vec | Fema single embedding | Fema attribute embeddings (error reduction) ||\n|| ppcmbe | No | 81.12 | 81.35 | 81.66 | 81.65 | 81.75 | 82.34 | [BOLD] 82.46 (7%) ||\n|| ppceme | No | 74.15 | 74.34 | 75.89 | 76.04 | 75.85 | 77.77 | [BOLD] 77.92 (15%) ||\n|| ppceme | Yes | 76.73 | 76.87 | 77.61 | 77.65 | 77.76 | 78.85 | [BOLD] 79.05 (19%∗) ||",
    "claim": "Fema attribute embeddings improve performance by 2.34% on PPCMBE data and by 3.77% on unnormalized PPCEME data. Spelling normalization improves the baseline systems by more than 2.5%.",
    "label": "refute"
  },
  {
    "id": "training_836_refute",
    "table_caption": "Table: Comparison on German→English.",
    "table_content": "|| toolkit | BLEU [%] 2015 | BLEU [%] 2017 ||\n|| RETURNN | [BOLD] 31.2 | [BOLD] 31.3 ||\n|| Sockeye | 29.7 | 30.2 ||",
    "claim": "Sockeye achieves higher BLEU scores than RETURNN in both 2015 and 2017.",
    "label": "refute"
  },
  {
    "id": "training_843_refute",
    "table_caption": "Table: Test correlations for our models when trained on sentences with particular length ranges (averaged over languages and data sources for the NMT rows). Results are on STS datasets (Pearson’s r×100).",
    "table_content": "|| Data | Model | Length Range 0-10 | Length Range 10-20 | Length Range 20-30 | Length Range 30-100 ||\n|| SimpWiki | GRAN | 67.4 | 67.7 | 67.1 | 67.3 ||\n|| SimpWiki | Avg | 65.9 | 65.7 | 65.6 | 65.9 ||\n|| NMT | GRAN | 66.6 | 66.5 | 66.0 | 64.8 ||\n|| NMT | Avg | 65.7 | 65.6 | 65.3 | 65.0 ||",
    "claim": "Performance for NMT data increases as sentence length increases for the GRAN model, while it decreases for the Avg model.",
    "label": "refute"
  },
  {
    "id": "training_808_refute",
    "table_caption": "Table: Results of different methods on NYT and WebNLG datasets. Our re-implementation is marked by *.",
    "table_content": "|| Method | NYT  [ITALIC] Prec. | NYT  [ITALIC] Rec. | NYT  [ITALIC] F1 | WebNLG  [ITALIC] Prec. | WebNLG  [ITALIC] Rec. | WebNLG  [ITALIC] F1 ||\n|| NovelTagging  zheng2017Joint | 62.4 | 31.7 | 42.0 | 52.5 | 19.3 | 28.3 ||\n|| CopyR [ITALIC] OneDecoder  zeng2018Extracting | 59.4 | 53.1 | 56.0 | 32.2 | 28.9 | 30.5 ||\n|| CopyR [ITALIC] MultiDecoder  zeng2018Extracting | 61.0 | 56.6 | 58.7 | 37.7 | 36.4 | 37.1 ||\n|| GraphRel1 [ITALIC] p  fu2019GraphRel | 62.9 | 57.3 | 60.0 | 42.3 | 39.2 | 40.7 ||\n|| GraphRel2 [ITALIC] p  fu2019GraphRel | 63.9 | 60.0 | 61.9 | 44.7 | 41.1 | 42.9 ||\n|| CopyR [ITALIC] RL  zeng2019Learning | 77.9 | 67.2 | 72.1 | 63.3 | 59.9 | 61.6 ||\n|| CopyR∗ [ITALIC] RL | 72.8 | 69.4 | 71.1 | 60.9 | 61.1 | 61.0 ||\n|| CasRel [ITALIC] random | 81.5 | 75.7 | 78.5 | 84.7 | 79.5 | 82.0 ||\n|| CasRel [ITALIC] LSTM | 84.2 | 83.0 | 83.6 | 86.9 | 80.6 | 83.7 ||\n|| CasRel | [BOLD] 89.7 | [BOLD] 89.5 | [BOLD] 89.6 | [BOLD] 93.4 | [BOLD] 90.1 | [BOLD] 91.8 ||",
    "claim": "The CasRel model achieves the highest precision and recall on both the NYT and WebNLG datasets, but not the highest F1-score compared to all other methods.",
    "label": "refute"
  },
  {
    "id": "training_745_refute",
    "table_caption": "Table: This table shows the average performance of the classifiers with the various timeframes. The values are the arithmetic mean of the classifiers’ results. These are an SVM, a NB classifier and an LM classifier. A bold font highlights the best result considering this metric. The plus and minus symbols indicate whether the performance metric was calculated for disruptive (+) or constructive contributions (−).",
    "table_content": "|| [BOLD] time | [BOLD] recall+ | [BOLD] recall− | [BOLD] precision+ | [BOLD] precision− | [BOLD] F1+ | [BOLD] F1− | [BOLD] accuracy | [BOLD] AUC ||\n|| 13 hours | [BOLD] 49.81 | 75.35 | 66.68 | 60.26 | 56.8 | 66.87 | 62.58 | 0.551 ||\n|| 1 day | 49.29 | [BOLD] 76.63 | [BOLD] 67.83 | [BOLD] 60.27 | [BOLD] 56.93 | [BOLD] 67.42 | [BOLD] 62.96 | [BOLD] 0.562 ||\n|| 1.5 days | 47.45 | 76.03 | 66.46 | 59.22 | 55.18 | 66.52 | 61.74 | 0.542 ||\n|| 2 days | 48.14 | 75.0 | 65.85 | 59.24 | 55.38 | 66.11 | 61.57 | 0.542 ||\n|| 2.5 days | 46.57 | 75.77 | 65.88 | 58.72 | 54.35 | 66.09 | 61.17 | 0.536 ||\n|| 3 days | 46.49 | 74.04 | 64.33 | 58.12 | 53.71 | 65.03 | 60.26 | 0.529 ||\n|| 4 days | 45.35 | 74.03 | 63.72 | 57.59 | 52.76 | 64.7 | 59.69 | 0.521 ||\n|| 5 days | 45.13 | 74.51 | 64.07 | 57.64 | 52.72 | 64.92 | 59.82 | 0.522 ||\n|| 6 days | 42.95 | 74.6 | 63.02 | 56.7 | 50.85 | 64.35 | 58.77 | 0.512 ||",
    "claim": "Most performance metrics peak at the 2-day timeframe, including recall−, precision−, F1−, accuracy, and AUC.",
    "label": "refute"
  },
  {
    "id": "training_855_refute",
    "table_caption": "Table: Comparison of two masking methods on COCO Captions.",
    "table_content": "|| [EMPTY] | C | B@4 | M | S ||\n|| multi [MASK] | 117.8 | 36.1 | 28.2 | 21.3 ||\n|| single [MASK] | [BOLD] 118.1 | [BOLD] 36.4 | [BOLD] 28.3 | [BOLD] 21.3 ||",
    "claim": "Multi [MASK] achieves higher scores than Single [MASK] in at least one metric (C, B@4, M, S).",
    "label": "refute"
  },
  {
    "id": "training_877_refute",
    "table_caption": "Table: Comparison of different models on Reddit SWMH collection, where precision, recall, and F1 score are weighted average.",
    "table_content": "|| Model | Accuracy | Precision | Recall | F1 ||\n|| fastText | 0.5722 | 0.5760 | 0.5722 | 0.5721 ||\n|| CNN | 0.5657 | 0.5925 | 0.5657 | 0.5556 ||\n|| LSTM | 0.5934 | 0.6032 | 0.5934 | 0.5917 ||\n|| BiLSTM | 0.6196 | 0.6204 | 0.6196 | 0.6190 ||\n|| RCNN | 0.6096 | 0.6161 | 0.6096 | 0.6063 ||\n|| SSA | 0.6214 | 0.6249 | 0.6214 | 0.6226 ||\n|| RN | [BOLD] 0.6474 | [BOLD] 0.6510 | [BOLD] 0.6474 | [BOLD] 0.6478 ||",
    "claim": "SSA achieves the highest accuracy, precision, recall, and F1 score among the models tested.",
    "label": "refute"
  },
  {
    "id": "training_818_refute",
    "table_caption": "Table: Results for multilingual direct SLT systems with 6 and 8 target languages.",
    "table_content": "|| [EMPTY] | De | Nl | Es | Fr | It | Pt | Ro | Ru ||\n|| [EMPTY] | [BOLD] Baseline | [BOLD] Baseline | [BOLD] Baseline | [BOLD] Baseline | [BOLD] Baseline | [BOLD] Baseline | [BOLD] Baseline | [BOLD] Baseline ||\n|| [EMPTY] | 17.3 | 18.8 | 20.8 | 26.9 | 16.8 | 20.1 | 16.5 | 10.5 ||\n|| [EMPTY] | [BOLD] Multilingual | [BOLD] Multilingual | [BOLD] Multilingual | [BOLD] Multilingual | [BOLD] Multilingual | [BOLD] Multilingual | [BOLD] Multilingual | [BOLD] Multilingual ||\n|| 6 | 17.3 | 18.4 | 20.0 | 25.4 | 16.9 | 21.8 | - | - ||\n|| 8 | 16.5 | 17.8 | 18.9 | 24.5 | 16.2 | 20.8 | 15.9 | 9.8 ||\n|| [EMPTY] | [BOLD] + ASR | [BOLD] + ASR | [BOLD] + ASR | [BOLD] + ASR | [BOLD] + ASR | [BOLD] + ASR | [BOLD] + ASR | [BOLD] + ASR ||\n|| 6 | 17.4 | 19.2 | 19.7 | 26.0 | 17.2 | 21.8 | - | - ||\n|| 8 | 15.9 | 17.2 | 18.3 | 23.7 | 15.1 | 19.9 | 15.5 | 9.7 ||",
    "claim": "Adding ASR data to the 6-language system improves performance in Spanish and French, while using all 8 languages with ASR data results in worse performance.",
    "label": "refute"
  },
  {
    "id": "training_797_refute",
    "table_caption": "Table: Results of the synchronic evaluation in Pearson’s r averaged over all three VAD dimensions. The best system for each seed lexicon and those with statistically non-significant differences (p ≥ 0.05) are in bold.",
    "table_content": "|| Induction Method | Seed Selection | SVDPPMI | SGNS ||\n|| kNN | full | [BOLD] 0.548 | 0.487 ||\n|| ParaSimNum | full | [BOLD] 0.557 | 0.489 ||\n|| RandomWalkNum | full | [BOLD] 0.544 | 0.436 ||\n|| kNN | limited | 0.181 | 0.166 ||\n|| ParaSimNum | limited | 0.249 | 0.191 ||\n|| RandomWalkNum | limited | [BOLD] 0.330 | 0.181 ||",
    "claim": "SGNS embeddings perform better than SVDPPMI embeddings for the full seed lexicon, and results with the limited seed set are worse than those with the full seed lexicon.",
    "label": "refute"
  },
  {
    "id": "training_874_refute",
    "table_caption": "Table: Tabelle 4: Comparison of context word filtering methods.",
    "table_content": "|| Context sentence | [BOLD] en-it Bleu [%] | [BOLD] en-it Ter [%] | [BOLD] en-de Bleu [%] | [BOLD] en-de Ter [%] | #tokens ||\n|| None | 31.4 | 56.1 | 28.9 | 61.8 | - ||\n|| Full sentence | 32.5 | 54.8 | 30.3 | 59.9 | 100% ||\n|| Remove stopwords | 32.2 | 55.2 | 30.3 | 59.9 | 63% ||\n|| Remove most frequent words | 32.1 | 55.6 | 30.2 | 60.2 | 51% ||\n|| Retain only named entities | 32.3 | 55.4 | 30.3 | 60.3 | [BOLD] 13% ||\n|| Retain specific POS | [BOLD] 32.5 | [BOLD] 55.2 | [BOLD] 30.4 | [BOLD] 60.0 | 59% ||",
    "claim": "Retaining only named entities achieves better performance levels with only 13% of tokens compared to full context sentences.",
    "label": "refute"
  },
  {
    "id": "training_790_refute",
    "table_caption": "Table: Classification accuracies and F1-Scores for news arcticle classifications for different source and target domains. The first row corresponds to the baseline performance trained on only the target dataset. The following two rows shows the performance of instance-infusion method with and without the usage of penalty function. In all the three cases, our approach outperforms the baselines by a significant margin.",
    "table_content": "|| [BOLD] METHOD | Target Source | News20 BBC | News20 BBC | BBC News20 | BBC News20 | BBC Sports BBC | BBC Sports BBC ||\n|| [BOLD] METHOD | [EMPTY] | Accuracy | F1-Score | Accuracy | F1-Score | Accuracy | F1-Score ||\n|| Bi-LSTM (Target Only) | [EMPTY] | 65.17 | 0.6328 | 91.33 | 0.9122 | 84.22 | 0.8395 ||\n|| Instance-Infused Bi-LSTM | [EMPTY] | 76.44 | 0.7586 | 95.35 | 0.9531 | 88.78 | 0.8855 ||\n|| Instance-Infused Bi-LSTM (with penalty function) | [EMPTY] | [BOLD] 78.29 | [BOLD] 0.7773 | [BOLD] 96.09 | [BOLD] 0.9619 | [BOLD] 91.56 | [BOLD] 0.9100 ||",
    "claim": "The Instance-Infused Bi-LSTM (with penalty function) achieves an improvement of 5% in accuracy over the baseline model on the News20 BBC dataset and around 12% on the BBC News20 and BBC Sports BBC datasets.",
    "label": "refute"
  },
  {
    "id": "training_734_refute",
    "table_caption": "Table: BLEU scores of the individual and multilingual models on the 44 languages→English on the Ted talk dataset.",
    "table_content": "|| Language | Ar | Bg | Cs | Da | De | El | Es | Et | Fa ||\n|| [ITALIC] Individual | 31.07 | 38.64 | 26.42 | 38.21 | 34.63 | 36.69 | 41.20 | 7.43 | 26.67 ||\n|| [ITALIC] Multilingual (Baseline) | 27.84 | 27.76 | 27.17 | 40.41 | 32.85 | 36.04 | 39.80 | 14.86 | 24.93 ||\n|| [ITALIC] Multilingual (Our method) | 29.57 | 29.18 | 28.30 | 42.23 | 34.53 | 37.49 | 41.43 | 15.63 | 26.76 ||\n|| Language | Fi | Frca | Fr | Gl | He | Hi | Hr | Hu | Hy ||\n|| [ITALIC] Individual | 10.78 | 18.52 | 39.62 | 12.64 | 36.81 | 10.84 | 34.14 | 24.67 | 12.30 ||\n|| [ITALIC] Multilingual (Baseline) | 16.12 | 33.08 | 38.27 | 30.32 | 32.96 | 19.93 | 34.39 | 22.76 | 20.25 ||\n|| [ITALIC] Multilingual (Our method) | 17.22 | 34.32 | 39.75 | 31.9 | 35.22 | 21.00 | 35.6 | 24.56 | 21.17 ||\n|| Language | Id | It | Ja | Ka | Ko | Ku | Lt | Mk | My ||\n|| [ITALIC] Individual | 29.20 | 38.06 | 13.31 | 7.06 | 18.54 | 5.63 | 18.19 | 21.93 | 7.53 ||\n|| [ITALIC] Multilingual (Baseline) | 29.08 | 36.02 | 12.33 | 16.71 | 16.71 | 11.83 | 20.96 | 31.85 | 13.85 ||\n|| [ITALIC] Multilingual (Our method) | 30.56 | 37.50 | 13.28 | 18.26 | 18.14 | 13.38 | 22.65 | 32.65 | 15.16 ||\n|| Language | Nb | Nl | Pl | Ptbr | Pt | Ro | Ru | Sk | Sl ||\n|| [ITALIC] Individual | 27.28 | 35.85 | 22.98 | 44.28 | 33.81 | 34.07 | 24.36 | 25.67 | 11.80 ||\n|| [ITALIC] Multilingual (Baseline) | 39.88 | 33.97 | 23.50 | 42.96 | 40.59 | 33.03 | 24.02 | 28.97 | 22.52 ||\n|| [ITALIC] Multilingual (Our method) | 41.35 | 35.65 | 24.30 | 44.41 | 42.57 | 34.73 | 25.01 | 29.90 | 23.67 ||\n|| Language | Sq | Sr | Sv | Th | Tr | Uk | Vi | Zh | [EMPTY] ||\n|| [ITALIC] Individual | 29.70 | 32.13 | 34.53 | 20.95 | 24.46 | 25.76 | 26.38 | 12.56 | [EMPTY] ||\n|| [ITALIC] Multilingual (Baseline) | 33.05 | 32.27 | 35.92 | 21.50 | 21.79 | 26.82 | 25.76 | 18.81 | [EMPTY] ||\n|| [ITALIC] Multilingual (Our method) | 34.73 | 33.71 | 36.92 | 22.12 | 23.67 | 27.80 | 26.53 | 19.39 | [EMPTY] ||",
    "claim": "The multilingual model using the proposed method consistently underperforms compared to the individual model across various languages.",
    "label": "refute"
  },
  {
    "id": "training_858_refute",
    "table_caption": "Table: ROUGE F1 and METEOR scores of sanity check ablations, evaluated on CNN/DM validation set.",
    "table_content": "|| [EMPTY] | ROUGE 1 | ROUGE 2 | ROUGE L ||\n|| pg baseline | 37.73 | 16.52 | 34.49 ||\n|| pg + ptrdec | 37.66 | 16.50 | 34.47 ||\n|| pg-2layer | 37.92 | 16.48 | 34.62 ||\n|| pg-big | 38.03 | 16.71 | 34.84 ||\n|| pg + cbdec | [BOLD] 38.87 | [BOLD] 16.93 | [BOLD] 35.38 ||",
    "claim": "The \"pg-big\" model achieves the highest scores across all ROUGE metrics.",
    "label": "refute"
  },
  {
    "id": "training_770_refute",
    "table_caption": "Table: Effect of dimensionlity: additional results for the test performance of GLOSS-BoW and GLOSS-Pos models with different dimensionality of the latent vectors on unsupervised STS-(12-16, B) and supervised tasks. (∗) Following SentEval, STS-13 does not include the SMT dataset due to licensing issues. Best results for each task are in bold.",
    "table_content": "|| Model | Config. #Tok | Config. Dim | Unsupervised STS-() tasks 12 | Unsupervised STS-() tasks 13* | Unsupervised STS-() tasks 14 | Unsupervised STS-() tasks 15 | Unsupervised STS-() tasks 16 | Unsupervised STS-() tasks B | Unsupervised STS-() tasks Avg | Supervised tasks MR | Supervised tasks CR | Supervised tasks SUBJ | Supervised tasks MPQA | Supervised tasks TREC | Supervised tasks Avg ||\n|| GLOSS-BoW | 27M | 100 | 54.8 | 51.8 | 68.4 | 71.2 | [BOLD] 71.8 | [BOLD] 72.4 | 65.1 | 67.4 | 72.0 | 86.4 | 79.5 | 71.0 | 75.3 ||\n|| GLOSS-BoW | 27M | 300 | [BOLD] 55.9 | 55.6 | [BOLD] 69.2 | 73.4 | 71.2 | 72.1 | [BOLD] 66.2 | 69.5 | 74.7 | 88.6 | 82.3 | 78.0 | 78.6 ||\n|| GLOSS-BoW | 27M | 700 | 54.9 | [BOLD] 55.8 | 68.8 | [BOLD] 73.7 | 71.0 | 71.4 | 65.9 | 72.4 | 76.7 | 90.2 | 83.7 | [BOLD] 82.2 | 81.0 ||\n|| GLOSS-POS | 27M | 100 | 54.6 | 54.8 | 68.3 | 71.7 | 71.4 | 69.7 | 65.1 | 68.8 | 73.9 | 87.0 | 83.3 | 74.8 | 77.6 ||\n|| GLOSS-POS | 27M | 300 | 54.2 | 52.7 | 68.1 | 73.4 | 70.5 | 69.0 | 64.7 | 71.8 | 75.5 | 89.3 | 84.7 | 80.2 | 80.3 ||\n|| GLOSS-POS | 27M | 700 | 53.6 | 53.3 | 67.8 | 73.3 | 70.1 | 68.1 | 64.4 | 72.7 | 77.4 | 89.9 | 85.4 | 81.4 | 81.4 ||\n|| GLOSS-POS | 27M | 1K | 53.0 | 52.9 | 67.5 | 72.0 | 69.8 | 68.0 | 63.9 | [BOLD] 73.4 | [BOLD] 78.1 | [BOLD] 91.0 | [BOLD] 86.3 | 82.1 | [BOLD] 82.2 ||",
    "claim": "GLOSS-BoW is better than GLOSS-POS for unsupervised tasks, and increasing dimensionality improves accuracy on unsupervised tasks.",
    "label": "refute"
  },
  {
    "id": "training_869_refute",
    "table_caption": "Table: Human evaluation of the data collected with each MR (** = p<0.01 and *** = p<0.001 for Pictorial versus Textual conditions). Italics denote averages across all numbers of attributes.",
    "table_content": "|| [EMPTY] | [BOLD] Textual MR Mean | [BOLD] Textual MR StDev | [BOLD] Pictorial MR Mean | [BOLD] Pictorial MR StDev ||\n|| [ITALIC] Informativeness | [ITALIC] 4.28** | [ITALIC] 1.54 | [ITALIC] 4.51** | [ITALIC] 1.37 ||\n|| 3 attributes | 4.02 | 1.39 | 4.11 | 1.32 ||\n|| 5 attributes | 4.31 | 1.54 | 4.46 | 1.36 ||\n|| 8 attributes | 4.52 | 1.65 | 4.98 | 1.29 ||\n|| [ITALIC] Naturalness | [ITALIC] 4.09*** | [ITALIC] 1.56 | [ITALIC] 4.43*** | [ITALIC] 1.35 ||\n|| 3 attributes | 4.13 | 1.47 | 4.35 | 1.29 ||\n|| 5 attributes | 4.07 | 1.56 | 4.41 | 1.36 ||\n|| 8 attributes | 4.07 | 1.65 | 4.55 | 1.42 ||\n|| [ITALIC] Phrasing | [ITALIC] 4.01*** | [ITALIC] 1.69 | [ITALIC] 4.40*** | [ITALIC] 1.52 ||\n|| 3 attributes | 4.01 | 1.62 | 4.37 | 1.47 ||\n|| 5 attributes | 4.04 | 1.70 | 4.28 | 1.57 ||\n|| 8 attributes | 3.98 | 1.75 | 4.53 | 1.54 ||",
    "claim": "The average Informativeness, Naturalness, and Phrasing scores for utterances elicited using the textual/logical modality are higher than those for utterances elicited using the pictorial modality.",
    "label": "refute"
  },
  {
    "id": "training_762_refute",
    "table_caption": "Table: Experiment results of our model in comparison with several baselines, including increments of the number of parameters over Transformer baseline (\\Delta|{\\bm{\\theta}}|), training/testing speeds (v_{\\rm{train}}/v_{\\rm{test}}, some of them are derived from maruf2019selective [maruf2019selective]), and translation results of the testsets in BLEU score.",
    "table_content": "|| [BOLD] Model | \\Delta|{\\bm{\\theta}}| | v_{\\rm{train}} | v_{\\rm{test}} | Zh-En TED | En-De TED | En-De News | En-De Europarl | En-De avg. ||\n|| SentNmt [Vaswani2017Attention] | 0.0m | 1.0\\times | 1.0\\times | 17.0 | 23.10 | 22.40 | 29.40 | 24.96 ||\n|| DocT [zhang2018improving] | 9.5m | 0.65\\times | 0.98\\times | [EMPTY] | 24.00 | 23.08 | 29.32 | 25.46 ||\n|| HAN [miculicich2018document] | 4.8m | 0.32\\times | 0.89\\times | 17.9 | 24.58 | [BOLD] 25.03 | 28.60 | 26.07 ||\n|| SAN [maruf2019selective] | 4.2m | 0.51\\times | 0.86\\times | [EMPTY] | 24.42 | 24.84 | 29.75 | 26.33 ||\n|| QCN [yang2019enhancing] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [BOLD] 25.19 | 22.37 | 29.82 | 25.79 ||\n|| Final | 4.7m | 0.22\\times | 1.08\\times | [BOLD] 19.1 | 25.10 | 24.91 | [BOLD] 30.40 | [BOLD] 26.80 ||",
    "claim": "The \"Final\" model achieves the highest scores on TED Zh-En and Europarl tasks but has the lowest average performance on English-German benchmarks.",
    "label": "refute"
  },
  {
    "id": "training_835_refute",
    "table_caption": "Table: Training speed and memory consumption on WMT 2017 German→English. Train time is for seeing the full train dataset once. Batch size is in words, such that it almost maximizes the GPU memory consumption. The BLEU score is for the converged models, reported for newstest2015 (dev) and newstest2017. The encoder has one bidirectional LSTM layer and either 3 or 5 unidirectional LSTM layers.",
    "table_content": "|| toolkit | encoder n. layers | time [h] | batch size | BLEU [%] 2015 | BLEU [%] 2017 ||\n|| RETURNN | 4 | [BOLD] 11.25 | 8500 | 28.0 | 28.4 ||\n|| Sockeye | [EMPTY] | 11.45 | 3000 | [BOLD] 28.9 | [BOLD] 29.2 ||\n|| RETURNN | 6 | [BOLD] 12.87 | 7500 | 28.7 | 28.7 ||\n|| Sockeye | [EMPTY] | 14.76 | 2500 | [BOLD] 29.4 | [BOLD] 29.1 ||",
    "claim": "Sockeye achieves higher BLEU scores than RETURNN for both 2015 and 2017, and Sockeye has shorter training times.",
    "label": "refute"
  },
  {
    "id": "training_850_refute",
    "table_caption": "Table: Comparison between machine translation and human interpretation. The interpretation reference consists of a collection of interpretations from S, A and B. Our model is trained on the large-scale corpus.",
    "table_content": "|| Models | Translation Reference BLEU | Translation Reference Brevity Penalty | Interpretation Reference (3-references) BLEU | Interpretation Reference (3-references) Brevity Penalty ||\n|| [ITALIC] Our Model | 20.93 | 1.000 | 28.08 | 1.000 ||\n|| [ITALIC] S | 16.02 | 0.845 | - | - ||\n|| [ITALIC] A | 16.38 | 0.887 | - | - ||\n|| [ITALIC] B | 12.08 | 0.893 | - | - ||",
    "claim": "Model S achieves the highest Translation Reference BLEU score and Interpretation Reference BLEU score compared to models Our Model, A, and B.",
    "label": "refute"
  },
  {
    "id": "training_819_refute",
    "table_caption": "Table: Results with concat (C-*) and merge (M-*) target forcing on 6 languages. The baselines are one-to-one systems. All the other results are computed with one multilingual system for En→De,NL and one for En→Es,Fr,It,Pt.",
    "table_content": "|| [EMPTY] | De | Nl | Es | Fr | It | Pt ||\n|| Baseline | 17.3 | 18.8 | 20.8 | [BOLD] 26.9 | 16.8 | 20.1 ||\n|| C-Pre | 14.0 | 11.6 | 13.0 | 16.3 | 10.7 | 14.5 ||\n|| C-Post | 12.0 | 13.8 | 12.3 | 18.0 | 9.3 | 14.6 ||\n|| C-Final | 14.5 | 12.1 | 13.6 | 16.7 | 10.2 | 16.2 ||\n|| M-Pre | 17.6 | 19.5 | 20.5 | 26.2 | 17.2 | 22.3 ||\n|| M-Post | 17.1 | 19.2 | 20.5 | 26.2 | 17.4 | 22.3 ||\n|| M-Final | 17.4 | 18.8 | 20.4 | 26.7 | 17.2 | 22.2 ||\n|| M-Dec. | 17.3 | 19.1 | 20.6 | 26.2 | 17.2 | 22.0 ||\n|| M-Pre + ASR | [BOLD] 17.7 | [BOLD] 20.0 | [BOLD] 20.9 | 26.5 | [BOLD] 18.0 | [BOLD] 22.6 ||",
    "claim": "M-Pre shows high results consistently across all languages, with the largest improvement in En-Fr and a notable gain in En-Nl.",
    "label": "refute"
  },
  {
    "id": "training_812_refute",
    "table_caption": "Table: Sentiment attitudes Granger-causality on the FT I dataset.",
    "table_content": "|| Stock | Model | Lag | Attitude | Price⇒ ||\n|| Stock | Model | Lag | ⇒Price | Attitude ||\n|| S&P 500 | Standard | 1 | 0.1929 | 0.1105 ||\n|| S&P 500 | Standard | 2 | 0.2611 | [BOLD] 0.0780 ||\n|| S&P 500 | Temporal | 1 | 0.2689 | [BOLD] 0.0495 ||\n|| S&P 500 | Temporal | 2 | 0.1692 | [BOLD] 0.0940 ||\n|| APPL | Standard | 1 | 0.7351 | 0.4253 ||\n|| APPL | Standard | 2 | 0.9117 | 0.6426 ||\n|| APPL | Temporal | 1 | 0.9478 | 0.6725 ||\n|| APPL | Temporal | 2 | 0.9715 | 0.8245 ||\n|| GOOGL | Standard | 1 | 0.5285 | 0.4035 ||\n|| GOOGL | Standard | 2 | 0.8075 | [BOLD] 0.0418 ||\n|| GOOGL | Temporal | 1 | 0.6920 | 0.5388 ||\n|| GOOGL | Temporal | 2 | 0.8516 | [BOLD] 0.0422 ||\n|| HPQ | Standard | 1 | 0.1534 | 0.3996 ||\n|| HPQ | Standard | 2 | 0.1877 | 0.5322 ||\n|| HPQ | Temporal | 1 | 0.4069 | [BOLD] 0.0836 ||\n|| HPQ | Temporal | 2 | 0.5097 | 0.1180 ||\n|| JPM | Standard | 1 | 0.8991 | [BOLD] 0.0461 ||\n|| JPM | Standard | 2 | 0.9963 | [BOLD] 0.0435 ||\n|| JPM | Temporal | 1 | 0.9437 | 0.1204 ||\n|| JPM | Temporal | 2 | 0.7722 | 0.2720 ||",
    "claim": "Stock price changes Granger-cause sentiment attitudes, with the strongest causality found using the temporal sentiment analysis model. For Google, causality is detected with a two-day lag, and for HP, with a two-day lag using temporal sentiment.",
    "label": "refute"
  },
  {
    "id": "training_879_refute",
    "table_caption": "Table: Performance on each class of UMD suicidality dataset",
    "table_content": "|| Label | Metrics | BiLSTM | SSA | RN ||\n|| -1 | Precision | 0.62 | 0.57 | 0.69 ||\n|| -1 | Recall | 0.77 | [BOLD] 0.92 | 0.70 ||\n|| -1 | F1-score | 0.69 | 0.70 | 0.69 ||\n|| [EMPTY] | Precision | 0.51 | 0.57 | 0.48 ||\n|| 1 | Recall | 0.55 | 0.31 | [BOLD] 0.62 ||\n|| [EMPTY] | F1-score | 0.53 | 0.41 | 0.54 ||\n|| [EMPTY] | Precision | 0.15 | 0.00 | 0.24 ||\n|| 0 | Recall | 0.02 | 0.00 | 0.09 ||\n|| [EMPTY] | F1-score | 0.04 | 0.00 | 0.13 ||",
    "claim": "The SSA model has the highest recall for posts with high suicide risk (label 1), while all models perform poorly on posts with low suicide risk (label 0).",
    "label": "refute"
  },
  {
    "id": "training_848_refute",
    "table_caption": "Table: Test results when using more training data. More data helps both Avg and GRAN, where both models get close to or surpass training on SimpWiki and comfortably surpass the PPDB baseline. The amount of training examples used is in parentheses.",
    "table_content": "|| Data | GRAN | Avg ||\n|| PPDB | 64.6 | 66.3 ||\n|| SimpWiki (100k/168k) | 67.4 | [BOLD] 67.7 ||\n|| CC-CS (24k) | 66.8 | - ||\n|| CC-DE (24k) | - | 66.6 ||\n|| CC-CS (100k) | [BOLD] 68.5 | - ||\n|| CC-DE (168k) | - | 67.6 ||",
    "claim": "PPDB outperforms both SimpWiki and CC-DE data sources.",
    "label": "refute"
  },
  {
    "id": "training_821_refute",
    "table_caption": "Table: Percentage of sentences in the correct language computed with langdetect.",
    "table_content": "|| [EMPTY] | De | Nl | Es | Fr | It | Pt ||\n|| M-Pre | 95.7 | 98.5 | 97.2 | 94.6 | 95.3 | 96.6 ||\n|| M-Pre + ASR | 96.1 | 98.7 | 97.9 | 95.3 | 95.4 | 95.2 ||",
    "claim": "The percentage of correct language identification increases slightly in all languages except for Italian when using ASR data.",
    "label": "refute"
  },
  {
    "id": "training_810_refute",
    "table_caption": "Table: Results on relational triple elements.",
    "table_content": "|| Element | NYT  [ITALIC] Prec. | NYT  [ITALIC] Rec. | NYT  [ITALIC] F1 | WebNLG  [ITALIC] Prec. | WebNLG  [ITALIC] Rec. | WebNLG  [ITALIC] F1 ||\n|| [ITALIC] E1 | 94.6 | 92.4 | 93.5 | 98.7 | 92.8 | 95.7 ||\n|| [ITALIC] E2 | 94.1 | 93.0 | 93.5 | 97.7 | 93.0 | 95.3 ||\n|| [ITALIC] R | 96.0 | 93.8 | 94.9 | 96.6 | 91.5 | 94.0 ||\n|| [ITALIC] (E1, R) | 93.6 | 90.9 | 92.2 | 94.8 | 90.3 | 92.5 ||\n|| [ITALIC] (R, E2) | 93.1 | 91.3 | 92.2 | 95.4 | 91.1 | 93.2 ||\n|| [ITALIC] (E1, E2) | 89.2 | 90.1 | 89.7 | 95.3 | 91.7 | 93.5 ||\n|| [ITALIC] (E1, R, E2) | 89.7 | 89.5 | 89.6 | 93.4 | 90.1 | 91.8 ||",
    "claim": "For NYT, the F1-scores for E1 and E2 are consistent with those for (E1, R) and (R, E2), and there is a larger gap between the F1-scores of (E1, E2) and (E1, R, E2), but a smaller gap between (E1, R, E2) and (E1, R)/(R, E2).",
    "label": "refute"
  },
  {
    "id": "training_889_refute",
    "table_caption": "Table: BLEU score vs context length on different models",
    "table_content": "|| Context length | 0 | 32 | 64 | 128 ||\n|| Transformer | 18.85 | 18.66 | 17.59 | 15.55 ||\n|| BPT (k=4) | 19.19 | 19.84 | 19.71 | 19.84 ||\n|| BPT (k=8) | 19.13 | 19.59 | 19.78 | 19.60 ||",
    "claim": "The performance of BPT models decreases across different context lengths, while the performance of the Transformer model decreases as context length increases.",
    "label": "refute"
  },
  {
    "id": "training_861_refute",
    "table_caption": "Table: ROUGE F1 scores of ablation studies, evaluated on CNN/Daily Mail validation set.",
    "table_content": "|| [EMPTY] | ROUGE 1 | ROUGE 2 | ROUGE L ||\n|| Fixed-encoder ablation | Fixed-encoder ablation | Fixed-encoder ablation | Fixed-encoder ablation ||\n|| pg baseline’s encoder | 37.59 | 16.27 | 34.33 ||\n|| 2-decoder’s encoder | [BOLD] 38.44 | [BOLD] 16.85 | [BOLD] 35.17 ||\n|| Gradient-Flow-Cut ablation | Gradient-Flow-Cut ablation | Gradient-Flow-Cut ablation | Gradient-Flow-Cut ablation ||\n|| pg baseline | 37.73 | 16.52 | 34.49 ||\n|| stop \\⃝raisebox{-0.9pt}{1} | 37.72 | 16.58 | 34.54 ||\n|| stop \\⃝raisebox{-0.9pt}{2} | [BOLD] 38.35 | [BOLD] 16.79 | [BOLD] 35.13 ||",
    "claim": "The pg baseline's encoder achieves higher ROUGE scores than the 2-decoder's encoder in the Fixed-Encoder Ablation study, and stopping the gradient flow between the encoder and closed-book decoder results in slightly lower ROUGE scores compared to when the flow is not stopped.",
    "label": "refute"
  },
  {
    "id": "training_724_refute",
    "table_caption": "Table: Figure 1: Left: Encounter-level F1-scores of the 20 most frequent CPT codes. #Docs is the average number of documents found in the encounters that contain the code; prevalence is the percentage of all encounters that contain that code. Right: Macro average of encounter-level F1 scores for every 10 codes (from most to least frequent). ΔELDAN =\\textscELDAN+transfer−\\textscELDAN.",
    "table_content": "|| Average | Prevalence | ELDN | ELDAN | ELDAN +transfer | ΔELDAN ||\n|| 1st to 10th | 1.97% | 65.22 | 66.93 | [BOLD] 67.14 | 0.22 ||\n|| 11st to 20th | 0.78% | 50.82 | 53.87 | [BOLD] 55.38 | 1.50 ||\n|| 21st to 30th | 0.51% | 55.93 | [BOLD] 63.07 | 62.23 | -0.85 ||\n|| 31st to 40th | 0.40% | 44.93 | 51.92 | [BOLD] 55.24 | 3.32 ||\n|| 41st to 50th | 0.30% | 32.08 | 38.61 | [BOLD] 39.35 | 0.74 ||\n|| 51st to 60th | 0.26% | 33.83 | 38.80 | [BOLD] 39.10 | 0.30 ||\n|| 61st to 70th | 0.23% | 28.37 | 35.05 | [BOLD] 36.62 | 1.56 ||\n|| 71st to 80th | 0.21% | 25.66 | 30.62 | [BOLD] 32.93 | 2.31 ||\n|| 81st to 90th | 0.18% | 34.92 | 42.03 | [BOLD] 43.26 | 1.23 ||\n|| 91st to 100th | 0.16% | 24.54 | 29.06 | [BOLD] 31.32 | 2.25 ||\n|| 101st to 110th | 0.14% | 25.15 | 33.17 | [BOLD] 34.57 | 1.40 ||\n|| 111st to 120th | 0.12% | 24.87 | 31.74 | [BOLD] 32.84 | 1.09 ||\n|| 121st to 130th | 0.11% | 18.14 | 24.10 | [BOLD] 28.09 | 3.99 ||\n|| 131st to 140th | 0.10% | 20.39 | 28.53 | [BOLD] 32.21 | 3.68 ||\n|| 141st to 150th | 0.08% | 26.93 | 33.13 | [BOLD] 40.94 | 7.82 ||",
    "claim": "ELDAN+transfer consistently outperforms ELDN, and the performance improvement (ΔELDAN) decreases as codes become rarer.",
    "label": "refute"
  },
  {
    "id": "training_776_refute",
    "table_caption": "Table: WER results",
    "table_content": "|| [EMPTY] | 1st decoding with LM0 | Rescoring with LM2 | Rescoring with LM1 | Proposed method ||\n|| Test-clean | 18.8 | - | 11.50 | 11.50 ||\n|| Test-other | 33.35 | - | 22.59 | 22.59 ||\n|| Reading-test | 3.84 | 2.42 | - | 2.42 ||\n|| Conversation-test | 20.54 | 19.00 | - | 19.2 ||\n|| YouTube-test | 24.55 | 21.57 | - | 21.83 ||\n|| VLSP2018 | 10.12 | 8.16 | - | 8.16 ||",
    "claim": "The proposed method achieves a WER of 11.50 on Test-clean, 22.59 on Test-other, 2.42 on Reading-test, 19.2 on Conversation-test, 22.00 on YouTube-test, and 8.16 on VLSP2018.",
    "label": "refute"
  },
  {
    "id": "training_791_refute",
    "table_caption": "Table: Test Accuracy for proposed model using instances from multiple source datasets with 50% target dataset",
    "table_content": "|| Dataset | Single Source Accuracy | Single Source F1-Score | Multiple Sources Accuracy | Multiple Sources F1-Score ||\n|| News20 | 61.72 | 0.6133 | 67.32 | 0.6650 ||\n|| BBC | 91.01 | 0.9108 | 91.41 | 0.9120 ||\n|| BBC Sports | 81.72 | 0.7990 | 82.81 | 0.8027 ||",
    "claim": "Multiple sources improve both accuracy and F1-score compared to a single source for News20 and BBC datasets, but not for BBC Sports.",
    "label": "refute"
  },
  {
    "id": "training_856_refute",
    "table_caption": "Table: Distillation-augmented performances for selected high sparsity levels. All pruning methods benefit from distillation signal further enhancing the ratio Performance VS Model Size.",
    "table_content": "|| [EMPTY] | BERT base fine-tuned | Remaining Weights (%) | MaP | [ITALIC] L0 Regu | MvP | soft MvP ||\n|| SQuAD - Dev EM/F1 | 80.4/88.1 | 10% | 70.2/80.1 | 72.4/81.9 | 75.6/84.3 | [BOLD] 76.6/ [BOLD] 84.9 ||\n|| SQuAD - Dev EM/F1 | 80.4/88.1 | 3% | 45.5/59.6 | 65.5/75.9 | 67.5/78.0 | [BOLD] 72.9/ [BOLD] 82.4 ||\n|| MNLI - Dev acc/MM acc | 84.5/84.9 | 10% | 78.3/79.3 | 78.7/79.8 | 80.1/80.4 | [BOLD] 81.2/ [BOLD] 81.8 ||\n|| MNLI - Dev acc/MM acc | 84.5/84.9 | 3% | 69.4/70.6 | 76.2/76.5 | 76.5/77.4 | [BOLD] 79.6/ [BOLD] 80.2 ||\n|| QQP - Dev acc/F1 | 91.4/88.4 | 10% | 79.8/65.0 | 88.1/82.8 | 89.7/86.2 | [BOLD] 90.5/ [BOLD] 87.1 ||\n|| QQP - Dev acc/F1 | 91.4/88.4 | 3% | 72.4/57.8 | 87.1/82.0 | 86.1/81.5 | [BOLD] 89.3/ [BOLD] 85.6 ||",
    "claim": "MaP yields the strongest performances across all pruning methods and studied datasets.",
    "label": "refute"
  },
  {
    "id": "training_799_refute",
    "table_caption": "Table: Aspect and opinion term extraction performance.",
    "table_content": "|| Approach | SE14-R Aspect | SE14-R Opinion | SE14-L Aspect | SE14-L Opinion | SE15-R Aspect | SE15-R Opinion ||\n|| BiLSTM-CRF + word2vec | 84.06 | 84.59 | 73.47 | 75.41 | 66.17 | 68.16 ||\n|| BERT fine-tuning | 84.36 | 85.50 | 75.67 | 79.75 | 65.84 | 74.21 ||\n|| BERT feature-based | 85.14 | 85.74 | 76.81 | 81.41 | 66.84 | 73.92 ||\n|| RINANTE+BERT | [BOLD] 85.51 | [BOLD] 86.82 | [BOLD] 79.93 | [BOLD] 82.09 | [BOLD] 68.50 | [BOLD] 74.54 ||",
    "claim": "BERT feature-based achieves the highest performance across all tasks compared to other approaches.",
    "label": "refute"
  },
  {
    "id": "training_772_refute",
    "table_caption": "Table: Accuracy results for temporal adaptation in the PPCMBE and the PPCEME of historical English. Percentage error reduction is shown for the best-performing method, Fema-attribute.",
    "table_content": "|| Task | baseline SVM | baseline MEMM (Stanford) | SCL | Brown | word2vec | Fema single embedding | Fema attribute embeddings (error reduction) ||\n|| [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [ITALIC] Modern British English (training from 1840-1914) | [EMPTY] ||\n|| → 1770-1839 | 96.30 | 96.57 | 96.42 | 96.45 | 96.44 | 96.80 | [BOLD] 96.84 (15%) ||\n|| → 1700-1769 | 94.57 | 94.83 | 95.07 | 95.15 | 94.85 | 95.65 | [BOLD] 95.75 (22%) ||\n|| average | 95.43 | 95.70 | 95.74 | 95.80 | 95.64 | 96.23 | [BOLD] 96.30 (19%) ||\n|| [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [ITALIC] Early Modern English (training from 1640-1710) | [EMPTY] ||\n|| → 1570-1639 | 93.62 | 93.98 | 94.23 | 94.36 | 94.18 | 95.01 | [BOLD] 95.20 (25%) ||\n|| → 1500-1569 | 87.59 | 87.47 | 89.39 | 89.73 | 89.30 | 91.40 | [BOLD] 91.63 (33%) ||\n|| average | 90.61 | 90.73 | 91.81 | 92.05 | 91.74 | 93.20 | [BOLD] 93.41 (30%) ||",
    "claim": "Fema attribute embeddings achieve the highest accuracy across most tasks, but baseline MEMM (Stanford) outperforms it in one task.",
    "label": "refute"
  },
  {
    "id": "training_884_refute",
    "table_caption": "Table: Perplexity score (PPL) comparison on the PTB dataset and the AWD model. We use two different values for the batch size K --- the original one with K=20, and a prime one with K=19. The results directly corroborate the observation portrayed in Figure 4, where the obtained score is related to the diversity of grayscale values in each row.",
    "table_content": "|| Experiment | K=20 | K=19 ||\n|| Alleviated TOI 2 | 59.37 | 57.97 ||\n|| Alleviated TOI 5 | 60.50 | 57.14 ||\n|| Alleviated TOI 7 | [BOLD] 56.70 | 57.16 ||\n|| Alleviated TOI 10 | 65.88 | [BOLD] 56.46 ||",
    "claim": "For K=20, the highest score is at P=10, while for K=19, the highest score is at P=7.",
    "label": "refute"
  },
  {
    "id": "training_800_refute",
    "table_caption": "Table: Aspect and opinion term extraction performance of different approaches. F1 score is reported. IHS_RD, DLIREC, Elixa and WDEmb* use manually designed features. For different versions of RINANTE, “Shared” and “Double” means shared BiLSTM model and double BiLSTM model, respectively; “Alt” and “Pre” means the first and the second training method, respectively. RINANTE-Double-Pre†: fine-tune the pre-trained model for only extracting aspect terms, and use the same number of validation samples as DE-CNN Xu et al. (2018). The results of RINANTE-Double-Pre† are obtained after this paper gets accepted and are not included in the ACL version. RINANTE-Double-Pre† achieves the best performance on SE15-R.",
    "table_content": "|| Approach | SE14-R Aspect | SE14-R Opinion | SE14-L Aspect | SE14-L Opinion | SE15-R Aspect | SE15-R Opinion ||\n|| DP Qiu et al. ( 2011 ) | 38.72 | 65.94 | 19.19 | 55.29 | 27.32 | 46.31 ||\n|| IHS_RD Chernyshevich ( 2014 ) | 79.62 | - | 74.55 | - | - | - ||\n|| DLIREC Toh and Wang ( 2014 ) | 84.01 | - | 73.78 | - | - | - ||\n|| Elixa Vicente et al. ( 2017 ) | - | - | - | - | [BOLD] 70.04 | - ||\n|| WDEmb Yin et al. ( 2016 ) | 84.31 | - | 74.68 | - | 69.12 | - ||\n|| WDEmb* Yin et al. ( 2016 ) | 84.97 | - | 75.16 | - | 69.73 | - ||\n|| RNCRF Wang et al. ( 2016 ) | 82.23 | 83.93 | 75.28 | 77.03 | 65.39 | 63.75 ||\n|| CMLA Wang et al. ( 2017 ) | 82.46 | 84.67 | 73.63 | 79.16 | 68.22 | 70.50 ||\n|| NCRF-AE Zhang et al. ( 2017 ) | 83.28 | 85.23 | 74.32 | 75.44 | 65.33 | 70.16 ||\n|| HAST Li et al. ( 2018 ) | 85.61 | - | 79.52 | - | 69.77 | - ||\n|| DE-CNN Xu et al. ( 2018 ) | 85.20 | - | [BOLD] 81.59 | - | 68.28 | - ||\n|| Mined Rules | 70.82 | 79.60 | 67.67 | 76.10 | 57.67 | 64.29 ||\n|| RINANTE (No Rule) | 84.06 | 84.59 | 73.47 | 75.41 | 66.17 | 68.16 ||\n|| RINANTE-Shared-Alt | [BOLD] 86.76 | 86.05 | 77.92 | 79.20 | 67.47 | 71.41 ||\n|| RINANTE-Shared-Pre | 85.09 | 85.63 | 79.16 | 79.03 | 68.15 | 70.44 ||\n|| RINANTE-Double-Alt | 85.80 | [BOLD] 86.34 | 78.59 | 78.94 | 67.42 | 70.53 ||\n|| RINANTE-Double-Pre | 86.45 | 85.67 | 80.16 | [BOLD] 81.96 | 69.90 | [BOLD] 72.09 ||\n|| RINANTE-Double-Pre† | 86.20 | - | 81.37 | - | 71.89 | - ||",
    "claim": "All four versions of RINANTE achieve worse performances than RINANTE (no rule), especially on SE14-L and SE15-R.",
    "label": "refute"
  },
  {
    "id": "training_872_refute",
    "table_caption": "Table: Comparison of different activation functions with same hyperparameters values",
    "table_content": "|| [BOLD] Activation Function | [BOLD] F-Score (AiMed) | [BOLD] F-Score (BioInfer) ||\n|| Sigmoid | 86.45 | 77.35 ||\n|| ReLU | 85.77 | 76.92 ||\n|| tanh | 84.40 | 75.79 ||",
    "claim": "The ReLU activation function achieves higher F1-scores than sigmoid and tanh on both the AiMed and BioInfer datasets.",
    "label": "refute"
  },
  {
    "id": "training_820_refute",
    "table_caption": "Table: Comparison of the Baseline and the best multilingual system with the single language cascade (BL-Cascade) and the multilingual cascade (M-Cascade)",
    "table_content": "|| [EMPTY] | De | Nl | Es | Fr | It | Pt ||\n|| Baseline | 17.3 | 18.8 | 20.8 | 26.9 | 16.8 | 20.1 ||\n|| M-Pre + ASR | 17.7 | 20.0 | 20.9 | 26.5 | 18.0 | 22.6 ||\n|| BL-Cascade | 18.5 | 22.2 | 22.5 | 27.9 | 18.9 | 21.5 ||\n|| M-Cascade | 18.6 | 22.0 | 22.1 | 27.3 | 18.5 | 22.8 ||",
    "claim": "BL-Cascade outperforms direct SLT baselines across all languages, with differences ranging from −1.0 for French to −3.4 for Dutch. M-Cascade shows higher results than BL-Cascade in 3 out of 4 Romance languages, with an improvement of +1.3 for Portuguese.",
    "label": "refute"
  },
  {
    "id": "training_750_refute",
    "table_caption": "Table: This table shows the performance of the SVM and NB classifier using the sliding window approach with stratified sampling. Function words classifications are marked with “FW”. All others were full text classifications. The plus and minus symbols indicate whether the performance metric was calculated for disruptive (+) or constructive contributions (−).",
    "table_content": "|| [BOLD] Classifier | [BOLD] Recall+ | [BOLD] Recall− | [BOLD] Precision+ | [BOLD] Precision− | [BOLD] F1+ | [BOLD] F1− | [BOLD] Accuracy | [BOLD] AUC ||\n|| SVM | 88.96 | 84.60 | 85.25 | 88.46 | 87.07 | 86.49 | 86.78 | 0.950 ||\n|| SVM (FW) | 78.73 | 50.75 | 61.52 | 70.47 | 69.07 | 59.01 | 64.74 | 0.710 ||\n|| NB | 69.57 | 93.43 | 91.37 | 75.43 | 78.99 | 83.47 | 81.50 | 0.670 ||\n|| NB (FW) | 42.27 | 79.33 | 67.16 | 57.88 | 51.88 | 66.93 | 60.80 | 0.670 ||",
    "claim": "SVM outperforms SVM (FW) across all performance metrics, except for Recall+, where SVM (FW) performs better.",
    "label": "refute"
  },
  {
    "id": "training_864_refute",
    "table_caption": "Table: GEC results with W&I+LOCNESS test data.",
    "table_content": "|| Team | TP | FP | FN | P | R | F0.5 ||\n|| UEDIN-MS | 2,312 | 982 | 2,506 | 70.19 | 47.99 | 64.24 ||\n|| Kakao&Brain | 2,412 | 1,413 | 2,797 | 63.06 | 46.30 | 58.80 ||\n|| LAIX | 1,443 | 884 | 3,175 | 62.01 | 31.25 | 51.81 ||\n|| CAMB-CUED | 1,814 | 1,450 | 2,956 | 55.58 | 38.03 | 50.88 ||\n|| UFAL, Charles University, Prague | 1,245 | 1,222 | 2,993 | 50.47 | 29.38 | 44.13 ||\n|| Siteimprove | 1,299 | 1,619 | 3,199 | 44.52 | 28.88 | 40.17 ||\n|| WebSpellChecker.com | 2,363 | 3,719 | 3,031 | 38.85 | 43.81 | 39.75 ||\n|| TMU | 1,638 | 4,314 | 3,486 | 27.52 | 31.97 | 28.31 ||\n|| Buffalo | 446 | 1,243 | 3,556 | 26.41 | 11.14 | 20.73 ||",
    "claim": "The F0.5 score for TMU is 28.31, ranking eighth among the nine teams, and TMU does not have the highest number of false positives.",
    "label": "refute"
  },
  {
    "id": "training_834_refute",
    "table_caption": "Table: Automatic Evaluation on the QA dataset",
    "table_content": "|| Models | BLEU | Precision | Recall ||\n|| S2SA | 0.05 | 0.08±0.125 | 0.07±0.13 ||\n|| GenQA | 0.12 | 0.06±0.11 | 0.04±0.09 ||\n|| GenQAD | 0.13 | 0.25±0.2 | 0.34±0.235 ||\n|| GenDS-Single | 0.226 | 0.76±0.205 | [BOLD] 0.77±0.21 ||\n|| GenDS-Static | 0.19 | 0.64±0.23 | 0.66±0.235 ||\n|| GenDS | [BOLD] 0.227 | [BOLD] 0.77±0.205 | 0.76±0.215 ||",
    "claim": "GenDS achieves the highest BLEU score, precision, and recall.",
    "label": "refute"
  },
  {
    "id": "training_938_refute",
    "table_caption": "Table: Results on IMDB Movie Review Dataset",
    "table_content": "|| [BOLD] Method | [BOLD] Accuracy ||\n|| Maas et al.(2011) | 88.89 ||\n|| NBSVM-bi (Wang & Manning, 2012) | 91.22 ||\n|| NBSVM-uni (Wang & Manning, 2012) | 88.29 ||\n|| SVM-uni (Wang & Manning, 2012) | 89.16 ||\n|| Paragraph Vector (Le and Mikolov(2014)) | 92.58 ||\n|| Weighted WordVector+Wiki(Our Method) | 88.60 ||\n|| Weighted WordVector+TfIdf(Our Method) | 90.67 ||\n|| Composite Document Vector | [BOLD] 93.91 ||",
    "claim": "Paragraph Vector achieves the highest accuracy of 93.91.",
    "label": "refute"
  },
  {
    "id": "training_785_refute",
    "table_caption": "Table: Improvements of data diversification under conditions maximum likelihood and beam search in the IWSLT’14 English-German and German-English.",
    "table_content": "|| [BOLD] Task | [BOLD] Baseline | [BOLD] Ours  [BOLD] Beam=1 | [BOLD] Ours  [BOLD] Beam=5 ||\n|| En-De | 28.6 | 30.3 | 30.4 ||\n|| De-En | 34.7 | 36.6 | 36.8 ||",
    "claim": "For both En-De and De-En tasks, \"Ours Beam=1\" achieves higher performance scores than \"Ours Beam=5\" and the Baseline.",
    "label": "refute"
  },
  {
    "id": "training_839_refute",
    "table_caption": "Table: Pretraining comparison.",
    "table_content": "|| encoder num. layers | BLEU [%] no pretrain | BLEU [%] with pretrain ||\n|| 2 | 29.3 | - ||\n|| 3 | 29.9 | - ||\n|| 4 | 29.1 | 30.3 ||\n|| 5 | - | 30.3 ||\n|| 6 | - | 30.6 ||\n|| 7 | - | [BOLD] 30.9 ||",
    "claim": "BLEU scores improve with pretraining as the number of encoder layers increases, reaching the highest score of 30.9 with 6 layers.",
    "label": "refute"
  },
  {
    "id": "training_809_refute",
    "table_caption": "Table: F1-score of extracting relational triples from sentences with different number (denoted as N) of triples.",
    "table_content": "|| Method | NYT  [ITALIC] N=1 | NYT  [ITALIC] N=2 | NYT  [ITALIC] N=3 | NYT  [ITALIC] N=4 | NYT  [ITALIC] N≥5 | WebNLG  [ITALIC] N=1 | WebNLG  [ITALIC] N=2 | WebNLG  [ITALIC] N=3 | WebNLG  [ITALIC] N=4 | WebNLG  [ITALIC] N≥5 ||\n|| CopyR [ITALIC] OneDecoder | 66.6 | 52.6 | 49.7 | 48.7 | 20.3 | 65.2 | 33.0 | 22.2 | 14.2 | 13.2 ||\n|| CopyR [ITALIC] MultiDecoder | 67.1 | 58.6 | 52.0 | 53.6 | 30.0 | 59.2 | 42.5 | 31.7 | 24.2 | 30.0 ||\n|| GraphRel1 [ITALIC] p | 69.1 | 59.5 | 54.4 | 53.9 | 37.5 | 63.8 | 46.3 | 34.7 | 30.8 | 29.4 ||\n|| GraphRel2 [ITALIC] p | 71.0 | 61.5 | 57.4 | 55.1 | 41.1 | 66.0 | 48.3 | 37.0 | 32.1 | 32.1 ||\n|| CopyR∗ [ITALIC] RL | 71.7 | 72.6 | 72.5 | 77.9 | 45.9 | 63.4 | 62.2 | 64.4 | 57.2 | 55.7 ||\n|| CasRel | [BOLD] 88.2 | [BOLD] 90.3 | [BOLD] 91.9 | [BOLD] 94.2 | [BOLD] 83.7 (+37.8) | [BOLD] 89.3 | [BOLD] 90.8 | [BOLD] 94.2 | [BOLD] 92.4 | [BOLD] 90.9 (+35.2) ||",
    "claim": "GraphRel2 [ITALIC] p achieves the highest F1-scores across all classes for both NYT and WebNLG datasets.",
    "label": "refute"
  },
  {
    "id": "training_932_refute",
    "table_caption": "Table: Performance of LSTMs for PoS tagging using word identities and word shapes",
    "table_content": "|| [BOLD] Hidden Units | [BOLD] Test Acc.  [ITALIC] n≤20 | [BOLD] Test Acc.  [ITALIC] n≤25 | [BOLD] Training Acc.  [ITALIC] n≤20 | [BOLD] Training Acc.  [ITALIC] n≤25 ||\n|| 16 | 84.85 | 86.45 | 94.91 | 96.40 ||\n|| 32 | 83.41 | 83.89 | 94.43 | 93.18 ||\n|| 64 | 83.82 | 85.93 | 96.04 | 96.23 ||\n|| 100 | 83.49 | 85.53 | 93.93 | 95.74 ||\n|| 128 | 84.67 | 86.75 | 97.49 | 97.52 ||\n|| 150 | 85.84 | 86.33 | 97.49 | 97.27 ||\n|| 200 | [BOLD] 85.98 | [BOLD] 87.46 | 97.27 | 97.46 ||",
    "claim": "The LSTMs tagger achieves 85.98% test accuracy with 150 hidden units.",
    "label": "refute"
  },
  {
    "id": "training_860_refute",
    "table_caption": "Table: ROUGE F1 and METEOR scores (with-coverage) on the CNN/Daily Mail test set. Coverage mechanism See et al. (2017) is used in all models except the RL model Paulus et al. (2018). The model marked with ⋆ is trained and evaluated on the anonymized version of the data.",
    "table_content": "|| [EMPTY] | ROUGE 1 | ROUGE 2 | ROUGE L | MTR Full ||\n|| previous works | previous works | previous works | previous works | previous works ||\n|| pg (See17) | 39.53 | 17.28 | 36.38 | 18.72 ||\n|| RL⋆ (Paulus17) | 39.87 | 15.82 | 36.90 | [EMPTY] ||\n|| our models | our models | our models | our models | our models ||\n|| pg (baseline) | 39.22 | 17.02 | 35.95 | 18.70 ||\n|| pg + cbdec | 40.05 | 17.66 | 36.73 | 19.48 ||\n|| RL + pg | 39.59 | 17.18 | 36.16 | 19.70 ||\n|| RL + pg + cbdec | [BOLD] 40.66 | [BOLD] 17.87 | [BOLD] 37.06 | [BOLD] 20.51 ||",
    "claim": "The \"RL + pg\" model outperforms the \"RL + pg + cbdec\" model in at least one metric.",
    "label": "refute"
  },
  {
    "id": "training_929_refute",
    "table_caption": "Table: Comparison between different adaptive hard-attention techniques with average number of attended parts, and aggregation operation. We consider a simple summation, and the non-local pairwise aggregation. Since AdaHAN adaptively selects relevant features, based on the fixed threshold 1w∗h, we report here the average number of attended parts.",
    "table_content": "|| [EMPTY] | Percentage | Overall | Yes/No | Number | Other ||\n|| [EMPTY] | of cells | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| AdaHAN+sum | 25.66% | 27.40 | 40.70 | 11.13 | 24.86 ||\n|| AdaHAN+pairwise | 32.63% | 28.65 | 52.25 | 13.79 | 20.33 ||\n|| HAN+sum | 32% | 27.43 | 41.05 | 11.38 | 24.68 ||\n|| HAN+sum | 48% | 27.94 | 41.35 | 11.93 | 25.27 ||\n|| HAN+pairwise | 32% | 27.45 | 40.91 | 11.48 | 24.75 ||\n|| HAN+pairwise | 48% | 28.23 | 41.23 | 11.40 | 25.98 ||",
    "claim": "On average, the adaptive mechanism uses 32.63 cells with sum pooling and 25.66 cells with the non-local pairwise aggregation mechanism.",
    "label": "refute"
  },
  {
    "id": "training_941_refute",
    "table_caption": "Table: Accuracies on our newly released 700-Movie Review Dataset",
    "table_content": "|| [BOLD] Method | [BOLD] Feature Selection | [BOLD] Accuracy ||\n|| Document Vector + tfidf | None | 74.57 ||\n|| Document Vector + tfidf | PCA(n=50) | 76.33 ||\n|| Document Vector + tfidf | ANOVA-F | 88.07 ||\n|| Weighted Word Vector + tfidf | None | 76.43 ||\n|| Weighted Word Vector + tfidf | ANOVA-F | 90.37 ||\n|| Weighted Word Vector + tfidf | PCA(n=50) | 78.61 ||",
    "claim": "PCA feature selection results in higher accuracy than ANOVA-F for both Document Vector + tfidf and Weighted Word Vector + tfidf methods.",
    "label": "refute"
  },
  {
    "id": "training_778_refute",
    "table_caption": "Table: Language model evaluation",
    "table_content": "|| [EMPTY] | [BOLD] Multilinguage LM0 | [BOLD] English LM1 | [BOLD] Vietnamese LM2 ||\n|| [BOLD] Test-clean | 569.3 | 187.1 | - ||\n|| [BOLD] Test-other | 522.2 | 174.3 | - ||\n|| [BOLD] Reading-test | 136.6 | - | 87.2 ||\n|| [BOLD] Conversation-test | 95.2 | - | 62.7 ||\n|| [BOLD] YouTube-test | 199.5 | - | 111.4 ||\n|| [BOLD] VLSP2018 | 75.7 | - | 47.5 ||",
    "claim": "LM0 has a perplexity of 569.3 on Test-clean and 522.2 on Test-other, while LM1 has a perplexity of 187.1 on Test-clean and 174.3 on Test-other. LM2 has a perplexity of 87.2 on Reading-test, 62.7 on Conversation-test, 111.4 on YouTube-test, and 47.8 on VLSP2018.",
    "label": "refute"
  },
  {
    "id": "training_833_refute",
    "table_caption": "Table: Automatic Evaluation on the Music dataset",
    "table_content": "|| Models | BLEU | Precision | Recall ||\n|| S2SA | 0.11 | 0.01±0.01 | 0.004±0.02 ||\n|| GenQA | 0.05 | 0.1134±0.14 | 0.05±0.1 ||\n|| GenQAD | 0.06 | 0.15±0.16 | 0.05±0.1 ||\n|| GenDS-Single | 0.108 | 0.28±0.19 | [BOLD] 0.19±0.18 ||\n|| GenDS-Static | 0.108 | 0.14±0.15 | 0.10±0.14 ||\n|| GenDS | [BOLD] 0.122 | [BOLD] 0.40 ±0.25 | 0.14±0.16 ||",
    "claim": "GenDS achieves the highest BLEU and precision scores among all models, while GenDS achieves the highest recall.",
    "label": "refute"
  },
  {
    "id": "training_893_refute",
    "table_caption": "Table: WERs (%) of neural network with binary weights and activations on the AMI dataset. The number of hidden units is 2048, and b denotes binarization.",
    "table_content": "|| Model | [ITALIC] b | dev | eval ||\n|| Baseline | – | 26.1 | 27.5 ||\n|| Binary weights ( [ITALIC] p=0) | (−1,+1) | 30.3 | 32.7 ||\n|| Binary weights ( [ITALIC] p=.001) | (−1,+1) | 30.0 | 32.2 ||\n|| Binary weights ( [ITALIC] p=.01) | (−1,+1) | 29.6 | 31.7 ||\n|| Binary weights ( [ITALIC] p=.05) | (−1,+1) | 29.6 | 31.9 ||\n|| Binary activations ( [ITALIC] k=1) | (−1,+1) | 30.1 | 32.5 ||\n|| Binary activations ( [ITALIC] k=2) | (−1,+1) | 29.9 | 32.3 ||\n|| Binary activations ( [ITALIC] k=3) | (−1,+1) | 30.2 | 32.4 ||\n|| Binary activations ( [ITALIC] k=4) | (−1,+1) | 29.8 | 32.0 ||\n|| Binary activations ( [ITALIC] k=1) | (0,1) | 27.5 | 29.5 ||\n|| Binary activations ( [ITALIC] k=2) | (0,1) | 28.0 | 30.2 ||\n|| Binary activations ( [ITALIC] k=3) | (0,1) | 29.8 | 32.2 ||\n|| Binary neural network | (−1,+1) | Not Converged | Not Converged ||",
    "claim": "The binary activation system with (−1,+1) performs better than its counterpart with (0,1) when k=1.",
    "label": "refute"
  },
  {
    "id": "training_881_refute",
    "table_caption": "Table: Comparison between state-of-the-art models Merity et al. (2017); Yang et al. (2017) and a Simple LSTM, and the same models with Alleviated TOI. The comparison highlights how the addition of Alleviated TOI is able to improve state-of-the-art models, as well as a simple model that does not benefit from extensive hyper-parameter optimization.",
    "table_content": "|| Model | test ppl ||\n|| AWD-LSTM Merity et al. ( 2017 ) | 58.8 ||\n|| AWD-LSTM + Alleviated TOI | 56.46 ||\n|| AWD-LSTM-MoS Yang et al. ( 2017 ) | 55.97 ||\n|| AWD-LSTM-MoS + Alleviated TOI | 54.58 ||\n|| Simple-LSTM | 75.36 ||\n|| Simple-LSTM + Alleviated TOI | 74.44 ||",
    "claim": "AWD-LSTM-MoS achieves the lowest test perplexity of 54.58.",
    "label": "refute"
  },
  {
    "id": "training_825_refute",
    "table_caption": "Table: Test Results for Event Coreference with the Singleton and Matching baselines.",
    "table_content": "|| [EMPTY] | [ITALIC] B3 | CEAF-E | MUC | BLANC | AVG. ||\n|| Singleton | 78.10 | 68.98 | 0.00 | 48.88 | 52.01 ||\n|| Matching | 78.40 | 65.82 | [BOLD] 69.83 | 76.29 | 71.94 ||\n|| LCC | 82.85 | 74.66 | 68.50 | [BOLD] 77.61 | 75.69 ||\n|| UI-CCG | 83.75 | 75.81 | 63.78 | 73.99 | 74.28 ||\n|| LTI | 82.27 | 75.15 | 60.93 | 71.57 | 72.60 ||\n|| This work | [BOLD] 85.59 | [BOLD] 79.65 | 67.81 | 77.37 | [BOLD] 77.61 ||",
    "claim": "This work achieves the highest average F-score and performs the best on B3 and MUC metrics.",
    "label": "refute"
  },
  {
    "id": "training_868_refute",
    "table_caption": "Table: Macro-average AUC and APR over test queries with different DAG structures are used to evaluate the performance. All and H-Neg. denote macro-averaged across all query types and query types with hard negative sampling (see Section 3.2.3).",
    "table_content": "|| Dataset Metric | Bio AUC | Bio AUC | Bio APR | Bio APR | DB18 AUC | DB18 AUC | DB18 APR | DB18 APR | WikiGeo19 AUC | WikiGeo19 AUC | WikiGeo19 APR | WikiGeo19 APR ||\n|| [EMPTY] | All | H-Neg | All | H-Neg | All | H-Neg | All | H-Neg | All | H-Neg | All | H-Neg ||\n|| Billinear[mean_simple] | 81.65 | 67.26 | 82.39 | 70.07 | 82.85 | 64.44 | 85.57 | 71.72 | 81.82 | 60.64 | 82.35 | 64.22 ||\n|| Billinear[min_simple] | 82.52 | 69.06 | 83.65 | 72.7 | 82.96 | 64.66 | 86.22 | 73.19 | 82.08 | 61.25 | 82.84 | 64.99 ||\n|| TransE[mean] | 80.64 | 73.75 | 81.37 | 76.09 | 82.76 | 65.74 | 85.45 | 72.11 | 80.56 | 65.21 | 81.98 | 68.12 ||\n|| TransE[min] | 80.26 | 72.71 | 80.97 | 75.03 | 81.77 | 63.95 | 84.42 | 70.06 | 80.22 | 64.57 | 81.51 | 67.14 ||\n|| GQE[mean] | 83.4 | 71.76 | 83.82 | 73.41 | 83.38 | 65.82 | 85.63 | 71.77 | 83.1 | 63.51 | 83.81 | 66.98 ||\n|| GQE[min] | 83.12 | 70.88 | 83.59 | 73.38 | 83.47 | 66.25 | 86.09 | 73.19 | 83.26 | 63.8 | 84.3 | 67.95 ||\n|| GQE+KG[min] | 83.69 | 72.23 | 84.07 | 74.3 | 84.23 | 68.06 | 86.32 | 73.49 | 83.66 | 64.48 | 84.73 | 68.51 ||\n|| [BOLD] CGA+KG+1[min] | 84.57 | 74.87 | 85.18 | 77.11 | 84.31 | 67.72 | 87.06 | 74.94 | 83.91 | 64.83 | 85.03 | 69 ||\n|| [BOLD] CGA+KG+4[min] | [BOLD] 85.13 | [BOLD] 76.12 | 85.46 | [BOLD] 77.8 | 84.46 | 67.88 | 87.05 | 74.66 | 83.96 | 64.96 | 85.36 | 69.64 ||\n|| [BOLD] CGA+KG+8[min] | 85.04 | 76.05 | [BOLD] 85.5 | 77.76 | [BOLD] 84.67 | [BOLD] 68.56 | [BOLD] 87.29 | [BOLD] 75.23 | [BOLD] 84.15 | [BOLD] 65.23 | [BOLD] 85.69 | [BOLD] 70.28 ||\n|| Relative Δ over GQE | 2.31 | [BOLD] 7.29 | 2.28 | [BOLD] 5.97 | 1.44 | [BOLD] 3.49 | 1.39 | [BOLD] 2.79 | 1.07 | [BOLD] 2.24 | 1.65 | [BOLD] 3.43 ||",
    "claim": "CGA+KG+4[min] achieves the highest AUC and APR scores across all datasets and query types in the table.",
    "label": "refute"
  },
  {
    "id": "training_854_refute",
    "table_caption": "Table: Results of image retrieval task on Flickr30k.",
    "table_content": "|| [EMPTY] | R@1 | R@5 | R@10 ||\n|| ViLBERT  | 58.2 | 84.9 | 91.5 ||\n|| ViLBERT + augmentation | [BOLD] 60.4 | [BOLD] 86.4 | [BOLD] 91.9 ||",
    "claim": "ViLBERT achieves a higher R@1 score than ViLBERT + augmentation.",
    "label": "refute"
  },
  {
    "id": "training_890_refute",
    "table_caption": "Table: BLEU score on newstest 2014",
    "table_content": "|| Model | BLEU ||\n|| ByteNet kalchbrenner2016neural | 23.75 ||\n|| GNMT+RL wu2016google | 24.6 ||\n|| ConvS2S GehringAGYD17 | 25.16 ||\n|| Transformer vaswani2017attention | 27.3 ||\n|| Transformer (our implementation) | 27.2 ||\n|| BPT (k=1) | 26.9 ||\n|| BPT (k=2) | 27.4 ||\n|| BPT (k=4) | [BOLD] 27.6 ||\n|| BPT (k=8) | 26.7 ||",
    "claim": "BPT with k=2 and k=4 does not outperform the Vanilla Transformer.",
    "label": "refute"
  },
  {
    "id": "training_915_refute",
    "table_caption": "Table: Comparison of different strategies on CoNLL-2003 dataset. ERR is the relative error rate reduction of our model compared to the baseline.",
    "table_content": "|| [EMPTY] | Strategy | [ITALIC] F1 | ERR ||\n|| base model | - | 91.60 | - ||\n|| sentence-level | mean-pooling | 91.65 | 0.60 ||\n|| sentence-level | label-embedding | 91.80 | 2.23 ||\n|| document-level | dot-product | 91.63 | 1.55 ||\n|| document-level | scaled dot-product | 91.75 | 1.79 ||\n|| document-level | cosine similarity | 91.79 | 2.38 ||\n|| ALL | - | [BOLD] 91.96 | [BOLD] 3.81 ||",
    "claim": "Dot-product performs best among the compatibility functions, while cosine similarity performs worst.",
    "label": "refute"
  },
  {
    "id": "training_898_refute",
    "table_caption": "Table: The average accuracies across all combinations of language pairs for different linear transformations and post-processing techniques. The size of bilingual dictionary was set to n=20,000. No trans. denotes the monolingual experiments without transforming the spaces.",
    "table_content": "|| [EMPTY] | [EMPTY] | [BOLD] -  [BOLD] Acc@1 | [BOLD] -  [BOLD] Acc@5 | [BOLD] -c  [BOLD] Acc@1 | [BOLD] -c  [BOLD] Acc@5 | [BOLD] -u  [BOLD] Acc@1 | [BOLD] -u  [BOLD] Acc@5 | [BOLD] -cu  [BOLD] Acc@1 | [BOLD] -cu  [BOLD] Acc@5 ||\n|| Monoling. | No trans. | 49.6 | 63.7 | 50.1 | 64.6 | 50.6 | 64.6 | [BOLD] 51.1 | [BOLD] 65.2 ||\n|| Monoling. | M-LS | 40.2 | 55.3 | 40.3 | 55.6 | 41.3 | 56.5 | 41.3 | 56.6 ||\n|| Monoling. | M-OT | 49.6 | 63.7 | 50.1 | 64.6 | 50.6 | 64.6 | [BOLD] 51.1 | [BOLD] 65.2 ||\n|| Monoling. | M-CCA | 46.8 | 61.8 | 47.6 | 62.5 | 47.5 | 62.4 | 48.1 | 63.0 ||\n|| Cross-lingual | B-LS | 33.7 | 51.4 | 34.3 | 52.3 | 33.5 | 51.1 | 34.0 | 52.0 ||\n|| Cross-lingual | B-OT | 40.1 | 55.9 | 40.6 | 56.6 | 40.7 | 56.5 | 41.2 | 57.3 ||\n|| Cross-lingual | B-CCA | 42.3 | 57.5 | 42.7 | 58.2 | 42.6 | 57.8 | [BOLD] 43.1 | [BOLD] 58.5 ||\n|| Cross-lingual | M-LS | 32.2 | 48.8 | 32.7 | 49.3 | 32.9 | 49.6 | 32.5 | 49.3 ||\n|| Cross-lingual | M-OT | 37.3 | 53.7 | 37.6 | 54.3 | 37.8 | 54.4 | [BOLD] 38.2 | [BOLD] 55.0 ||\n|| Cross-lingual | M-CCA | 35.3 | 52.7 | 36.2 | 53.8 | 35.5 | 52.9 | 36.0 | 53.5 ||",
    "claim": "Orthogonal transformation provides the same results as the original semantic space in monolingual settings, while orthogonal transformation performs best for bilingual cases in cross-lingual settings. Mean centering followed by vector normalization led to the best results in all cases.",
    "label": "refute"
  },
  {
    "id": "training_806_refute",
    "table_caption": "Table: Performance of updated word vectors on the word analogy task. We split the English word analogy datasets (Mikolov et al., 2013a) into Out of vocab and In vocab questions. Out of vocab questions are composed of words that are out of vocabulary for S0, hence the null accuracy.",
    "table_content": "|| [EMPTY] | Out of vocab | In vocab ||\n|| Train on  [ITALIC] S0 | 00.0 | 70.1 ||\n|| Train on  [ITALIC] S1 | 66.9 | 66.1 ||\n|| Train on  [ITALIC] S0∪ [ITALIC] S1 | 68.6 | 71.2 ||\n|| Fine-tune | 67.7 | 66.8 ||\n|| Subwords | 37.0 | 71.6 ||\n|| RCSLS+Fine. | [BOLD] 67.9 | [BOLD] 72.1 ||",
    "claim": "\"RCSLS+Fine.\" achieves the highest accuracy for \"Out of vocab\" questions but not for \"In vocab\" questions.",
    "label": "refute"
  },
  {
    "id": "training_918_refute",
    "table_caption": "Table: High Resource Results",
    "table_content": "|| Model | WER | WER 100 | PER ||\n|| wFST | [BOLD] 44.17 | 21.97 | [BOLD] 14.70 ||\n|| LangID-High | 47.88 | [BOLD] 15.50 | 16.89 ||\n|| LangID-All | 48.76 | 15.78 | 17.35 ||\n|| NoLangID-High | 69.72 | 29.24 | 35.16 ||\n|| NoLangID-All | 69.82 | 29.27 | 35.47 ||",
    "claim": "The wFST model achieves the lowest WER 100 and PER among the models listed.",
    "label": "refute"
  },
  {
    "id": "training_930_refute",
    "table_caption": "Table: Comparison between different number of the attended cells as the percentage of the whole input. The results are reported on VQA-CP v2. The second column denotes the percentage of the attended input. The third column denotes number of layers of the MLP (Equations 2 and 3).",
    "table_content": "|| [EMPTY] | Percentage of cells | Number of layers | Overall | Yes/No | Number | Other ||\n|| HAN+sum | 25% | 0 | 26.38 | 43.21 | 13.12 | 21.17 ||\n|| HAN+sum | 50% | 0 | 26.75 | 41.42 | 10.94 | 23.38 ||\n|| HAN+sum | 75% | 0 | 26.82 | 41.30 | 11.48 | 23.42 ||\n|| HAN+sum | 25% | 2 | 26.99 | 40.53 | 11.38 | 24.15 ||\n|| HAN+sum | 50% | 2 | 27.43 | 41.05 | 11.38 | 24.68 ||\n|| HAN+sum | 75% | 2 | 27.94 | 41.35 | 11.93 | 25.27 ||",
    "claim": "The overall performance remains unchanged with the addition of layers on top of the pretrained embeddings.",
    "label": "refute"
  },
  {
    "id": "training_831_refute",
    "table_caption": "Table: Cross-lingual keyword spotting results (%) for different variants of XVisionSpeechCNN on development data.",
    "table_content": "|| Model | [ITALIC] P@10 | [ITALIC] P@ [ITALIC] N | EER | AP ||\n|| XVisionSpeechCNN | 60.8 | 39.3 | 23.1 | 38.0 ||\n|| KeyXVisionSpeechCNN | 60.0 | 39.6 | 24.5 | 36.9 ||\n|| OracleXVisionSpeechCNN | 57.4 | 37.6 | 24.8 | 36.5 ||",
    "claim": "XVisionSpeechCNN performs slightly worse on most metrics compared to KeyXVisionSpeechCNN and OracleXVisionSpeechCNN.",
    "label": "refute"
  },
  {
    "id": "training_942_refute",
    "table_caption": "Table: Accuracies for Product Review and Movie Review Datasets.",
    "table_content": "|| [BOLD] Features | [BOLD] Accuracy(1) | [BOLD] Accuracy(2) ||\n|| WordVector Averaging | 78.0 | 79.62 ||\n|| WordVector+tf-idf | 90.73 | 89.52 ||\n|| WordVector+tf-idf without stop words | 91.14 | 89.97 ||\n|| Weighted WordVector | 89.71 | 85.90 ||\n|| Weighted WordVector+tfidf | [BOLD] 92.89 | [BOLD] 90.30 ||",
    "claim": "The highest accuracy is achieved with the \"WordVector+tf-idf without stop words\" method.",
    "label": "refute"
  },
  {
    "id": "training_911_refute",
    "table_caption": "Table: F1 scores on CoNLL-2003. † refers to models trained on both training and development datasets.",
    "table_content": "|| [BOLD] Models | [ITALIC] F1 ||\n||  | 90.94 ||\n||  | 91.21 ||\n||  | 91.62 ||\n||  | 91.24 ± 0.12 ||\n||  | 91.35 ||\n||  | 91.57 ||\n||  | 91.64 ± 0.17 ||\n||  | 91.10 ||\n||  | 91.44 ± 0.10 ||\n||  | 91.74 ||\n||  | 91.54 ||\n|| Ours | [BOLD] 91.96 ± 0.03 ||\n|| [BOLD] + Language Models / External knowledge | [BOLD] + Language Models / External knowledge ||\n|| † | 91.62 ± 0.33 ||\n||  | 91.71 ± 0.10 ||\n||  (ELMo) | 92.20 ||\n||  | 92.61 ||\n||  (BERT) | 92.80 ||\n|| † | 93.09 ||\n|| † | 93.18 ||\n||  (BERT) | 93.23 ||\n|| Ours + BERT | [BOLD] 93.37 ± 0.04 ||",
    "claim": "The \"Ours\" model achieves an F1 score of 91.64 ± 0.17, and the \"Ours + BERT\" model achieves an F1 score of 92.80.",
    "label": "refute"
  },
  {
    "id": "training_939_refute",
    "table_caption": "Table: Comparison of accuracies on 3 Datasets (IMDB, Amazon Electronics Review and Hindi Movie Reviews (IITB)) for various types of document composition models. The state of the art for these tasks are: IMDB: 92.58% [Le and Mikolov2014]; Amazon:85.90% [Dredze et al.2008], Hindi:79.0% [Bakliwal et al.2012].",
    "table_content": "|| [BOLD] Method | [BOLD] IMDB | [BOLD] Amazon | [BOLD] Hindi ||\n|| RNNLM (Baseline) | 86.45 | 90.03 | 78.84 ||\n|| Paragraph Vector  | 92.58 | 91.30 | 74.57 ||\n|| Averaged Vector | 88.42 | 88.52 | 79.62 ||\n|| Weighted Average Vector | 89.56 | 88.63 | 85.90 ||\n|| Composite Document Vector | 93.91 | 92.17 | 90.30 ||",
    "claim": "Composite Document Vector achieves the highest accuracy across most datasets, but Weighted Average Vector outperforms it on the Hindi dataset.",
    "label": "refute"
  },
  {
    "id": "training_953_refute",
    "table_caption": "Table: Accuracies of the top three QA systems submitted in the competition.",
    "table_content": "|| Name | Accuracy ||\n|| Our system | [BOLD] 0.85 ||\n|| Acelove | 0.675 ||\n|| Lunit.io | 0.6 ||\n|| Baseline | 0.55 ||",
    "claim": "Acelove achieved the highest accuracy.",
    "label": "refute"
  },
  {
    "id": "training_873_refute",
    "table_caption": "Table: Tabelle 3: Comparison of document-level model architectures and complexity.",
    "table_content": "|| Approach | Context Encoder Architecture | Context Encoder #layers | [BOLD] en-it Bleu [%] | [BOLD] en-it Ter [%] | [BOLD] en-de Bleu [%] | [BOLD] en-de Ter [%] ||\n|| Baseline | ⋅ | ⋅ | 31.4 | 56.1 | 28.9 | 61.8 ||\n|| Single-Encoder | Transformer | 6 | 31.5 | 57.2 | 28.9 | 61.4 ||\n|| Multi-Encoder (Out.) | Transformer | 6 | 31.3 | 56.1 | 29.1 | 61.4 ||\n|| Multi-Encoder (Seq.) | Transformer | 6 | 32.6 | 55.2 | 29.9 | 60.7 ||\n|| Multi-Encoder (Para.) | Transformer | 6 | [BOLD] 32.7 | [BOLD] 54.7 | 30.1 | 60.3 ||\n|| Multi-Encoder (Para.) | Transformer | 2 | 32.6 | 55.2 | 30.2 | 60.5 ||\n|| Multi-Encoder (Para.) | Transformer | 1 | 32.2 | 55.8 | 30.0 | 60.4 ||\n|| Multi-Encoder (Para.) | Word Embedding | ⋅ | 32.5 | 54.8 | [BOLD] 30.3 | [BOLD] 59.9 ||",
    "claim": "Context encoding without sequential modeling shows inferior performance to using a full 6-layer encoder, as indicated by the Multi-Encoder (Para.) with Word Embedding achieving lower BLEU and higher TER scores compared to the Multi-Encoder (Para.) with a 6-layer Transformer.",
    "label": "refute"
  },
  {
    "id": "training_828_refute",
    "table_caption": "Table: Stopping method results on 20Newsgroups for different batch sizes using various window sizes. The top number in each row shows the number of annotations at the stopping point and the bottom number shows the F-Measure at the stopping point.",
    "table_content": "|| [width=15em]Stopping MethodBatch Percent | 1% | 5% | 10% ||\n|| Oracle Method | 1514.20 | 2490.40 | 3901.95 ||\n|| Oracle Method | 76.17 | 75.55 | 75.49 ||\n|| BV2009 (Window Size = 3) | 1299.50 | 3877.10 | 6446.70 ||\n|| BV2009 (Window Size = 3) | 74.44 | 75.17 | 75.19 ||\n|| BV2009 (Window Size = 1) | 1101.75 | 3141.30 | 5089.50 ||\n|| BV2009 (Window Size = 1) | 74.40 | 75.04 | 75.11 ||",
    "claim": "When using a window size of one, BV2009 requires more annotations than when using a window size of three, without losing much F-Measure.",
    "label": "refute"
  },
  {
    "id": "training_892_refute",
    "table_caption": "Table: WERs (%) of neural networks with binary weights and activations on the WSJ1 dataset. We set p=0 for the system with binary weights, and k=1 for the system with binary activations.",
    "table_content": "|| Model | Size | [ITALIC] b | dev93 | eval92 ||\n|| Baseline | 1024 | – | 6.8 | 3.8 ||\n|| Baseline | 2048 | – | 6.5 | 3.5 ||\n|| Binary weights | 1024 | (−1,+1) | 7.7 | 4.8 ||\n|| Binary weights | 1024 | (0,1) | Not Converged | Not Converged ||\n|| Binary activations | 1024 | (−1,+1) | 8.2 | 4.4 ||\n|| Binary activations | 1024 | (0,1) | 7.2 | 4.1 ||\n|| Binary neural network | 1024 | (−1,+1) | 15.6 | 10.7 ||\n|| Binary activations | 2048 | (−1,+1) | 7.3 | 4.4 ||\n|| Binary weights | 2048 | (−1,+1) | 7.5 | 4.4 ||\n|| Binary neural network | 2048 | (−1,+1) | Not Converged | Not Converged ||",
    "claim": "Using (0,1) for binary weights did not converge, while using (0,1) for binary activations achieved higher WER compared to (−1,+1) binary activations.",
    "label": "refute"
  },
  {
    "id": "training_764_refute",
    "table_caption": "Table: Binary accuracy for different variants of C-MFN and training scenarios outlined in Section 5. The best performance is achieved using all three modalities of text (T), vision (V) and acoustic (A).",
    "table_content": "|| Modality | T | A+V | T+A | T+V | T+A+V ||\n|| C-MFN (P) | 62.85 | 53.3 | 63.28 | 63.22 | 64.47 ||\n|| C-MFN (C) | 57.96 | 50.23 | 57.78 | 57.99 | 58.45 ||\n|| C-MFN | 64.44 | 57.99 | 64.47 | 64.22 | 65.23 ||",
    "claim": "C-MFN outperforms C-MFN (P) and C-MFN (C) across all modalities, and C-MFN (C) performs better than C-MFN (P).",
    "label": "refute"
  },
  {
    "id": "training_910_refute",
    "table_caption": "Table: Comparison results on NYT122 with different prediction and regularization strategies (using encoder or decoder).",
    "table_content": "|| [BOLD] Model | [BOLD] Metrics Prediction based on encoder | [BOLD] Metrics Prediction based on encoder | [BOLD] Metrics Prediction based on encoder | [BOLD] Metrics Prediction based on encoder | [BOLD] Metrics Prediction based on decoder | [BOLD] Metrics Prediction based on decoder | [BOLD] Metrics Prediction based on decoder | [BOLD] Metrics Prediction based on decoder ||\n|| [EMPTY] | F1 | F1 | NMI | NMI | F1 | F1 | NMI | NMI ||\n|| [EMPTY] | Mean | Std | Mean | Std | Mean | Std | Mean | Std ||\n|| DVAE | 0.417 | 0.011 | 0.339 | 0.009 | 0.419 | 0.011 | 0.337 | 0.014 ||\n|| RegDVAE (Euclidean at encoder) | [BOLD] 0.469 | 0.014 | [BOLD] 0.430 | 0.020 | [BOLD] 0.448 | 0.020 | [BOLD] 0.384 | 0.020 ||\n|| RegDVAE (KL at encoder) | 0.375 | 0.009 | 0.359 | 0.014 | 0.380 | 0.011 | 0.355 | 0.014 ||\n|| RegDVAE (JS at encoder) | 0.435 | 0.038 | 0.370 | 0.042 | 0.409 | 0.012 | 0.336 | 0.005 ||\n|| RegDVAE (Euclidean at decoder) | 0.416 | 0.019 | 0.329 | 0.017 | 0.350 | 0.012 | 0.201 | 0.054 ||",
    "claim": "RegDVAE with Euclidean distance at the decoder achieves the highest F1 and NMI scores compared to other regularization methods.",
    "label": "refute"
  },
  {
    "id": "training_863_refute",
    "table_caption": "Table: M2 and GLEU results. The bold scores represent the best score in unsupervised SMT. The underlined scores represent the best overall score.",
    "table_content": "|| [EMPTY] | iter | CoNLL-14 (M2) P | CoNLL-14 (M2) R | CoNLL-14 (M2) F0.5 | JFLEG GLEU ||\n|| No edit | - | - | - | - | 40.54 ||\n|| Supervised NMT | - | 53.11 | 26.47 | 44.21 | 54.04 ||\n|| Supervised SMT | - | 43.02 | 33.18 | 40.61 | 55.93 ||\n|| Unsupervised SMT | 0 | 21.82 | [BOLD] 36.75 | 23.75 | 49.94 ||\n|| w/ forward_refine | 1 | [BOLD] 25.92 | 32.65 | [BOLD] 27.04 | [BOLD] 50.65 ||\n|| [EMPTY] | 2 | 25.58 | 31.02 | 26.51 | 50.19 ||\n|| [EMPTY] | 3 | 23.95 | 33.13 | 24.54 | 50.40 ||\n|| w/ backward_refine | 1 | 22.39 | 33.39 | 23.97 | 49.02 ||\n|| [EMPTY] | 2 | 24.96 | 27.13 | 25.36 | 48.90 ||\n|| [EMPTY] | 3 | 26.07 | 21.01 | 24.87 | 48.75 ||",
    "claim": "The F0.5 score for USMTforward in iter 1 is 10.57 points lower than that of supervised SMT and 20.17 points lower than that of supervised NMT. On JFLEG, the highest score was achieved with USMTforward in iter 1 among the unsupervised SMT models; its GLEU scores are 2.28 points and 6.39 points lower than those of supervised SMT and supervised NMT, respectively.",
    "label": "refute"
  },
  {
    "id": "training_955_refute",
    "table_caption": "Table: Adversarial Success for different models.",
    "table_content": "|| Model | adver-1 | adver-2 | adver-3 ||\n|| VLV-GM (MMI) | [BOLD] 0.174 | [BOLD] 0.120 | [BOLD] 0.054 ||\n|| LDA-HMM-GM (MMI) | 0.130 | 0.104 | 0.043 ||\n|| Seq2Seq (MMI) | 0.120 | 0.090 | 0.039 ||\n|| Seq2Seq (bi) | 0.108 | 0.078 | 0.030 ||\n|| Seq2Seq (uni) | 0.101 | 0.068 | 0.024 ||",
    "claim": "VLV-GM (MMI) does not achieve the highest scores across all adversarial metrics, while all models perform poorly on the adver-3 evaluation metric.",
    "label": "refute"
  },
  {
    "id": "training_862_refute",
    "table_caption": "Table: The effect of source languages for comparable corpus creation. These News Crawl corpora are as of 2017 version. The number of sentences in each dataset is approximately 20M, respectively. These results are obtained by USMTforward in iter 1.",
    "table_content": "|| Src | Precision | Recall | F0.5 ||\n|| Fi News Crawl | 29.17 | 28.52 | 29.04 ||\n|| Ru News Crawl | 27.11 | 29.84 | 27.62 ||\n|| Fr News Crawl | 25.05 | 30.27 | 25.94 ||\n|| De News Crawl | 23.26 | 26.04 | 25.04 ||",
    "claim": "The outputs using Russian data achieve the best F0.5 score among the various languages.",
    "label": "refute"
  },
  {
    "id": "training_937_refute",
    "table_caption": "Table: (b) NMT sub-task",
    "table_content": "|| Systems | [BOLD] TER↓ | BLEU↑ ||\n|| FBK | 16.46 | 75.53 ||\n|| [BOLD] MS-UEdin (Ours) | [BOLD] 16.50 | [BOLD] 75.44 ||\n|| POSTECH | 16.70 | 75.14 ||\n|| Baseline | 16.84 | 74.73 ||\n|| USAAR DFKI | 17.23 | 74.22 ||\n|| DFKI-MLT | 18.84 | 70.87 ||",
    "claim": "MS-UEdin outperforms all other submissions in terms of BLEU but not in TER.",
    "label": "refute"
  },
  {
    "id": "training_813_refute",
    "table_caption": "Table: Market trend prediction using FT news articles and RWNC headlines (2011-2015).",
    "table_content": "|| Type | Baseline Acc | Baseline  [ITALIC] Fup1 | Baseline  [ITALIC] Fdown1 | Financial Times Acc | Financial Times  [ITALIC] Fup1 | Financial Times  [ITALIC] Fdown1 | Reddit Headlines Acc | Reddit Headlines  [ITALIC] Fup1 | Reddit Headlines  [ITALIC] Fdown1 ||\n|| DJIA | 0.700 | [BOLD] 0.754 | 0.615 | [BOLD] 0.706 | 0.752 | [BOLD] 0.639 | 0.618 | 0.716 | 0.417 ||\n|| AAPL | [BOLD] 0.685 | [BOLD] 0.723 | [BOLD] 0.634 | 0.652 | 0.723 | 0.531 | 0.624 | 0.700 | 0.496 ||\n|| JPM | 0.673 | 0.733 | 0.578 | [BOLD] 0.679 | [BOLD] 0.739 | [BOLD] 0.583 | 0.615 | 0.713 | 0.415 ||\n|| EUR/USD | 0.641 | 0.715 | 0.518 | [BOLD] 0.653 | 0.691 | [BOLD] 0.605 | 0.638 | 0.684 | 0.578 ||\n|| GBP/USD | [BOLD] 0.714 | 0.705 | [BOLD] 0.723 | 0.711 | [BOLD] 0.708 | 0.715 | 0.615 | 0.625 | 0.605 ||",
    "claim": "The sentiment-enriched model outperforms the baseline model for DJIA and JPM, but underperforms for AAPL, GBP/USD, and EUR/USD.",
    "label": "refute"
  },
  {
    "id": "training_935_refute",
    "table_caption": "Table: Experiments with WMT 2017+eSCAPE data for SMT system.",
    "table_content": "|| Model | dev 2016  [BOLD] TER↓ | dev 2016 BLEU↑ | test 2016  [BOLD] TER↓ | test 2016 BLEU↑ | test 2017  [BOLD] TER↓ | test 2017 BLEU↑ ||\n|| Transformer all | 17.84 | 73.45 | 17.81 | 72.79 | 18.10 | 71.72 ||\n|| Transformer 1M | 17.59 | 73.45 | 18.29 | 72.20 | 18.42 | 71.50 ||\n|| Transformer 2M | 17.92 | 73.37 | 18.02 | 72.41 | 18.35 | 71.57 ||\n|| Transformer 4M | 17.75 | 73.51 | 17.89 | 72.70 | 18.09 | 71.78 ||\n|| [BOLD] Transformer x4 (all above) | [BOLD] 17.31 | [BOLD] 74.14 | [BOLD] 17.34 | [BOLD] 73.43 | [BOLD] 17.47 | [BOLD] 72.84 ||",
    "claim": "Transformer 4M achieves the best TER and BLEU scores across all test sets.",
    "label": "refute"
  },
  {
    "id": "training_837_refute",
    "table_caption": "Table: Performance comparison on WMT 2017 English→German. The baseline systems (upper half) are trained on the parallel data of the WMT Enlgish→German 2017 task. We downloaded the hypotheses from here.66footnotemark: 6 The WMT 2017 system hypotheses (lower half) are generated using systems having additional back- translation (bt) data. These hypotheses are downloaded from here.77footnotemark: 7",
    "table_content": "|| System | BLEU [%] ||\n|| [EMPTY] | newstest2017 ||\n|| RETURNN | [BOLD] 26.1 ||\n|| OpenNMT-py | 21.8 ||\n|| OpenNMT-lua | 22.6 ||\n|| Marian | 25.6 ||\n|| Nematus | 23.5 ||\n|| Sockeye | 25.3 ||\n|| WMT 2017 Single Systems + bt data | WMT 2017 Single Systems + bt data ||\n|| LMU | 26.4 ||\n|| + reranking | 27.0 ||\n|| Systran | 26.5 ||\n|| Edinburgh | 26.5 ||",
    "claim": "RETURNN achieves a BLEU score of 26.1%, outperforming other toolkits listed, and performs better than the WMT 2017 single systems with back-translated data.",
    "label": "refute"
  },
  {
    "id": "training_871_refute",
    "table_caption": "Table: Comparative results of the proposed model (sdpLSTM) with different baselines and state-of-the-art systems. Ref. choi2016extraction ∗ and li2015approach ∗ denote the reimplementation of the systems proposed in choi2016extraction and li2015approach with the authors reported experimental setups.",
    "table_content": "|| [BOLD] Model | [BOLD] Approach | [BOLD] AIMED  [BOLD] Precision | [BOLD] AIMED  [BOLD] Recall | [BOLD] AIMED  [BOLD] F1-Score | [BOLD] BioInfer  [BOLD] Precision | [BOLD] BioInfer  [BOLD] Recall | [BOLD] BioInfer  [BOLD] F1-Score ||\n|| Baseline 1 | MLP (SDP+Feature Embedding) | 59.73 | 75.93 | 66.46 | 68.56 | 72.05 | 70.22 ||\n|| Baseline 2 | RNN (SDP+Feature Embedding) | 66.23 | 74.72 | 70.22 | 71.89 | 74.59 | 73.21 ||\n|| Proposed Model | sdpLSTM (SDP+Feature Embedding) | 91.10 | 82.2 | 86.45 | 72.40 | 83.10 | 77.35 ||\n|| hua2016shortest  | sdpCNN (SDP+CNN) | 64.80 | 67.80 | 66.00 | 73.40 | 77.00 | 75.20 ||\n|| choi2016extraction  | DCNN (CNN+word/position embeddings+ Semantic (WordNet) feature embeddings) | - | - | 85.2 | - | - | - ||\n|| choi2016extraction  ∗ | DCNN | 88.61 | 81.72 | 85.03 | 72.05 | 77.51 | 74.68 ||\n|| qian2012tree  | Single kernel+ Multiple Parser+SVM | 59.10 | 57.60 | 58.10 | 63.61 | 61.24 | 62.40 ||\n|| peng2017deep  | McDepCNN (CNN+word+PoS+Chunk+NEs Multi-channel embedding) | 67.3 | 60.1 | 63.5 | 62.7 | 68.2 | 65.3 ||\n|| zhao2016protein  | Deep neutral network | 51.5 | 63.4 | 56.1 | 53.9 | 72.9 | 61.6 ||\n|| tikk2010comprehensive  | All-path graph kernel | 49.2 | 64.6 | 55.3 | 53.3 | 70.1 | 60.0 ||\n|| li2015approach  | Multiple kernel+ Word Embedding+ SVM | - | - | 69.70 | - | - | 74.0 ||\n|| li2015approach  ∗ | Multiple kernel+ Word Embedding+ SVM | 67.18 | 69.35 | 68.25 | 72.33 | 74.94 | 73.61 ||\n|| choi2010simplicity  | Tuned tree kernels +SVM | 72.80 | 62.10 | 67.00 | 74.5 | 70.9 | 72.60 ||",
    "claim": "The proposed sdpLSTM model achieves an F1-score of 86.45 on the AIMED dataset and 77.35 on the BioInfer dataset, but it does not outperform all other models listed in the table.",
    "label": "refute"
  },
  {
    "id": "training_917_refute",
    "table_caption": "Table: Mean Reciprocal Rank (MRR) and HITS@n scores obtained for the link prediction tasks on the WN18, FB15k, WN18RR, and FB15k-237 datasets. The highest result for each column is shown in bold. The results of TransE and TorusE were reported by Ebisu and Ichise Ebisu and Ichise (2018), the results of RESCAL were reported by Nickel et al. Nickel et al. (2016), the results of DistMult and ComplEx were reported by Trouillon et al. Trouillon et al. (2016), the results of R-GCN and ConvE were reported by Dettmers et al. Dettmers et al. (2018), the results of PRA were reported by Liu et al. Liu et al. (2016), and the results of Node+LinkFeat were reported by Toutanova and Chen Toutanova and Chen (2015).",
    "table_content": "|| [EMPTY] | WN18 MRR | WN18 HITS@ | WN18 HITS@ | WN18 HITS@ | FB15k MRR | FB15k HITS@ | FB15k HITS@ | FB15k HITS@ | WN18RR MRR | WN18RR HITS@ | WN18RR HITS@ | WN18RR HITS@ | FB15k-237 MRR | FB15k-237 HITS@ | FB15k-237 HITS@ | FB15k-237 HITS@ ||\n|| Model | [EMPTY] | 1 | 3 | 10 | [EMPTY] | 1 | 3 | 10 | [EMPTY] | 1 | 3 | 10 | [EMPTY] | 1 | 3 | 10 ||\n|| TransE | 0.397 | 0.040 | 0.745 | 0.923 | 0.414 | 0.247 | 0.534 | 0.688 | 0.182 | 0.027 | 0.295 | 0.444 | 0.257 | 0.174 | 0.284 | 0.420 ||\n|| TorusE | 0.947 | 0.943 | 0.950 | 0.954 | 0.733 | 0.674 | 0.771 | 0.832 | – | – | – | – | – | – | – | – ||\n|| RESCAL | 0.890 | 0.842 | 0.904 | 0.928 | 0.354 | 0.235 | 0.409 | 0.587 | – | – | – | – | – | – | – | – ||\n|| DistMult | 0.822 | 0.728 | 0.914 | 0.936 | 0.654 | 0.546 | 0.733 | 0.824 | 0.43 | 0.39 | 0.44 | 0.49 | 0.241 | 0.155 | 0.263 | 0.419 ||\n|| ComplEx | 0.941 | 0.936 | 0.945 | 0.947 | 0.692 | 0.599 | 0.759 | 0.840 | 0.44 | 0.41 | 0.46 | 0.51 | 0.240 | 0.152 | 0.263 | 0.419 ||\n|| R-GCN | 0.814 | 0.686 | 0.928 | 0.955 | 0.651 | 0.541 | 0.736 | 0.825 | – | – | – | – | 0.248 | 0.153 | 0.258 | 0.417 ||\n|| ConvE | 0.942 | 0.935 | 0.947 | 0.955 | 0.745 | 0.670 | 0.801 | 0.873 | 0.46 | 0.39 | 0.43 | 0.48 | 0.316 | [BOLD] 0.239 | 0.350 | [BOLD] 0.491 ||\n|| PRA | 0.458 | 0.422 | – | 0.481 | 0.336 | 0.303 | – | 0.392 | – | – | – | – | – | – | – | – ||\n|| Node+LinkFeat | 0.940 | – | – | 0.943 | 0.822 | – | – | 0.870 | – | – | – | – | 0.272 | – | – | 0.414 ||\n|| GPro | [BOLD] 0.950 | [BOLD] 0.946 | [BOLD] 0.954 | [BOLD] 0.959 | 0.793 | 0.759 | 0.810 | 0.858 | 0.467 | 0.430 | [BOLD] 0.485 | [BOLD] 0.543 | 0.229 | 0.163 | 0.250 | 0.360 ||\n|| GRank (dMAP) | [BOLD] 0.950 | [BOLD] 0.946 | 0.953 | 0.957 | 0.841 | 0.814 | 0.855 | 0.890 | 0.466 | 0.434 | 0.480 | 0.530 | 0.312 | 0.233 | 0.340 | 0.473 ||\n|| GRank(fdMAP) | [BOLD] 0.950 | [BOLD] 0.946 | [BOLD] 0.954 | 0.958 | [BOLD] 0.842 | [BOLD] 0.816 | [BOLD] 0.856 | [BOLD] 0.891 | [BOLD] 0.470 | [BOLD] 0.437 | 0.482 | 0.539 | [BOLD] 0.322 | [BOLD] 0.239 | [BOLD] 0.352 | 0.489 ||",
    "claim": "GRank with fdMAP achieves the highest MRR and HITS@ scores on the WN18 dataset.",
    "label": "refute"
  },
  {
    "id": "training_924_refute",
    "table_caption": "Table: Effect in success rates of allowing for synonyms in GF",
    "table_content": "|| System | GF scores with synonyms Overall | GF scores with synonyms 10% | GF scores with synonyms 20% | GF scores without synonyms Overall | GF scores without synonyms 10% | GF scores without synonyms 20% ||\n|| Google | 0.757 | 0.711 | 0.776 | 0.592 | 0.565 | 0.619 ||\n|| Bing | [BOLD] 0.795 | [BOLD] 0.785 | [BOLD] 0.804 | [BOLD] 0.618 | [BOLD] 0.595 | [BOLD] 0.640 ||\n|| Homebrew | 0.704 | 0.711 | 0.697 | 0.550 | 0.547 | 0.553 ||\n|| Systran | 0.765 | 0.750 | 0.781 | 0.569 | 0.544 | 0.595 ||\n|| MT Average | 0.755 | 0.746 | 0.765 | 0.582 | 0.563 | 0.602 ||\n|| No hint (random) | 0.339 | 0.379 | 0.299 | 0.258 | 0.302 | 0.213 ||\n|| No hint (entropy) | 0.306 | 0.322 | 0.290 | 0.193 | 0.191 | 0.195 ||\n|| No hint (average) | 0.322 | 0.350 | 0.294 | 0.225 | 0.247 | 0.204 ||",
    "claim": "Google achieves the highest GF scores both with and without synonyms across all conditions.",
    "label": "refute"
  },
  {
    "id": "training_974_refute",
    "table_caption": "Table: Inference time for NVIDIA Tesla V100 on CoNLL-2014 (test), single model, batch size=128.",
    "table_content": "|| [BOLD] GEC system | [BOLD] Time (sec) ||\n|| Transformer-NMT, beam size = 12 | 4.35 ||\n|| Transformer-NMT, beam size = 4 | 1.25 ||\n|| Transformer-NMT, beam size = 1 | 0.71 ||\n|| GECToR (XLNet), 5 iterations | 0.40 ||\n|| GECToR (XLNet), 1 iteration | 0.20 ||",
    "claim": "GECToR (XLNet) runs slower than Transformer-NMT.",
    "label": "refute"
  },
  {
    "id": "training_921_refute",
    "table_caption": "Table: Performance comparison for the video+subtitle task on MovieQA public validation/test dataset. (–) means that the method does not participate on the task. Baselines include DEMM (Deep embedded memory network), OVQAP (Only video question answer pairs) and VCFSM (Video clip features with simple MLP).",
    "table_content": "|| Methods | Video+Subtitle val | Video+Subtitle test ||\n|| OVQAP | – | 23.61 ||\n|| Simple MLP | – | 24.09 ||\n|| LSTM + CNN | – | 23.45 ||\n|| LSTM + Discriminative CNN | – | 24.32 ||\n|| VCFSM | – | 24.09 ||\n|| DEMN  | – | 29.97 ||\n|| MEMN2N  | 34.20 | – ||\n|| RWMN-noRW | 34.20 | – ||\n|| RWMN-noR | 36.50 | – ||\n|| RWMN-noQ | 38.17 | – ||\n|| RWMN-noVid | 37.20 | – ||\n|| RWMN | [BOLD] 38.67 | [BOLD] 36.25 ||\n|| RWMN-bag | 38.37 | 35.69 ||\n|| RWMN-ensemble | 38.30 | – ||",
    "claim": "RWMN achieves the best performance on the validation set but not on the test set.",
    "label": "refute"
  },
  {
    "id": "training_975_refute",
    "table_caption": "Table: Performance of trained image captioning models. Higher BLEU-4 score means better performance.",
    "table_content": "|| Training Method | BLEU-4 | Improvement ||\n|| Dense | 31.0 | 0 ||\n|| Coarse-grained | 30.3 | -2.26% ||\n|| 3/4 coarse + 1/4 dense | 30.8 | -0.65% ||\n|| Fine-grained | 30.6 | -1.29% ||\n|| 3/4 fine + 1/4 dense | 31.1 | +0.32% ||",
    "claim": "The Coarse-grained training method achieves the highest BLEU-4 score with a +0.32% improvement.",
    "label": "refute"
  },
  {
    "id": "training_783_refute",
    "table_caption": "Table: WMT’14 English-German (En-De) and English-French (En-Fr) diversity performances in BLEU and Pairwise-BLEU scores. Lower Pairwise-BLEU means more diversity, higher BLEU means better quality.",
    "table_content": "|| [BOLD] Method | [BOLD] Pairwise-BLEU  [BOLD] En-De | [BOLD] Pairwise-BLEU  [BOLD] En-Fr | [BOLD] BLEU  [BOLD] En-De | [BOLD] BLEU  [BOLD] En-Fr ||\n|| Sampling | 24.1 | 32.0 | 37.8 | 46.5 ||\n|| Beam | 73.0 | 77.1 | 69.9 | 79.8 ||\n|| Div-beam | 53.7 | 64.9 | 60.0 | 72.5 ||\n|| hMup | 50.2 | 64.0 | 63.8 | 74.6 ||\n|| Human | 35.5 | 46.5 | 69.6 | 76.9 ||\n|| Ours | 57.1 | 70.1 | 69.5 | 77.0 ||",
    "claim": "In the En-De experiments, \"Ours\" method has a lower Pairwise-BLEU score (57.1) than \"hMup\" (50.2), indicating less diversity, but achieves a BLEU score (69.5) close to human performance (69.6).",
    "label": "refute"
  },
  {
    "id": "training_948_refute",
    "table_caption": "Table: Effect of automatically predicting language ID and POS tags with MaLOPa on LAS scores.",
    "table_content": "|| LAS | language ID | coarse POS | target language de | target language en | target language es | target language fr | target language it | target language pt | target language sv | average ||\n|| [EMPTY] | gold | gold | 78.6 | 84.2 | 83.4 | 82.4 | 89.1 | 84.2 | 82.6 | 83.5 ||\n|| [EMPTY] | predicted | gold | 78.5 | 80.2 | 83.4 | 82.1 | 88.9 | 83.9 | 82.5 | 82.7 ||\n|| [EMPTY] | gold | predicted | 71.2 | 79.9 | 80.5 | 78.5 | 85.0 | 78.4 | 75.5 | 78.4 ||\n|| [EMPTY] | predicted | predicted | 70.8 | 74.1 | 80.5 | 78.2 | 84.7 | 77.1 | 75.5 | 77.2 ||",
    "claim": "The performance of the parser decreases by 0.8 LAS points when using predicted language IDs and by 3.1 LAS points when using predicted POS tags compared to using gold data.",
    "label": "refute"
  },
  {
    "id": "training_926_refute",
    "table_caption": "Table: Average Monetary gains and Confidence scores (All Adults).",
    "table_content": "|| [EMPTY] | [BOLD] Monetary gains | [BOLD] Confidence ||\n|| Graphs only | 81.15 | 78.5% ||\n|| Multi-modal | 117.51 | 83.7% ||\n|| NLG only | 101.33 | 66% ||",
    "claim": "Multi-modal representation results in higher monetary gains, but NLG-only representation results in higher confidence compared to graphics-only and multi-modal representations.",
    "label": "refute"
  },
  {
    "id": "training_815_refute",
    "table_caption": "Table: Effect of objective function",
    "table_content": "|| Objective | Accuracy on Test, % | Accuracy on Test, % | Accuracy on Test, % | Valid value | Valid value | Valid value ||\n|| function | SwBD | MRDA | FSC | Cosine | L2 | L1 ||\n|| Cosine | 55.56 | 59.64 | 89.45 | 0.13 | 0.08 | 0.21 ||\n|| L2 | 53.73 | 59.91 | 88.64 | 0.13 | 0.07 | 0.20 ||\n|| L1 | [BOLD] 56.32 | [BOLD] 60.39 | [BOLD] 89.98 | 0.13 | 0.07 | 0.20 ||",
    "claim": "The L2 objective function yields the highest accuracy on SwBD, MRDA, and FSC datasets.",
    "label": "refute"
  },
  {
    "id": "training_931_refute",
    "table_caption": "Table: Performance of the CRF model for PoS tagging with two feature sets",
    "table_content": "|| [BOLD] Feature set | [BOLD] Test Acc.  [ITALIC] n≤20 | [BOLD] Test Acc.  [ITALIC] n≤25 | [BOLD] Training Acc.  [ITALIC] n≤20 | [BOLD] Training Acc.  [ITALIC] n≤25 ||\n|| {Word identities, word shapes} | 87.62 | 88.93 | 90.75 | 91.56 ||\n|| {Word identities, word shapes, word embeddings} | [BOLD] 88.97 | [BOLD] 90.26 | 91.64 | 92.29 ||",
    "claim": "Incorporating word embedding features reduces the test accuracy of the CRF model from 87.62% to 88.97%.",
    "label": "refute"
  },
  {
    "id": "training_952_refute",
    "table_caption": "Table: Accuracies of our question answering system. NQS and NTP stand for Neural Quiz Solver and Neural Type Predictor, respectively.",
    "table_content": "|| Name | Sent 1 | Sent 1–2 | Sent 1–3 | Full ||\n|| Full model (NQS + NTP + IR) | 0.56 | 0.78 | 0.88 | 0.97 ||\n|| NQS | 0.31 | 0.54 | 0.70 | 0.88 ||\n|| NQS + coarse-grained NTP | 0.33 | 0.56 | 0.72 | 0.89 ||\n|| NQS + fine-grained NTP | 0.33 | 0.57 | 0.73 | 0.89 ||\n|| NQS + NTP | 0.34 | 0.57 | 0.73 | 0.89 ||\n|| NQS + NTP + IR-Wikipedia | 0.48 | 0.71 | 0.84 | 0.95 ||\n|| NQS + NTP + IR-Dataset | 0.49 | 0.73 | 0.86 | 0.96 ||",
    "claim": "The full model (NQS + NTP + IR) achieves 56% accuracy with a single sentence and 96% accuracy with the full set of sentences.",
    "label": "refute"
  },
  {
    "id": "training_822_refute",
    "table_caption": "Table: Quadratic Weighted Kappa (QWK) of different AES models. The CO-ATTN model significantly outperforms the Rubric and SG models, respectively (p<0.05).",
    "table_content": "|| [BOLD] AES Model | [BOLD] QWK ||\n|| Rubric | 0.632 ||\n|| SG | 0.653 ||\n|| CO-ATTN | 0.697 ||",
    "claim": "The SG model has the best performance with a QWK score of 0.697.",
    "label": "refute"
  },
  {
    "id": "training_944_refute",
    "table_caption": "Table: WER comparison on CHiME-4 single-channel track evaluation sets with adversarial examples (AdvEx) (ϵ=0.1).",
    "table_content": "|| system | et05_simu BUS | et05_simu CAF | et05_simu PED | et05_simu STR | et05_simu  [BOLD] AVE. | et05_real BUS | et05_real CAF | et05_real PED | et05_real STR | et05_real  [BOLD] AVE. ||\n|| Baseline | 20.25 | 30.69 | 26.62 | 28.74 | 26.57 | 43.95 | 33.64 | 25.95 | 18.68 | 30.55 ||\n|| AdvEx | 19.65 | 29.29 | 24.75 | 26.95 | [BOLD] 25.16 | 41.00 | 31.34 | 24.74 | 18.23 | [BOLD] 28.82 ||",
    "claim": "AdvEx achieves lower average WER than the Baseline on the et05_simu set but higher average WER on the et05_real set.",
    "label": "refute"
  },
  {
    "id": "training_888_refute",
    "table_caption": "Table: Test accuracy on SST-5 and IMDB. In BPT, k=2 and k=4 for SST and IMDB respectively. The last model used word embeddings pretrained with translation and additional character-level embeddings.",
    "table_content": "|| Model | SST-5 | IMDB ||\n|| [BOLD] BPT | 52.71(0.32) | [BOLD] 92.12(0.11) ||\n|| Star Transformer | 52.9 | 90.50 ||\n|| Transformer | 50.4 | 89.24 ||\n|| Bi-LSTM (li2015tree) | 49.8 | - ||\n|| Tree-LSTM (socher2013recursive) | 51.0 | - ||\n|| QRNN DBLP:conf/iclr/0002MXS17 | - | 91.4 ||\n|| BCN+Char+CoVe mccann2017learned | 53.7 | 91.8 ||",
    "claim": "On SST-5, BPT outperforms Transformer and LSTM-based models. On IMDB, BPT does not outperform BCN+Char+CoVe.",
    "label": "refute"
  },
  {
    "id": "training_933_refute",
    "table_caption": "Table: Performances of NER systems at VLSP 2016",
    "table_content": "|| [BOLD] Team | [BOLD] Model | [BOLD] Performance ||\n|| Le-Hong  | ME | 88.78 ||\n|| [Anonymous] | CRF | 86.62 ||\n|| Nguyen et al.  | ME | 84.08 ||\n|| Nguyen et al.  | LSTM | 83.80 ||\n|| Le et al.  | CRF | 78.40 ||",
    "claim": "Le-Hong's ME model does not achieve the highest performance, with a score of 88.78.",
    "label": "refute"
  },
  {
    "id": "training_830_refute",
    "table_caption": "Table: Cross-lingual keyword spotting results (%) on test data.",
    "table_content": "|| Model | [ITALIC] P@10 | [ITALIC] P@ [ITALIC] N | EER | AP ||\n|| DETextPrior | 07.2 | 06.3 | 50 | 10.4 ||\n|| DEVisionCNN | 41.5 | 32.9 | 25.9 | 29.7 ||\n|| XVisionSpeechCNN | 58.2 | 40.4 | 23.5 | 40.0 ||\n|| XBoWCNN | 80.8 | 54.3 | 19.1 | 54.3 ||",
    "claim": "DEVisionCNN outperforms XVisionSpeechCNN across all metrics, and XBoWCNN is the only model to achieve a higher P@10 than XVisionSpeechCNN.",
    "label": "refute"
  },
  {
    "id": "training_840_refute",
    "table_caption": "Table: Pearson’s r of EyeScore for different feature sets with MET (training/development set, 88 participants) and TOEFL (all 53 participants). Fixed denotes the Fixed Text regime in which all the participants read the same sentences, and Any denotes the Any Text regime where different readers read different sentences.",
    "table_content": "|| [BOLD] Features | [BOLD] MET Fixed | [BOLD] MET Any | [BOLD] TOEFL Fixed | [BOLD] TOEFL Any ||\n|| Reading Speed | 0.28 | 0.27 | 0.15 | 0.13 ||\n|| WP-Coefficients | 0.38 | 0.37 | 0.21 | 0.13 ||\n|| S-Clusters | 0.45 | [BOLD] 0.48 | 0.50 | [BOLD] 0.45 ||\n|| Transitions | 0.45 | [EMPTY] | 0.44 | [EMPTY] ||\n|| WFC | [BOLD] 0.50 | [EMPTY] | [BOLD] 0.54 | [EMPTY] ||",
    "claim": "The WFC feature set achieves the highest Pearson's r value of 0.54 for MET Any, and the S-Clusters feature set achieves the highest Pearson's r value of 0.50 for MET Any.",
    "label": "refute"
  },
  {
    "id": "training_920_refute",
    "table_caption": "Table: Results on languages not in the training corpus",
    "table_content": "|| Model | WER | WER 100 | PER ||\n|| LangID-High | [BOLD] 85.94 | 58.10 | [BOLD] 53.06 ||\n|| LangID-Adapted | 87.78 | 68.40 | 65.62 ||\n|| LangID-All | 86.27 | 62.31 | 54.33 ||\n|| NoLangID-High | 88.52 | 58.21 | 62.02 ||\n|| NoLangID-Adapted | 91.27 | 57.61 | 74.07 ||\n|| NoLangID-All | 89.96 | [BOLD] 56.29 | 62.79 ||",
    "claim": "NoLangID models have a slight advantage over LangID models in WER and PER.",
    "label": "refute"
  },
  {
    "id": "training_849_refute",
    "table_caption": "Table: Accuracies for the natural language inference task. Shown are our implementations of a system without attention, and with logistic, soft, and sparse attentions.",
    "table_content": "|| [EMPTY] | Dev Acc. | Test Acc. ||\n|| NoAttention | 81.84 | 80.99 ||\n|| LogisticAttention | 82.11 | 80.84 ||\n|| SoftAttention | 82.86 | 82.08 ||\n|| SparseAttention | 82.52 | [BOLD] 82.20 ||",
    "claim": "NoAttention has the highest test accuracy, and both SoftAttention and SparseAttention outperform NoAttention and LogisticAttention systems.",
    "label": "refute"
  },
  {
    "id": "training_922_refute",
    "table_caption": "Table: Performance of the RWMN on the video+subtitle task, according to the structure parameters of write/read networks. νw/r: the number of layers for write/read networks, (fw/rvi,sw/rvi,fw/rci): the height and the stride of convolution filters, and the number of output channels.",
    "table_content": "|| # Layers  [ITALIC] νw | # Layers  [ITALIC] νr | Write network ( [ITALIC] fwvi, [ITALIC] swvi, [ITALIC] fwci) | Read network ( [ITALIC] frvi, [ITALIC] srvi, [ITALIC] fwri) | Acc. ||\n|| 0 | 0 | – | – | 34.2 ||\n|| 1 | 0 | (40,7,1) | – | 33.9 ||\n|| 1 | 0 | (40,30,3) | – | 36.5 ||\n|| 1 | 1 | (40,30,3) | (3,1,1) | [BOLD] 38.6 ||\n|| 1 | 1 | (40,60,3) | (3,1,1) | 33.6 ||\n|| 2 | 1 | (40,10,3), (10,5,3) | (3,1,1) | 37.2 ||\n|| 2 | 1 | (5,3,1), (5,3,1) | (3,1,1) | 37.3 ||\n|| 2 | 2 | (4,2,1), (4,2,1) | (3,1,1), (3,1,1) | 36.9 ||\n|| 2 | 2 | (4,2,1), (4,2,1) | (4,2,1), (4,2,1) | 37.3 ||\n|| 3 | 1 | (10,3,3), (40,3,3), (100,3,3) | (3,1,1) | 35.1 ||\n|| 3 | 1 | (40,3,3), (10,3,3), (10,3,3) | (3,1,1) | 37.9 ||\n|| 3 | 1 | (40,3,3), (40,3,3), (40,3,3) | (3,1,1) | 35.7 ||\n|| 3 | 1 | (100,3,3), (40,3,3), (10,3,3) | (3,1,1) | 35.8 ||",
    "claim": "The highest accuracy of 38.6 is achieved with a configuration of 2 write layers and 1 read layer using (40,30,3) for the write network and (3,1,1) for the read network.",
    "label": "refute"
  },
  {
    "id": "training_966_refute",
    "table_caption": "Table: Effect of different models on canonical/non-canonical words.",
    "table_content": "|| [EMPTY] | Bilty | +Norm | +Vecs | +Comb ||\n|| canonical | 86.1 | 85.6 | 91.2 | 90.1 ||\n|| non-canon. | 50.8 | 70.3 | 71.1 | 78.5 ||",
    "claim": "The +Norm model achieves the highest accuracy on non-canonical tokens, while the +Comb model achieves the highest accuracy on canonical tokens.",
    "label": "refute"
  },
  {
    "id": "training_973_refute",
    "table_caption": "Table: Comparison of single models and ensembles. The M2 score for CoNLL-2014 (test) and ERRANT for the BEA-2019 (test) are reported. In ensembles we simply average output probabilities from single models.",
    "table_content": "|| [BOLD] GEC system | [BOLD] Ens. | [BOLD] CoNLL-2014 (test)  [BOLD] P | [BOLD] CoNLL-2014 (test)  [BOLD] R | [BOLD] CoNLL-2014 (test)  [BOLD] F0.5 | [BOLD] BEA-2019 (test)  [BOLD] P | [BOLD] BEA-2019 (test)  [BOLD] R | [BOLD] BEA-2019 (test)  [BOLD] F0.5 ||\n|| Zhao et al. ( 2019 ) | [EMPTY] | 67.7 | 40.6 | 59.8 | - | - | - ||\n|| Awasthi et al. ( 2019 ) | [EMPTY] | 66.1 | 43.0 | 59.7 | - | - | - ||\n|| Kiyono et al. ( 2019 ) | [EMPTY] | 67.9 | [BOLD] 44.1 | 61.3 | 65.5 | [BOLD] 59.4 | 64.2 ||\n|| Zhao et al. ( 2019 ) | ✓ | 74.1 | 36.3 | 61.3 | - | - | - ||\n|| Awasthi et al. ( 2019 ) | ✓ | 68.3 | 43.2 | 61.2 | - | - | - ||\n|| Kiyono et al. ( 2019 ) | ✓ | 72.4 | [BOLD] 46.1 | 65.0 | 74.7 | 56.7 | 70.2 ||\n|| Kantor et al. ( 2019 ) | ✓ | - | - | - | 78.3 | 58.0 | 73.2 ||\n|| GECToR (BERT) | [EMPTY] | 72.1 | 42.0 | 63.0 | 71.5 | 55.7 | 67.6 ||\n|| GECToR (RoBERTa) | [EMPTY] | 73.9 | 41.5 | 64.0 | 77.2 | 55.1 | 71.5 ||\n|| GECToR (XLNet) | [EMPTY] | [BOLD] 77.5 | 40.1 | [BOLD] 65.3 | [BOLD] 79.2 | 53.9 | [BOLD] 72.4 ||\n|| GECToR (RoBERTa + XLNet) | ✓ | 76.6 | 42.3 | 66.0 | [BOLD] 79.4 | 57.2 | [BOLD] 73.7 ||\n|| GECToR (BERT + RoBERTa + XLNet) | ✓ | [BOLD] 78.2 | 41.5 | [BOLD] 66.5 | 78.9 | [BOLD] 58.2 | 73.6 ||",
    "claim": "GECToR (XLNet) achieves F0.5 = 64.0 on CoNLL-2014 (test) and F0.5 = 72.4 on BEA-2019 (test).",
    "label": "refute"
  },
  {
    "id": "training_934_refute",
    "table_caption": "Table: Experiments with WMT 2017 data, correcting a phrase-base system.",
    "table_content": "|| Model | dev 2016  [BOLD] TER↓ | dev 2016 BLEU↑ | test 2016  [BOLD] TER↓ | test 2016 BLEU↑ | test 2017  [BOLD] TER↓ | test 2017 BLEU↑ ||\n|| Uncorrected | 24.81 | 62.92 | 24.76 | 62.11 | 24.48 | 62.49 ||\n|| WMT17: FBK Primary | 19.22 | 71.89 | 19.32 | 70.88 | 19.60 | 70.07 ||\n|| WMT17: AMU Primary | — | — | 19.21 | 70.51 | 19.77 | 69.50 ||\n|| Baseline (single model) | 19.77 | 70.54 | 20.10 | 69.25 | 20.43 | 68.48 ||\n|| +Tied embeddings | 19.39 | 70.70 | 19.82 | 68.87 | 20.09 | 69.06 ||\n|| +Shared encoder | 19.23 | 71.14 | 19.44 | 70.06 | 20.15 | 69.04 ||\n|| Transformer-base (Tied+Shared) | 18.73 | 71.71 | 18.92 | 70.86 | 19.49 | 69.72 ||\n|| Transformer-base x4 | 18.22 | 72.34 | 18.86 | 71.04 | 19.03 | 70.46 ||",
    "claim": "The Transformer-base x4 model does not achieve the best performance on all datasets, as it has higher TER and lower BLEU scores compared to other models on at least one dataset.",
    "label": "refute"
  },
  {
    "id": "training_880_refute",
    "table_caption": "Table: Performance of different variants of risk indicator injection",
    "table_content": "|| Model | Accuracy | Precision | Recall | F1 ||\n|| BiLSTM | 0.8208 | 0.8207 | 0.8208 | 0.8195 ||\n|| BiLSTM+concat | 0.8167 | 0.8262 | 0.8167 | 0.8190 ||\n|| BiLSTM+RN+sentiment | 0.8240 | 0.8246 | 0.8240 | 0.8239 ||\n|| BiLSTM+RN+topic | 0.8198 | 0.8177 | 0.8198 | 0.8183 ||\n|| BiLSTM+RN+sent.+topic | [BOLD] 0.8385 | [BOLD] 0.8381 | [BOLD] 0.8385 | [BOLD] 0.8377 ||",
    "claim": "The BiLSTM+RN+sentiment model achieves the highest performance across all metrics (accuracy, precision, recall, F1) compared to other variants.",
    "label": "refute"
  },
  {
    "id": "training_946_refute",
    "table_caption": "Table: Our experiment results based on BTEC Japanese-English speech-to-speech translation.",
    "table_content": "|| [BOLD] Model (JA-EN)  [BOLD] Baseline Tacotron with MFCC source | [BOLD] Model (JA-EN)  [BOLD] Baseline Tacotron with MFCC source | [BOLD] BLEU - | [BOLD] METEOR - ||\n|| [BOLD] Proposed Speech2Code | [BOLD] Proposed Speech2Code | [BOLD] Proposed Speech2Code | [BOLD] Proposed Speech2Code ||\n|| Codebook | Time Reduction | [EMPTY] | [EMPTY] ||\n|| 32 | 4 | 14.8 | 15 ||\n|| 32 | 8 | 14.2 | 15.6 ||\n|| 32 | 12 | 16 | 16 ||\n|| 64 | 4 | 10.8 | 12.1 ||\n|| 64 | 8 | 14.2 | 14.7 ||\n|| 64 | 12 | 14.7 | 14.8 ||\n|| 128 | 4 | 11.9 | 13.5 ||\n|| 128 | 8 | 15.3 | 15.3 ||\n|| 128 | 12 | 14.9 | 14.5 ||\n|| [BOLD] Topline (Cascade ASR ->TTS) | [BOLD] Topline (Cascade ASR ->TTS) | 37.4 | 32.8 ||",
    "claim": "The best performance was achieved with a codebook size of 128 and a time-reduction factor of 12, resulting in a BLEU score of 15.3 and a METEOR score of 15.3.",
    "label": "refute"
  },
  {
    "id": "training_977_refute",
    "table_caption": "Table: Performance of trained NMT models. Higher BLEU score means better performance.",
    "table_content": "|| Training Method | BLEU | Improvement ||\n|| Dense | 20.32 | 0 ||\n|| Coarse-grained | 19.60 | -3.5% ||\n|| 3/4 coarse + 1/4 dense | 20.30 | -0.1% ||\n|| 5/6 coarse + 1/6 dense | 20.18 | -0.7% ||\n|| Fine-grained | 19.92 | -2.0% ||\n|| 3/4 fine + 1/4 dense | 20.45 | +0.64% ||\n|| 5/6 fine + 1/6 dense | 20.17 | -0.74% ||",
    "claim": "The coarse-grained method suffers a 3.5% BLEU score decrease compared to the dense method, while the fine-grained method has a 2.0% increase.",
    "label": "refute"
  },
  {
    "id": "training_928_refute",
    "table_caption": "Table: Comparison between different number of attended cells (percentage of the whole input), and aggregation operation. We consider a simple summation, and non-local pairwise computations as the aggregation tool.",
    "table_content": "|| [EMPTY] | Percentage | Overall | Yes/No | Number | Other ||\n|| [EMPTY] | of cells | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| HAN+sum | 16% | 26.99 | 40.53 | 11.38 | 24.15 ||\n|| HAN+sum | 32% | 27.43 | 41.05 | 11.38 | 24.68 ||\n|| HAN+sum | 48% | 27.94 | 41.35 | 11.93 | 25.27 ||\n|| HAN+sum | 64% | 27.80 | 40.74 | 11.29 | 25.52 ||\n|| sum | 100% | 27.96 | 43.23 | 12.09 | 24.29 ||\n|| HAN+pairwise | 16% | 26.81 | 41.24 | 10.87 | 23.61 ||\n|| HAN+pairwise | 32% | 27.45 | 40.91 | 11.48 | 24.75 ||\n|| HAN+pairwise | 48% | 28.23 | 41.23 | 11.40 | 25.98 ||\n|| Pairwise | 100% | 28.06 | 44.10 | 13.20 | 23.71 ||\n|| SAN  | - | 24.96 | 38.35 | 11.14 | 21.74 ||\n|| SAN (ours) | - | 26.60 | 39.69 | 11.25 | 23.92 ||\n|| SAN+pos (ours) | - | 27.77 | 40.73 | 11.31 | 25.47 ||\n|| GVQA  | - | 31.30 | 57.99 | 13.68 | 22.14 ||",
    "claim": "Performance is almost the same with less than half of the units attended, and SAN on its own performs better than non-local pairwise aggregation.",
    "label": "refute"
  },
  {
    "id": "training_964_refute",
    "table_caption": "Table: Performance on Named Entity Recognition and Part-of-Speech Tagging tasks. All methods are evaluated on test data containing OOV words. Results demonstrate that the proposed approach, HiCE + Morph + MAML, improves the downstream model by learning better representations for OOV words.",
    "table_content": "|| Methods | Named Entity Recognition (F1-score) Rare-NER | Named Entity Recognition (F1-score) Bio-NER | POS Tagging (Acc) Twitter POS ||\n|| Word2vec | 0.1862 | 0.7205 | 0.7649 ||\n|| FastText | 0.1981 | 0.7241 | 0.8116 ||\n|| Additive | 0.2021 | 0.7034 | 0.7576 ||\n|| nonce2vec | 0.2096 | 0.7289 | 0.7734 ||\n|| ` [ITALIC] a la carte | 0.2153 | 0.7423 | 0.7883 ||\n|| HiCE w/o Morph | 0.2394 | 0.7486 | 0.8194 ||\n|| HiCE + Morph | 0.2375 | 0.7522 | 0.8227 ||\n|| HiCE + Morph + MAML | [BOLD] 0.2419 | [BOLD] 0.7636 | [BOLD] 0.8286 ||",
    "claim": "HiCE + Morph + MAML achieves the highest F1-scores for Rare-NER and Bio-NER, but not the highest accuracy for Twitter POS compared to all other methods.",
    "label": "refute"
  },
  {
    "id": "training_959_refute",
    "table_caption": "Table: Performance on AAPD",
    "table_content": "|| Models (a) Seq2set (simp.) | Models (a) Seq2set (simp.) | maF1 | miF1 | ebF1 | ACC | HA | Average ||\n||  |  | - | 0.705 | - | - | 0.9753 | - ||\n|| (b) Seq2set | (b) Seq2set | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n||  |  | - | 0.698 | - | - | 0.9751 | - ||\n|| (c) SGM+GE | (c) SGM+GE | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n||  |  | - | 0.710 | - | - | 0.9755 | - ||\n|| Baselines | Baselines | Baselines | Baselines | Baselines | Baselines | Baselines | Baselines ||\n|| (d) BR | (d) BR | 0.523 | 0.694 | 0.695 | 0.368 | 0.9741 | 0.651 ||\n|| (e) BR++ | (e) BR++ | 0.521 | 0.700 | 0.703 | 0.390 | 0.9750 | 0.658 ||\n|| (f) Seq2seq | (f) Seq2seq | 0.511 | 0.695 | 0.707 | [BOLD] 0.421 | 0.9743 | 0.662 ||\n|| (g) Seq2seq + SS | (g) Seq2seq + SS | 0.541 | 0.703 | 0.713 | 0.406 | 0.9742 | 0.667 ||\n|| (h) Order-free RNN | (h) Order-free RNN | 0.539 | 0.696 | 0.708 | 0.413 | 0.9742 | 0.666 ||\n|| (i) Order-free RNN + SS | (i) Order-free RNN + SS | 0.548 | 0.699 | 0.709 | 0.416 | 0.9743 | 0.669 ||\n|| Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods ||\n|| (j) OCD | (j) OCD | 0.541 | 0.707 | 0.723 | 0.403 | 0.9740 | 0.670 ||\n|| OCD + MTL | (k) RNN dec. | 0.578 | 0.711 | 0.727 | 0.391 | 0.9742 | 0.676 ||\n|| OCD + MTL | (l) BR dec. | 0.562 | 0.711 | 0.718 | 0.382 |  [BOLD] 0.9760 | 0.670 ||\n|| OCD + MTL | (m) Logistic rescore |  [BOLD] 0.585 |  [BOLD] 0.720 |  [BOLD] 0.736 | 0.395 | 0.9749 |  [BOLD] 0.682 ||\n|| OCD + MTL | (n) Logistic joint dec. | 0.580 | 0.719 | 0.731 | 0.399 | 0.9753 | 0.681 ||",
    "claim": "OCD performs worse than all the baselines (BR, BR++, Seq2seq, Seq2seq + SS, Order-free RNN, Order-free RNN + SS) in terms of the average metric score.",
    "label": "refute"
  },
  {
    "id": "training_859_refute",
    "table_caption": "Table: ROUGE F1 and METEOR scores (non-coverage) on CNN/Daily Mail test set of previous works and our models. ‘pg’ is the pointer-generator baseline, and ‘pg + cbdec’ is our 2-decoder model with closed-book decoder(cbdec). The model marked with ⋆ is trained and evaluated on the anonymized version of the data.",
    "table_content": "|| [EMPTY] | ROUGE 1 | ROUGE 2 | ROUGE L | MTR Full ||\n|| previous works | previous works | previous works | previous works | previous works ||\n|| ⋆(Nallapati16) | 35.46 | 13.30 | 32.65 | [EMPTY] ||\n|| pg (See17) | 36.44 | 15.66 | 33.42 | 16.65 ||\n|| our models | our models | our models | our models | our models ||\n|| pg (baseline) | 36.70 | 15.71 | 33.74 | 16.94 ||\n|| pg + cbdec | 38.21 | 16.45 | 34.70 | 18.37 ||\n|| RL + pg | 37.02 | 15.79 | 34.00 | 17.55 ||\n|| RL + pg + cbdec | [BOLD] 38.58 | [BOLD] 16.57 | [BOLD] 35.03 | [BOLD] 18.86 ||",
    "claim": "The RL + pg + cbdec model underperforms the pg (baseline) model in all metrics (ROUGE 1, ROUGE 2, ROUGE L, MTR Full).",
    "label": "refute"
  },
  {
    "id": "training_961_refute",
    "table_caption": "Table: Performance comparisons on Audio set.",
    "table_content": "|| Models Baselines | Models Baselines | maF1 Baselines | miF1 Baselines | ebF1 Baselines | ACC Baselines | HA Baselines | Average Baselines ||\n|| BR | BR | 0.349 | 0.480 | 0.416 | 0.086 | [BOLD] 0.9957 | 0.465 ||\n|| Seq2seq | Seq2seq | 0.345 | 0.448 | 0.421 | [BOLD] 0.140 | 0.9942 | 0.470 ||\n|| Seq2seq + SS | Seq2seq + SS | 0.340 | 0.448 | 0.419 | 0.137 | 0.9943 | 0.468 ||\n|| Order-free RNN | Order-free RNN | 0.310 | 0.438 | 0.410 | 0.096 | 0.9940 | 0.450 ||\n|| Order-free RNN + SS | Order-free RNN + SS | 0.310 | 0.437 | 0.408 | 0.095 | 0.9947 | 0.449 ||\n|| Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods ||\n|| OCD | OCD | 0.353 | 0.465 | 0.435 | 0.117 | 0.9941 | 0.473 ||\n|| OCD + MTL | RNN dec. | 0.359 | 0.466 | 0.438 | 0.115 | 0.9940 | 0.474 ||\n|| OCD + MTL | BR dec. | 0.353 | 0.485 | 0.420 | 0.075 | 0.9950 | 0.466 ||\n|| OCD + MTL | Logistic rescore |  [BOLD] 0.378 | 0.487 |  [BOLD] 0.456 | 0.096 | 0.9940 | 0.482 ||\n|| OCD + MTL | Logistic joint dec. | 0.377 |  [BOLD] 0.488 | 0.454 | 0.119 | 0.9945 |  [BOLD] 0.487 ||",
    "claim": "OCD + MTL with Logistic joint decoding achieves the highest maF1 and HA among all models.",
    "label": "refute"
  },
  {
    "id": "training_963_refute",
    "table_caption": "Table: Performance comparisons on Reuters-21578 with more than one label.",
    "table_content": "|| Models Baselines | Models Baselines | maF1 Baselines | miF1 Baselines | ebF1 Baselines | ACC Baselines | HA Baselines | Average Baselines ||\n|| BR | BR | 0.315 | 0.706 | 0.712 | 0.365 | 0.9850 | 0.617 ||\n|| Seq2seq | Seq2seq | 0.316 | 0.712 | 0.718 | 0.405 | 0.9855 | 0.627 ||\n|| Seq2seq+SS | Seq2seq+SS | 0.325 | 0.718 | 0.722 | 0.380 | 0.9859 | 0.626 ||\n|| Order-free RNN | Order-free RNN | 0.331 | 0.730 | 0.735 | 0.425 | 0.9862 | 0.641 ||\n|| Order-free RNN + SS | Order-free RNN + SS | 0.324 | 0.699 | 0.711 | 0.400 | 0.9849 | 0.624 ||\n|| Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods | Proposed methods ||\n|| OCD | OCD | 0.319 | 0.734 | 0.741 | 0.415 | 0.9864 | 0.639 ||\n|| OCD + MTL | RNN dec. | 0.335 | 0.745 | 0.749 |  [BOLD] 0.440 |  [BOLD] 0.9870 | 0.651 ||\n|| OCD + MTL | BR dec. | 0.322 | 0.739 | 0.737 | 0.430 | 0.9869 | 0.643 ||\n|| OCD + MTL | Logistic rescore | 0.337 |  [BOLD] 0.750 |  [BOLD] 0.752 | 0.435 | 0.9869 |  [BOLD] 0.652 ||\n|| OCD + MTL | Logistic joint dec. |  [BOLD] 0.342 | 0.743 | 0.746 | 0.435 |  [BOLD] 0.9870 | 0.651 ||",
    "claim": "Order-free RNN achieves the highest maF1 score of 0.342, and OCD + MTL with Logistic rescore achieves the highest miF1 score of 0.750 and ebF1 score of 0.752 among all models.",
    "label": "refute"
  },
  {
    "id": "training_912_refute",
    "table_caption": "Table: F1 scores on CoNLL-2002.",
    "table_content": "|| [BOLD] Models | [ITALIC] F1 ||\n||  | 82.95 ||\n||  | 85.75 ||\n||  | 85.77 ||\n||  | 86.68 ± 0.35 ||\n|| Ours | [BOLD] 87.08 ± 0.16 ||",
    "claim": "Our model achieves an F1 score of 87.08, surpassing the previous best score by 0.6.",
    "label": "refute"
  },
  {
    "id": "training_891_refute",
    "table_caption": "Table: WERs (%) of binary weight networks on WSJ1. The number of hidden units is 1024 for experiments in this table.",
    "table_content": "|| Model | Input | Softmax | dev93 | eval92 ||\n|| Baseline | — | — | 6.8 | 3.8 ||\n|| Binary weights ( [ITALIC] p=0) | fixed | fixed | 7.7 | 4.8 ||\n|| Binary weights ( [ITALIC] p=.001) | fixed | fixed | 8.0 | 4.5 ||\n|| Binary weights ( [ITALIC] p=.01) | fixed | fixed | 8.0 | 4.4 ||\n|| Binary weights ( [ITALIC] p=0) | real | real | 10.4 | 6.7 ||\n|| Binary weights ( [ITALIC] p=0) | binary | fixed | 12.0 | 7.3 ||\n|| Binary weights ( [ITALIC] p=0) | binary | binary | 19.0 | 12.0 ||",
    "claim": "Using binary weights for both the input and softmax layers results in the highest accuracy on both dev93 and eval92 sets.",
    "label": "refute"
  },
  {
    "id": "training_867_refute",
    "table_caption": "Table: The response generation performance when we vary different distant supervision signals. This table shows the results for the setting “k’=3”, where there are 3 positive response candidates for each conversation context. “SentBLEU” denotes using sentence-level BLEU scores as distant supervision signals.",
    "table_content": "|| Model Supervision | HybridNCM-RS BLEU | HybridNCM-RS ROUGE-L | HybridNCM-RSF BLEU | HybridNCM-RSF ROUGE-L ||\n|| BLEU-1 | [BOLD] 1.3450 | [BOLD] 10.4078 | [BOLD] 1.3695 | [BOLD] 10.3445 ||\n|| BLEU-2 | 1.1165 | 10.1584 | 0.8239 | 9.8575 ||\n|| ROUGE-L | 1.1435 | 10.0928 | 0.9838 | 9.7961 ||\n|| SentBLEU | 0.8326 | 9.2887 | 1.0631 | 9.6338 ||",
    "claim": "BLEU-2 achieves the highest scores among the supervision signals for both HybridNCM-RS and HybridNCM-RSF models.",
    "label": "refute"
  },
  {
    "id": "training_996_refute",
    "table_caption": "Table: Error rates of our models on 16 datasets against vanilla multi-task learning. ϕ (Books) means that we transfer the knowledge of the other 15 tasks to the target task Books.",
    "table_content": "|| [BOLD] Source Tasks | [BOLD] Single Task LSTM | [BOLD] Single Task BiLSTM | [BOLD] Single Task sLSTM | [BOLD] Single Task Avg. | [BOLD] Transfer Models SP-MTL-SC | [BOLD] Transfer Models SP-MTL-BC | [BOLD] Transfer Models ASP-MTL-SC | [BOLD] Transfer Models ASP-MTL-BC ||\n|| [ITALIC] ϕ (Books) | 20.5 | 19.0 | 18.0 | 19.2 | 17.8(−1.4) | 16.3(−2.9) | 16.8(−2.4) | 16.3(−2.9) ||\n|| [ITALIC] ϕ (Electronics) | 19.5 | 21.5 | 23.3 | 21.4 | 15.3(−6.1) | 14.8(−6.6) | 17.8(−3.6) | 16.8(−4.6) ||\n|| [ITALIC] ϕ (DVD) | 18.3 | 19.5 | 22.0 | 19.9 | 14.8(−5.1) | 15.5(−4.4) | 14.5(−5.4) | 14.3(−5.6) ||\n|| [ITALIC] ϕ (Kitchen) | 22.0 | 18.8 | 19.5 | 20.1 | 15.0(−5.1) | 16.3(−3.8) | 16.3(−3.8) | 15.0(−5.1) ||\n|| [ITALIC] ϕ (Apparel) | 16.8 | 14.0 | 16.3 | 15.7 | 14.8(−0.9) | 12.0(−3.7) | 12.5(−3.2) | 13.8(−1.9) ||\n|| [ITALIC] ϕ (Camera) | 14.8 | 14.0 | 15.0 | 14.6 | 13.3(−1.3) | 12.5(−2.1) | 11.8(−2.8) | 10.3(−4.3) ||\n|| [ITALIC] ϕ (Health) | 15.5 | 21.3 | 16.5 | 17.8 | 14.5(−3.3) | 14.3(−3.5) | 12.3(−5.5) | 13.5(−4.3) ||\n|| [ITALIC] ϕ (Music) | 23.3 | 22.8 | 23.0 | 23.0 | 20.0(−3.0) | 17.8(−5.2) | 17.5(−5.5) | 18.3(−4.7) ||\n|| [ITALIC] ϕ (Toys) | 16.8 | 15.3 | 16.8 | 16.3 | 13.8(−2.5) | 12.5(−3.8) | 13.0(−3.3) | 11.8(−4.5) ||\n|| [ITALIC] ϕ (Video) | 18.5 | 16.3 | 16.3 | 17.0 | 14.3(−2.7) | 15.0(−2.0) | 14.8(−2.2) | 14.8(−2.2) ||\n|| [ITALIC] ϕ (Baby) | 15.3 | 16.5 | 15.8 | 15.9 | 16.5(+0.6) | 16.8(+0.9) | 13.5(−2.4) | 12.0(−3.9) ||\n|| [ITALIC] ϕ (Magazines) | 10.8 | 8.5 | 12.3 | 10.5 | 10.5(+0.0) | 10.3(−0.2) | 8.8(−1.7) | 9.5(−1.0) ||\n|| [ITALIC] ϕ (Software) | 15.3 | 14.3 | 14.5 | 14.7 | 13.0(−1.7) | 12.8(−1.9) | 14.5(−0.2) | 11.8(−2.9) ||\n|| [ITALIC] ϕ (Sports) | 18.3 | 16.0 | 17.5 | 17.3 | 16.3(−1.0) | 16.3(−1.0) | 13.3(−4.0) | 13.5(−3.8) ||\n|| [ITALIC] ϕ (IMDB) | 18.3 | 15.0 | 18.5 | 17.3 | 12.8(−4.5) | 12.8(−4.5) | 12.5(−4.8) | 13.3(−4.0) ||\n|| [ITALIC] ϕ (MR) | 27.3 | 25.3 | 28.0 | 26.9 | 26.0(−0.9) | 26.5(−0.4) | 24.8(−2.1) | 23.5(−3.4) ||\n|| AVG | 18.2 | 17.4 | 18.3 | 18.0 | 15.6(−2.4) | 15.2(−2.8) | 14.7(−3.3) | 14.3(−3.7) ||",
    "claim": "The SP-MTL models achieve better average performance compared to ASP-MTL models, and within each type, the Bi-Channel model performs better than the Single-Channel model.",
    "label": "refute"
  },
  {
    "id": "training_914_refute",
    "table_caption": "Table: Ablation study on the three benchmark datasets.",
    "table_content": "|| [EMPTY] | CoNLL03 | CoNLL02 | OntoNotes ||\n|| base model | 91.60 | 86.65 | 87.58 ||\n|| + sentence-level | 91.80 | 86.95 | 87.86 ||\n|| + document-level | 91.79 | 86.76 | 87.81 ||\n|| + ALL | [BOLD] 91.96 | [BOLD] 87.08 | [BOLD] 87.98 ||",
    "claim": "Combining sentence-level and document-level representations (+ ALL) results in the highest performance for CoNLL03 and CoNLL02, but not for OntoNotes.",
    "label": "refute"
  },
  {
    "id": "training_981_refute",
    "table_caption": "Table: Different λ’s effect on our model performance. The compared models are trained for 2000 epochs",
    "table_content": "|| [BOLD] Method | [BOLD] WN18 MR | [BOLD] WN18 H10 | [BOLD] FB15k MR | [BOLD] FB15k H10 ||\n|| [ITALIC] λ=0.0003 | [BOLD] 217 | 95.0 | [BOLD] 68 | 80.4 ||\n|| [ITALIC] λ=0.001 | 223 | [BOLD] 95.2 | 73 | 80.6 ||\n|| [ITALIC] λ=0.003 | 239 | [BOLD] 95.2 | 82 | [BOLD] 80.9 ||",
    "claim": "As λ increases, Hits@10 decreases for FB15k, while mean rank also increases for both WN18 and FB15k.",
    "label": "refute"
  },
  {
    "id": "training_984_refute",
    "table_caption": "Table: Different methods to obtain sparse representations",
    "table_content": "|| [BOLD] Method | [BOLD] WN18 MR | [BOLD] WN18 H10 | [BOLD] FB15k MR | [BOLD] FB15k H10 ||\n|| Sparse Encoding | 211 | 86.6 | 66 | 79.1 ||\n|| ITransF | [BOLD] 205 | [BOLD] 94.2 | [BOLD] 65 | [BOLD] 81.0 ||",
    "claim": "ITransF achieves better performance than Sparse Encoding on both WN18 and FB15k benchmarks, except for the WN18 MR metric.",
    "label": "refute"
  },
  {
    "id": "training_970_refute",
    "table_caption": "Table: Number of votes for the given impact label. There are 241,884 total votes and majority of them belongs to the category medium impact.",
    "table_content": "|| Impact label | # votes- all claims ||\n|| No impact | 32,681 ||\n|| Low impact | 37,457 ||\n|| Medium impact | 60,136 ||\n|| High impact | 52,764 ||\n|| Very high impact | 58,846 ||\n|| Total # votes | 241,884 ||",
    "claim": "The majority of the impact votes belong to the medium impact category, and there are more low impact votes than high impact votes, while very high impact votes exceed no impact votes.",
    "label": "refute"
  },
  {
    "id": "training_992_refute",
    "table_caption": "Table: Zero-shot cross-lingual machine reading comprehension results on Japanese and French SQuAD data. † are extracted in Asai et al. (2018).",
    "table_content": "|| [EMPTY] | [BOLD] Japanese  [BOLD] EM | [BOLD] Japanese  [BOLD] F1 | [BOLD] French  [BOLD] EM | [BOLD] French  [BOLD] F1 ||\n|| Back-Translation† | 24.8 | 42.6 | 23.5 | 44.0 ||\n|| +Runtime MT† | 37.0 | 52.2 | 40.7 | 61.9 ||\n|| GNMT+BERT [ITALIC] Len | 26.9 | 46.2 | 39.1 | 67.0 ||\n|| +SimpleMatch | 37.3 | 58.0 | 47.4 | 71.5 ||\n|| BERT [ITALIC] SQ− [ITALIC] Bmul | 61.3 | 73.4 | 57.6 | 77.1 ||",
    "claim": "GNMT+BERT Len improves performance over SimpleMatch in both Japanese and French datasets, as shown by higher EM and F1 scores.",
    "label": "refute"
  },
  {
    "id": "training_907_refute",
    "table_caption": "Table: F1-score, Precision, and Recall with different IVA parameters",
    "table_content": "|| Parameters Metrics | R=1 Precision | R=1 Recall | R=1 F1-score | R=2 Precision | R=2 Recall | R=2 F1-score | R=3 Precision | R=3 Recall | R=3 F1-score | R=4 Precision | R=4 Recall | R=4 F1-score ||\n|| P=1 | 0.489 | 0.472 | 0.480 | 0.525 | 0.566 | 0.525 | 0.537 | 0.589 | 0.540 | 0.545 | 0.607 | 0.555 ||\n|| P=2 | 0.583 | 0.563 | 0.561 | 0.638 | 0.662 | 0.649 | 0.642 | 0.682 | 0.652 | 0.628 | 0.695 | 0.673 ||\n|| P=3 | 0.688 | 0.733 | 0.712 | 0.759 | 0.762 | 0.740 | 0.817 | 0.797 | 0.807 | 0.805 | 0.811 | 0.807 ||\n|| P=4 | 0.730 | 0.762 | 0.744 | 0.770 | 0.824 | 0.797 | 0.822 | 0.833 | 0.819 | 0.829 | 0.828 | 0.814 ||\n|| P=5 | 0.777 | 0.781 | 0.769 | 0.817 | 0.845 | 0.841 | 0.825 | 0.860 | 0.846 | 0.819 | 0.869 | 0.850 ||\n|| P=6 | 0.779 | 0.808 | 0.773 | 0.820 | 0.853 | 0.839 | 0.830 | 0.854 | 0.845 | 0.827 | 0.855 | 0.844 ||",
    "claim": "The F1-score decreases with the number of former neighbors R and increases with keyframe P.",
    "label": "refute"
  },
  {
    "id": "training_1005_refute",
    "table_caption": "Table: Classification accuracy for natural language inference on MultiNLI test set. * denotes ensemble methods.",
    "table_content": "|| [BOLD] Models | [BOLD] Accuracy (%)  [BOLD] matched | [BOLD] Accuracy (%)  [BOLD] mismatched ||\n|| ESIM  | 72.3 | 72.1 ||\n|| DIIN  | 78.8 | 77.8 ||\n|| CAFE  | 78.7 | 77.9 ||\n|| LM-Transformer  | [BOLD] 82.1 | [BOLD] 81.4 ||\n|| [BOLD] DRCN | 79.1 | 78.4 ||\n|| DIIN*  | 80.0 | 78.7 ||\n|| CAFE*  | 80.2 | 79.0 ||\n|| [BOLD] DRCN* | [BOLD] 80.6 | [BOLD] 79.5 ||\n|| [BOLD] DRCN+ELMo* | [BOLD] 82.3 | [BOLD] 81.4 ||",
    "claim": "DRCN+ELMo* outperforms the LM-Transformer in matched accuracy, but LM-Transformer has higher mismatched accuracy.",
    "label": "refute"
  },
  {
    "id": "training_949_refute",
    "table_caption": "Table: Micro and Macro accuracies over all classes and Macro accuracy for the 50% rarest classes.",
    "table_content": "|| Method | Micro | Macro | Rare classes ||\n|| Tgt | 73.4 | 47.8 | 25.7 ||\n|| Yang17C | 74.9 | 50.2 | 28.0 ||\n|| Reg-Our | 75.0 | 51.8 | 31.0 ||\n|| Src-Tune | 75.0 | 52.6 | 32.3 ||\n|| WECT | [BOLD] 75.4 | [BOLD] 53.4 | [BOLD] 33.8 ||",
    "claim": "WECT achieves the highest scores in Micro and Macro, but Reg-Our achieves the highest score in Rare classes.",
    "label": "refute"
  },
  {
    "id": "training_978_refute",
    "table_caption": "Table: Results of different act generation methods, where BiLSTM, Word-CNN and Transformer are baselines from Chen et al. (2019). MarCo is our act generator trained jointly with the response generator and Transformer (GEN) is that without joint training.",
    "table_content": "|| Method | F1 ||\n|| BiLSTM | 71.4 ||\n|| Word-CNN | 71.5 ||\n|| Transformer | 73.1 ||\n|| Transformer (GEN) | 73.2 ||\n|| MarCo | [BOLD] 73.9 ||",
    "claim": "Transformer (GEN) achieves the highest F1 score among the methods compared.",
    "label": "refute"
  },
  {
    "id": "training_947_refute",
    "table_caption": "Table: Our experiment results based on BTEC French-English speech-to-speech translation:",
    "table_content": "|| [BOLD] Model (FR-EN)  [BOLD] Baseline Tacotron with MFCC input | [BOLD] Model (FR-EN)  [BOLD] Baseline Tacotron with MFCC input | [BOLD] BLEU - | [BOLD] METEOR - ||\n|| [BOLD] Proposed Speech2Code | [BOLD] Proposed Speech2Code | [BOLD] Proposed Speech2Code | [BOLD] Proposed Speech2Code ||\n|| Codebook | Time Reduction | [EMPTY] | [EMPTY] ||\n|| 32 | 4 | 19.4 | 19.1 ||\n|| 32 | 8 | 23.8 | 22.2 ||\n|| 32 | 12 | 23.2 | 22.1 ||\n|| 64 | 4 | 16.1 | 16.9 ||\n|| 64 | 8 | 24.4 | 22.9 ||\n|| 64 | 12 | 25.0 | 23.2 ||\n|| 128 | 4 | 16.9 | 17.4 ||\n|| 128 | 8 | 23.3 | 22.1 ||\n|| 128 | 12 | 24.2 | 21.9 ||\n|| [BOLD] Topline (Cascade ASR ->TTS) | [BOLD] Topline (Cascade ASR ->TTS) | 47.4 | 41.2 ||",
    "claim": "The best performance was achieved with a codebook size of 64 and a time-reduction factor of 8, resulting in a BLEU score of 25.0 and a METEOR score of 23.2.",
    "label": "refute"
  },
  {
    "id": "training_807_refute",
    "table_caption": "Table: Classification accuracy on Yelp reviews from 2018. We compare models trained on 1.2M reviews from 2013-2014 (S0) and a those trained on a smaller sample of reviews from 2018 (S1). We vary the size of S1 from 10k to 500k samples, while keeping S0 of fixed size.",
    "table_content": "|| [EMPTY] | Size of  [ITALIC] S1 10k | Size of  [ITALIC] S1 30k | Size of  [ITALIC] S1 100k | Size of  [ITALIC] S1 500k ||\n|| Train on  [ITALIC] S0 | 74.9 | 74.9 | 74.9 | 74.9 ||\n|| Train on  [ITALIC] S1 | 70.8 | 72.7 | 74.4 | 76.2 ||\n|| Train on  [ITALIC] S0∪ [ITALIC] S1 | 75.1 | 75.1 | 75.3 | 76.2 ||\n|| Fine-tune | 72.9 | 73.8 | 75.0 | [BOLD] 76.3 ||\n|| RCSLS+Fine. | [BOLD] 75.1 | [BOLD] 75.3 | [BOLD] 75.8 | [BOLD] 76.3 ||",
    "claim": "The performance of models trained on S0 and S1 depends on the size of S1, with the best performance when S1 is large (500k), and training on the combination of S0 and S1 performs worse than the best individual dataset.",
    "label": "refute"
  },
  {
    "id": "training_1019_refute",
    "table_caption": "Table: Confusion matrix of different stages.",
    "table_content": "|| Stage1 | Unrelated | Unrelated 17,668 | Related 681 ||\n|| [EMPTY] | Related | 529 | 6,535 ||\n|| [EMPTY] | [EMPTY] | Neutral | Stance ||\n|| Stage2 | Neutral | 3,575 | 889 ||\n|| [EMPTY] | Stance | 760 | 1,840 ||\n|| [EMPTY] | [EMPTY] | Agree | Disagree ||\n|| Stage3 | Agree | 1,436 | 467 ||\n|| [EMPTY] | Disagree | 387 | 310 ||",
    "claim": "Stage 1 misclassifies less than 8% of related instances as unrelated, Stage 2 misclassifies 15% of stance instances as neutral, and Stage 3 misclassifies 55% of disagree instances as agree and around 25% of agree instances as disagree.",
    "label": "refute"
  },
  {
    "id": "training_916_refute",
    "table_caption": "Table: Detailed results on the CoNLL-2003 dataset for IV, OOTV, OOEV, OOBV.",
    "table_content": "|| [EMPTY] | [BOLD] Baseline  [ITALIC] P | [BOLD] Baseline  [ITALIC] R | [BOLD] Baseline  [ITALIC] F1 | [BOLD] Ours  [ITALIC] P | [BOLD] Ours  [ITALIC] R | [BOLD] Ours  [ITALIC] F1 ||\n|| IV | 94.58 | 93.16 | 93.87 | 94.96 | 93.58 | [BOLD] 94.26 ||\n|| OOTV | 93.46 | 91.57 | 92.51 | 94.07 | 91.85 | [BOLD] 92.95 ||\n|| OOEV | 94.12 | 94.12 | 94.12 | 94.12 | 94.12 | 94.12 ||\n|| OOBV | 88.42 | 84.81 | 86.58 | 88.51 | 85.56 | [BOLD] 87.01 ||",
    "claim": "Our model outperforms the baseline by 0.39 in terms of F1 score for IV, 0.54 for OOTV, and 0.43 for OOBV.",
    "label": "refute"
  },
  {
    "id": "training_1018_refute",
    "table_caption": "Table: Confusion matrix of our pipeline system.",
    "table_content": "|| [EMPTY] | Agree | Disagree | Neutral | Unrelated ||\n|| Agree | 1,006 | 278 | 495 | 124 ||\n|| Disagree | 237 | 160 | 171 | 129 ||\n|| Neutral | 555 | 252 | 3,381 | 276 ||\n|| Unrelated | 127 | 31 | 523 | 17,668 ||",
    "claim": "171 disagree instances (25.5%) are misclassified as neutral, and 20% of the disagree instances are misclassified as agree.",
    "label": "refute"
  },
  {
    "id": "training_997_refute",
    "table_caption": "Table: Error rates of our models on 16 datasets against typical baselines. The numbers in brackets represent the improvements relative to the average performance (Avg.) of three single task baselines.",
    "table_content": "|| [BOLD] Task | [BOLD] Single Task LSTM | [BOLD] Single Task BiLSTM | [BOLD] Single Task sLSTM | [BOLD] Single Task Avg. | [BOLD] Multiple Tasks MT-DNN | [BOLD] Multiple Tasks MT-CNN | [BOLD] Multiple Tasks FS-MTL | [BOLD] Multiple Tasks SP-MTL | [BOLD] Multiple Tasks ASP-MTL ||\n|| Books | 20.5 | 19.0 | 18.0 | 19.2 | 17.8(−1.4) | 15.5(−3.7) | 17.5(−1.7) | 18.8(−0.4) | 16.0(−3.2) ||\n|| Electronics | 19.5 | 21.5 | 23.3 | 21.4 | 18.3(−3.1) | 16.8(−4.6) | 14.3(−7.1) | 15.3(−6.1) | 13.2(−8.2) ||\n|| DVD | 18.3 | 19.5 | 22.0 | 19.9 | 15.8(−4.1) | 16.0(−3.9) | 16.5(−3.4) | 16.0(−3.9) | 14.5(−5.4) ||\n|| Kitchen | 22.0 | 18.8 | 19.5 | 20.1 | 19.3(−0.8) | 16.8(−3.3) | 14.0(−6.1) | 14.8(−5.3) | 13.8(−6.3) ||\n|| Apparel | 16.8 | 14.0 | 16.3 | 15.7 | 15.0(−0.7) | 16.3(+0.6) | 15.5(−0.2) | 13.5(−2.2) | 13.0(−2.7) ||\n|| Camera | 14.8 | 14.0 | 15.0 | 14.6 | 13.8(−0.8) | 14.0(−0.6) | 13.5(−1.1) | 12.0(−2.6) | 10.8(−3.8) ||\n|| Health | 15.5 | 21.3 | 16.5 | 17.8 | 14.3(−3.5) | 12.8(−5.0) | 12.0(−5.8) | 12.8(−5.0) | 11.8(−6.0) ||\n|| Music | 23.3 | 22.8 | 23.0 | 23.0 | 15.3(−7.7) | 16.3(−6.7) | 18.8(−4.2) | 17.0(−6.0) | 17.5(−5.5) ||\n|| Toys | 16.8 | 15.3 | 16.8 | 16.3 | 12.3(−4.0) | 10.8(−5.5) | 15.5(−0.8) | 14.8(−1.5) | 12.0(−4.3) ||\n|| Video | 18.5 | 16.3 | 16.3 | 17.0 | 15.0(−2.0) | 18.5(+1.5) | 16.3(−0.7) | 16.8(−0.2) | 15.5(−1.5) ||\n|| Baby | 15.3 | 16.5 | 15.8 | 15.9 | 12.0(−3.9) | 12.3(−3.6) | 12.0(−3.9) | 13.3(−2.6) | 11.8(−4.1) ||\n|| Magazines | 10.8 | 8.5 | 12.3 | 10.5 | 10.5(+0.0) | 12.3(+1.8) | 7.5(−3.0) | 8.0(−2.5) | 7.8(−2.7) ||\n|| Software | 15.3 | 14.3 | 14.5 | 14.7 | 14.3(−0.4) | 13.5(−1.2) | 13.8(−0.9) | 13.0(−1.7) | 12.8(−1.9) ||\n|| Sports | 18.3 | 16.0 | 17.5 | 17.3 | 16.8(−0.5) | 16.0(−1.3) | 14.5(−2.8) | 12.8(−4.5) | 14.3(−3.0) ||\n|| IMDB | 18.3 | 15.0 | 18.5 | 17.3 | 16.8(−0.5) | 13.8(−3.5) | 17.5(+0.2) | 15.3(−2.0) | 14.5(−2.8) ||\n|| MR | 27.3 | 25.3 | 28.0 | 26.9 | 24.5(−2.4) | 25.5(−1.4) | 25.3(−1.6) | 24.0(−2.9) | 23.3(−3.6) ||\n|| AVG | 18.2 | 17.4 | 18.3 | 18.0 | 15.7(−2.2) | 15.5(−2.5) | 15.3(−2.7) | 14.9(−3.1) | 13.9(−4.1) ||",
    "claim": "SP-MTL achieves a 4.1% average improvement over ASP-MTL, indicating better performance in multi-task learning.",
    "label": "refute"
  },
  {
    "id": "training_1025_refute",
    "table_caption": "Table: Summary of vocabulary overlaps for the various language sets. All figures in % of the shared vocabulary.",
    "table_content": "|| Languages | Unique in a Lang. | In All | From Parent ||\n|| ET-EN-FI | 24.4-18.2-26.2 | 19.5 | 49.4 ||\n|| ET-EN-RU | 29.9-20.7-29.0 | 8.9 | 41.0 ||\n|| ET-EN-CS | 29.6-17.5-21.2 | 20.3 | 49.2 ||\n|| AR-RU-ET-EN | 28.6-27.7-21.2-9.1 | 4.6 | 6.2 ||\n|| ES-FR-ET-EN | 15.7-13.0-24.8-8.8 | 18.4 | 34.1 ||\n|| ES-RU-ET-EN | 14.7-31.1-21.3-9.3 | 6.0 | 21.4 ||\n|| FR-RU-ET-EN | 12.3-32.0-22.3-8.1 | 6.3 | 23.1 ||",
    "claim": "AR-RU-ET-EN has a high percentage (49.2%) of subwords available from the parent.",
    "label": "refute"
  },
  {
    "id": "training_976_refute",
    "table_caption": "Table: Performance of trained language models by dense after sparse method. Lower perplexity means better performance.",
    "table_content": "|| Training Method | Perplexity | Improvement ||\n|| Dense | 93.101 | 0 ||\n|| Coarse-grained | 98.611 | -5.92% ||\n|| 1/2 coarse + 1/2 dense | 88.675 | +4.75% ||\n|| 3/4 coarse + 1/4 dense | 91.388 | +1.84% ||\n|| 5/6 coarse + 1/6 dense | 99.47 | -6.84% ||\n|| Fine-grained | 96.410 | -3.55% ||\n|| 1/2 fine + 1/2 dense | 88.607 | +4.83% ||\n|| 3/4 fine + 1/4 dense | 91.118 | +2.13% ||\n|| 5/6 fine + 1/6 dense | 96.151 | -3.28% ||",
    "claim": "With 83.33% steps of sparse training and 16.67% steps of dense training, the model achieves slightly better performance than the baseline model.",
    "label": "refute"
  },
  {
    "id": "training_940_refute",
    "table_caption": "Table: Results of Vector Composition with different Operations",
    "table_content": "|| Composition | Accuracy ||\n|| Multiplication | 50.30 ||\n|| Average | 88.42 ||\n|| Idf Graded Weighted Average | [BOLD] 89.56 ||",
    "claim": "Average achieves the highest accuracy among the composition methods listed.",
    "label": "refute"
  },
  {
    "id": "training_982_refute",
    "table_caption": "Table: Link prediction results on two datasets. Higher Hits@10 or lower Mean Rank indicates better performance. Following Nguyen et al. (2016b) and Shen et al. (2016), we divide the models into two groups. The first group contains intrinsic models without using extra information. The second group make use of additional information. Results in the brackets are another set of results STransE reported.",
    "table_content": "|| [BOLD] Model | [BOLD] Additional Information | [BOLD] WN18 Mean Rank | [BOLD] WN18 Hits@10 | [BOLD] FB15k Mean Rank | [BOLD] FB15k Hits@10 ||\n|| SE Bordes et al. ( 2011 ) | No | 985 | 80.5 | 162 | 39.8 ||\n|| Unstructured Bordes et al. ( 2014 ) | No | 304 | 38.2 | 979 | 6.3 ||\n|| TransE (Bordes et al.,  2013 ) | No | 251 | 89.2 | 125 | 47.1 ||\n|| TransH (Wang et al.,  2014 ) | No | 303 | 86.7 | 87 | 64.4 ||\n|| TransR (Lin et al.,  2015b ) | No | 225 | 92.0 | 77 | 68.7 ||\n|| CTransR (Lin et al.,  2015b ) | No | 218 | 92.3 | 75 | 70.2 ||\n|| KG2E (He et al.,  2015 ) | No | 348 | 93.2 | 59 | 74.0 ||\n|| TransD (Ji et al.,  2015 ) | No | 212 | 92.2 | 91 | 77.3 ||\n|| TATEC (García-Durán et al.,  2016 ) | No | - | - | [BOLD] 58 | 76.7 ||\n|| NTN (Socher et al.,  2013 ) | No | - | 66.1 | - | 41.4 ||\n|| DISTMULT (Yang et al.,  2015 ) | No | - | 94.2 | - | 57.7 ||\n|| STransE (Nguyen et al.,  2016b ) | No | 206 (244) | 93.4 (94.7) | 69 | 79.7 ||\n|| ITransF | No | [BOLD] 205 | 94.2 | 65 | 81.0 ||\n|| ITransF (domain sampling) | No | 223 | [BOLD] 95.2 | 77 | [BOLD] 81.4 ||\n|| rTransE García-Durán et al. ( 2015 ) | Path | - | - | 50 | 76.2 ||\n|| PTransE Lin et al. ( 2015a ) | Path | - | - | 58 | 84.6 ||\n|| NLFeat Toutanova and Chen ( 2015 ) | Node + Link Features | - | 94.3 | - | 87.0 ||\n|| Random Walk (Wei et al.,  2016 ) | Path | - | 94.8 | - | 74.7 ||\n|| IRN (Shen et al.,  2016 ) | External Memory | 249 | [ITALIC] 95.3 | [ITALIC] 38 | [ITALIC] 92.7 ||",
    "claim": "The model achieves a worse mean rank on WN18 compared to the IRN model, which uses external information, while maintaining comparable Hits@10.",
    "label": "refute"
  },
  {
    "id": "training_851_refute",
    "table_caption": "Table: The overall results on NIST Chinese-English translation task.",
    "table_content": "|| Models | NIST02 | NIST03 | NIST04 | NIST05 | NIST08 | Average ||\n|| [ITALIC] baseline | 49.40 | 49.71 | 50.03 | 48.83 | 44.38 | 40.39 ||\n|| [ITALIC] sub-sentence | 45.41 | 45.62 | 46.06 | 43.63 | 43.11 | 37.31 ||\n|| [ITALIC] wait-1 | 38.37 | 36.87 | 38.17 | 36.09 | 35.31 | 30.80 ||\n|| [ITALIC] wait-3 | 40.75 | 39.30 | 40.57 | 38.18 | 38.29 | 32.85 ||\n|| [ITALIC] wait-5 | 42.76 | 41.43 | 43.29 | 40.43 | 39.62 | 34.59 ||\n|| [ITALIC] wait-7 | 44.05 | 42.94 | 44.17 | 42.25 | 40.61 | 35.67 ||\n|| [ITALIC] wait-9 | 45.71 | 44.49 | 45.74 | 43.14 | 41.63 | 36.78 ||\n|| [ITALIC] wait-12 | [BOLD] 46.67 | 45.63 | 46.86 | 44.59 | 42.83 | 37.76 ||\n|| [ITALIC] wait-15 | 46.41 | [BOLD] 46.43 | [BOLD] 47.38 | [BOLD] 45.63 | [BOLD] 43.60 | [BOLD] 38.24 ||\n|| treat the information unit as sub-sentence ( [BOLD] IU=sub-sentence) | treat the information unit as sub-sentence ( [BOLD] IU=sub-sentence) | treat the information unit as sub-sentence ( [BOLD] IU=sub-sentence) | treat the information unit as sub-sentence ( [BOLD] IU=sub-sentence) | treat the information unit as sub-sentence ( [BOLD] IU=sub-sentence) | treat the information unit as sub-sentence ( [BOLD] IU=sub-sentence) | treat the information unit as sub-sentence ( [BOLD] IU=sub-sentence) ||\n|| [ITALIC] +context-aware | 47.79 | 48.11 | 48.29 | 46.55 | 44.57 | 39.22 ||\n|| [ITALIC] +partial decoding | 48.46 | 48.51 | 48.53 | 47.05 | 45.43 | 39.66 ||\n|| [ITALIC] +discard 2 tokens | 48.61 | 48.54 | 48.68 | 47.11 | 45.08 | 39.67 ||\n|| [ITALIC] +discard 3 tokens | 48.62 | 48.52 | 48.87 | 47.16 | [BOLD] 45.30 | 39.75 ||\n|| [ITALIC] +discard 4 tokens | 48.71 | 48.69 | [BOLD] 49.10 | [BOLD] 47.32 | 45.11 | [BOLD] 39.82 ||\n|| [ITALIC] +discard 5 tokens | 48.82 | [BOLD] 48.78 | 48.98 | 47.31 | 44.48 | 39.73 ||\n|| [ITALIC] +discard 6 tokens | [BOLD] 48.94 | 48.70 | 48.77 | 47.21 | 44.33 | 39.66 ||\n|| treat the information unit as segment ( [BOLD] IU=segment) | treat the information unit as segment ( [BOLD] IU=segment) | treat the information unit as segment ( [BOLD] IU=segment) | treat the information unit as segment ( [BOLD] IU=segment) | treat the information unit as segment ( [BOLD] IU=segment) | treat the information unit as segment ( [BOLD] IU=segment) | treat the information unit as segment ( [BOLD] IU=segment) ||\n|| [ITALIC] +discard 1 tokens | 46.89 | 45.40 | 47.05 | 45.36 | 43.06 | 37.96 ||\n|| [ITALIC] +discard 2 tokens | 48.09 | 46.98 | 48.45 | 46.50 | 44.00 | 39.00 ||\n|| [ITALIC] +discard 3 tokens | 48.70 | 47.87 | 48.85 | 47.01 | 44.48 | 39.49 ||\n|| [ITALIC] +discard 4 tokens | 48.75 | 48.09 | [BOLD] 48.99 | 46.86 | [BOLD] 45.07 | 39.63 ||\n|| [ITALIC] +discard 5 tokens | 48.84 | 48.37 | 48.71 | [BOLD] 46.95 | 44.76 | 39.56 ||\n|| [ITALIC] +discard 6 tokens | [BOLD] 48.88 | [BOLD] 48.60 | 48.85 | 47.17 | 44.84 | [BOLD] 39.72 ||",
    "claim": "The sub-sentence model shows a 3.08 drop in BLEU score compared to the baseline (40.39 → 37.31). The wait-15 model has a 2.15 drop in BLEU score compared to the baseline (40.39 → 38.24). The discard 6 tokens model achieves an average decrease of 1.76 BLEU score compared to the sub-sentence model (37.96 → 39.72).",
    "label": "refute"
  },
  {
    "id": "training_908_refute",
    "table_caption": "Table: F1-score, Precision, and Recall of each method",
    "table_content": "|| [BOLD] (a) TV-series Baselines | [BOLD] (a) TV-series Precision | [BOLD] (a) TV-series Recall | [BOLD] (a) TV-series F1-score ||\n|| KM | 0.443 | [BOLD] 0.892 | 0.577 ||\n|| LDA | 0.563 | 0.656 | 0.606 ||\n|| LI-NPP | 0.677 | 0.713 | 0.695 ||\n|| DN-GAA | 0.730 | 0.798 | 0.747 ||\n|| SBN | 0.782 | 0.791 | 0.779 ||\n|| SBN-WT | 0.762 | 0.743 | 0.754 ||\n|| SBN-IVA | [BOLD] 0.843 | 0.856 | [BOLD] 0.850 ||\n|| [BOLD] (b) Movies | [BOLD] (b) Movies | [BOLD] (b) Movies | [BOLD] (b) Movies ||\n|| Baselines | Precision | Recall | F1-score ||\n|| KM | 0.398 | [BOLD] 0.912 | 0.545 ||\n|| LDA | 0.497 | 0.604 | 0.562 ||\n|| LI-NPP | 0.654 | 0.727 | 0.688 ||\n|| DN-GAA | 0.714 | 0.792 | 0.751 ||\n|| SBN | 0.722 | 0.833 | 0.785 ||\n|| SBN-WT | 0.708 | 0.776 | 0.732 ||\n|| SBN-IVA | [BOLD] 0.753 | 0.856 | [BOLD] 0.811 ||\n|| [BOLD] (c) Sports | [BOLD] (c) Sports | [BOLD] (c) Sports | [BOLD] (c) Sports ||\n|| Baselines | Precision | Recall | F1-score ||\n|| KM | 0.373 | 0.797 | 0.509 ||\n|| LDA | 0.525 | 0.589 | 0.555 ||\n|| LI-NPP | 0.668 | 0.778 | 0.719 ||\n|| DN-GAA | 0.718 | 0.758 | 0.738 ||\n|| SBN | 0.782 | 0.809 | 0.789 ||\n|| SBN-WT | 0.722 | 0.749 | 0.741 ||\n|| SBN-IVA | [BOLD] 0.810 | [BOLD] 0.841 | [BOLD] 0.825 ||",
    "claim": "SBN-IVA achieves the highest F1-scores only in the Movies dataset.",
    "label": "refute"
  },
  {
    "id": "training_870_refute",
    "table_caption": "Table: Nature of the data collected with each MR. Italics denote averages across all numbers of attributes.",
    "table_content": "|| [EMPTY] | [BOLD] Textual MR Mean | [BOLD] Textual MR StDev | [BOLD] Pictorial MR Mean | [BOLD] Pictorial MR StDev ||\n|| [ITALIC] Time, sec | [ITALIC] 347.18 | [ITALIC] 301.74 | [ITALIC] 352.05 | [ITALIC] 249.34 ||\n|| 3 attributes | 283.37 | 265.82 | 298.97 | 272.44 ||\n|| 5 attributes | 321.75 | 290.89 | 355.56 | 244.57 ||\n|| 8 attributes | 433.41 | 325.04 | 405.56 | 215.43 ||\n|| [ITALIC] Length, char | [ITALIC] 100.83 | [ITALIC] 46.40 | [ITALIC] 93.06 | [ITALIC] 37.78 ||\n|| 3 attributes | 61.25 | 19.44 | 67.98 | 22.30 ||\n|| 5 attributes | 95.18 | 26.71 | 91.13 | 21.19 ||\n|| 8 attributes | 144.79 | 41.84 | 121.94 | 40.13 ||\n|| [ITALIC] No of sentences | [ITALIC] 1.43 | [ITALIC] 0.69 | [ITALIC] 1.31 | [ITALIC] 0.54 ||\n|| 3 attributes | 1.06 | 0.24 | 1.07 | 0.25 ||\n|| 5 attributes | 1.37 | 0.51 | 1.25 | 0.49 ||\n|| 8 attributes | 1.84 | 0.88 | 1.63 | 0.64 ||",
    "claim": "The average task duration and length of utterance increase with a larger number of attributes, while the number of sentences decreases.",
    "label": "refute"
  },
  {
    "id": "training_1032_refute",
    "table_caption": "Table: Error contribution.",
    "table_content": "|| [BOLD] Module | [BOLD] Contribution ||\n|| Language Tagger | 36 ||\n|| Back Transliteration | 12 ||\n|| Machine Translation | 25 ||",
    "claim": "The Language Tagger module contributes 36% to the errors, Machine Translation contributes 36%, and Back Transliteration contributes 12%.",
    "label": "refute"
  },
  {
    "id": "training_1017_refute",
    "table_caption": "Table: Document stance classification performance.",
    "table_content": "|| System | FNC | F1 [ITALIC] m | F1Unrel. | F1Neutral | F1Agree | F1Disagr. | F1 [ITALIC] mAgr/Dis ||\n|| Majority vote | 0.39 | 0.21 | 0.84 | 0.00 | 0.00 | 0.00 | 0.00 ||\n|| FNC baseline | 0.75 | 0.45 | 0.96 | 0.69 | 0.15 | 0.02 | 0.09 ||\n|| SOLAT (Baird et al.,  2017 ) | 0.82 | 0.58 | [BOLD] 0.99 | 0.76 | [BOLD] 0.54 | 0.03 | 0.29 ||\n|| Athene (Hanselowski et al.,  2017 ) | 0.82 | 0.60 | [BOLD] 0.99 | [BOLD] 0.78 | 0.49 | 0.15 | 0.32 ||\n|| UCLMR (Riedel et al.,  2017 ) | 0.82 | 0.58 | [BOLD] 0.99 | 0.75 | 0.48 | 0.11 | 0.30 ||\n|| CombNSE (Bhatt et al.,  2018 ) | [BOLD] 0.83 | 0.59 | 0.98 | 0.77 | 0.49 | 0.11 | 0.30 ||\n|| StackLSTM (Hanselowski et al.,  2018 ) | 0.82 | 0.61 | [BOLD] 0.99 | 0.76 | 0.50 | 0.18 | 0.34 ||\n|| LearnedMMD (Zhang et al.,  2019 ) | 0.79 | 0.57 | 0.97 | 0.73 | 0.50 | 0.09 | 0.29 ||\n|| 3-Stage Trad (Masood and Aker,  2018 ) | 0.82 | 0.59 | 0.98 | 0.76 | 0.52 | 0.10 | 0.31 ||\n|| L3S | 0.81 | [BOLD] 0.62 | 0.97 | 0.75 | 0.53 | [BOLD] 0.23 | [BOLD] 0.38 ||",
    "claim": "Athene achieves the highest performance in FNC (0.83), while L3S achieves the highest score in macro-averaged F1 (0.62).",
    "label": "refute"
  },
  {
    "id": "training_909_refute",
    "table_caption": "Table: Comparison of prediction results based on encoder using NYT122, NYT71, and NYT27 datasets with different KB regularization strategies.",
    "table_content": "|| [BOLD] Model | [BOLD] NYT122 F1 | [BOLD] NYT122 F1 | [BOLD] NYT122 NMI | [BOLD] NYT122 NMI | [BOLD] NYT71 F1 | [BOLD] NYT71 F1 | [BOLD] NYT71 NMI | [BOLD] NYT71 NMI | [BOLD] NYT27 F1 | [BOLD] NYT27 F1 | [BOLD] NYT27 NMI | [BOLD] NYT27 NMI ||\n|| [EMPTY] | Mean | Std | Mean | Std | Mean | Std | Mean | Std | Mean | Std | Mean | Std ||\n|| Majority | 0.355 | - | 0 | - | 0.121 | - | 0 | - | 0.549 | - | 0 | - ||\n|| DVAE | 0.417 | 0.011 | 0.339 | 0.009 | 0.325 | 0.011 | 0.375 | 0.023 | 0.433 | 0.018 | 0.384 | 0.021 ||\n|| DVAE+E | 0.385 | 0.021 | 0.341 | 0.043 | 0.339 | 0.021 | 0.418 | 0.022 | 0.396 | 0.034 | 0.381 | 0.039 ||\n|| DVAE+D | 0.452 | 0.033 | 0.438 | 0.022 | 0.352 | 0.038 | 0.339 | 0.009 | 0.499 | 0.040 | 0.469 | 0.027 ||\n|| RegDVAE | 0.469 | 0.014 | 0.430 | 0.020 | 0.377 | 0.020 | 0.466 | 0.036 | 0.587 | 0.005 | 0.451 | 0.005 ||\n|| RegDVAE+D | [BOLD] 0.499 | 0.022 | [BOLD] 0.497 | 0.013 | [BOLD] 0.432 | 0.028 | [BOLD] 0.589 | 0.071 | [BOLD] 0.665 | 0.022 | [BOLD] 0.562 | 0.038 ||",
    "claim": "RegDVAE+D achieves the highest F1 and NMI scores in NYT122 and NYT71, but not in NYT27.",
    "label": "refute"
  },
  {
    "id": "training_1010_refute",
    "table_caption": "Table: Training Data for Task 1.",
    "table_content": "|| Side | Vocabulary | Words ||\n|| English | 10211 | 377K ||\n|| German | 15820 | 369K ||",
    "claim": "The English vocabulary size is larger than the German vocabulary size, with 10211 unique tokens compared to 15820.",
    "label": "refute"
  },
  {
    "id": "training_995_refute",
    "table_caption": "Table: NMT with exact inference. In the absence of search errors, NMT often prefers the empty translation, causing a dramatic drop in length ratio and BLEU.",
    "table_content": "|| [BOLD] Search | [BOLD] BLEU | [BOLD] Ratio | [BOLD] #Search errors | [BOLD] #Empty ||\n|| Greedy | 29.3 | 1.02 | 73.6% | 0.0% ||\n|| Beam-10 | 30.3 | 1.00 | 57.7% | 0.0% ||\n|| Exact | 2.1 | 0.06 | 0.0% | 51.8% ||",
    "claim": "Beam-10 yields 15.9% more search errors than Greedy decoding (57.7% vs. 73.6%).",
    "label": "refute"
  },
  {
    "id": "training_853_refute",
    "table_caption": "Table: The overall results on BSTC Chinese-English translation task (Pre-train represents training on the NIST dataset, and fine-tune represents fine-tuning on the BSTC dataset.). Clean input indicates the input is from human annotated transcription, while the ASR input represents the input contains ASR errors. ASR + Auto IU indicates that the sentence boundary as well as sub-sentence is detected by our IU detector. Therefore, this data basically reflects the real environment in practical product.",
    "table_content": "|| Models | Clean Input Pre-train | Clean Input Fine-tune | ASR Input Pre-train | ASR Input Fine-tune | ASR + Auto IU Pre-train | ASR + Auto IU Fine-tune ||\n|| [ITALIC] baseline | [BOLD] 15.85 | [BOLD] 21.98 | [BOLD] 14.60 | [BOLD] 19.91 | [BOLD] 14.41 | [BOLD] 17.35 ||\n|| [ITALIC] sub-sentence | 14.39 | 18.61 | 13.50 | 16.99 | 13.76 | 16.29 ||\n|| [ITALIC] wait-3 | 12.23 | 16.74 | 11.62 | 15.59 | 11.75 | 14.68 ||\n|| [ITALIC] wait-5 | 12.84 | 17.70 | 11.96 | 16.23 | 12.25 | 15.45 ||\n|| [ITALIC] wait-7 | 13.34 | 19.32 | 12.67 | 17.41 | 12.55 | 16.08 ||\n|| [ITALIC] wait-9 | 13.92 | 19.77 | 13.05 | 18.29 | 13.12 | 16.49 ||\n|| [ITALIC] wait-12 | 14.35 | 20.15 | 13.34 | 19.07 | 13.48 | [BOLD] 17.25 ||\n|| [ITALIC] wait-15 | [BOLD] 14.70 | [BOLD] 21.11 | [BOLD] 13.56 | [BOLD] 19.53 | [BOLD] 13.70 | 17.21 ||\n|| [ITALIC] context-aware | 15.25 | 20.72 | 14.24 | 18.42 | 13.52 | 16.83 ||\n|| [ITALIC] +discard 2 tokens | 15.26 | 21.07 | 14.35 | 19.17 | 13.73 | 17.02 ||\n|| [ITALIC] +discard 3 tokens | 15.37 | 21.09 | 14.42 | 19.39 | 14.00 | 17.41 ||\n|| [ITALIC] +discard 4 tokens | 15.40 | 21.02 | 14.45 | 19.41 | 14.11 | 17.36 ||\n|| [ITALIC] +discard 5 tokens | [BOLD] 15.59 | [BOLD] 21.23 | 14.72 | [BOLD] 19.65 | [BOLD] 14.54 | 17.37 ||\n|| [ITALIC] +discard 6 tokens | 15.53 | 21.21 | [BOLD] 14.77 | 19.48 | 14.58 | [BOLD] 17.49 ||",
    "claim": "The distinction between clean input and ASR input results in a BLEU score difference smaller than 2 points (15.85 vs. 14.60 for pre-train, and 21.98 vs. 19.91 for fine-tune). Fine-tuning improves the performance of all models. Pre-trained models are sensitive to errors from Auto IU, while fine-tuned models are not.",
    "label": "refute"
  },
  {
    "id": "training_1039_refute",
    "table_caption": "Table: AUC performance when the model is trained on different combinations of the two tasks and evaluated on the dataset containing only PR and RW negatives.",
    "table_content": "|| Training | Val. Seen | Val. Unseen ||\n|| cma | 82.6 | 72.0 ||\n|| nvs | 63.0 | 62.1 ||\n|| cma + nvs | [BOLD] 84.0 | [BOLD] 79.2 ||",
    "claim": "The model trained on cma alone achieves the highest AUC scores in both Val. Seen and Val. Unseen categories.",
    "label": "refute"
  },
  {
    "id": "training_983_refute",
    "table_caption": "Table: Performance of model with dense attention vectors or sparse attention vectors. MR, H10 and Time denotes mean rank, Hits@10 and training time per epoch respectively",
    "table_content": "|| [BOLD] Method | [BOLD] WN18 MR | [BOLD] WN18 H10 | [BOLD] WN18 Time | [BOLD] FB15k MR | [BOLD] FB15k H10 | [BOLD] FB15k Time ||\n|| Dense | [BOLD] 199 | 94.0 | 4m34s | 69 | 79.4 | 4m30s ||\n|| Dense + ℓ1 | 228 | [BOLD] 94.2 | 4m25s | 131 | 78.9 | 5m47s ||\n|| Sparse | 207 | 94.1 | [BOLD] 2m32s | [BOLD] 67 | [BOLD] 79.6 | [BOLD] 1m52s ||",
    "claim": "Sparse attention has better or comparable performance to dense attention, and ℓ1 regularization produces sparse attention, especially on FB15k.",
    "label": "refute"
  },
  {
    "id": "training_965_refute",
    "table_caption": "Table: Performance on the Chimera benchmark dataset with different numbers of context sentences, which is measured by Spearman correlation. Baseline results are from the corresponding papers.",
    "table_content": "|| Methods | 2-shot | 4-shot | 6-shot ||\n|| Word2vec | 0.1459 | 0.2457 | 0.2498 ||\n|| FastText | 0.1775 | 0.1738 | 0.1294 ||\n|| Additive | 0.3627 | 0.3701 | 0.3595 ||\n|| Additive, no stop words | 0.3376 | 0.3624 | 0.4080 ||\n|| nonce2vec | 0.3320 | 0.3668 | 0.3890 ||\n|| ` [ITALIC] a la carte | 0.3634 | 0.3844 | 0.3941 ||\n|| HiCE w/o Morph | 0.3710 | 0.3872 | 0.4277 ||\n|| HiCE + Morph | [BOLD] 0.3796 | 0.3916 | 0.4253 ||\n|| HiCE + Morph + Fine-tune | 0.1403 | 0.1837 | 0.3145 ||\n|| HiCE + Morph + MAML | 0.3781 | [BOLD] 0.4053 | [BOLD] 0.4307 ||\n|| Oracle Embedding | 0.4160 | 0.4381 | 0.4427 ||",
    "claim": "`a la carte outperforms HiCE+Morph+MAML in 2-shot, 4-shot, and 6-shot learning, and its performance in 6-shot learning is close to the Oracle Embedding.",
    "label": "refute"
  },
  {
    "id": "training_1020_refute",
    "table_caption": "Table: SLU performances on the DSTC3 evaluation set when removing different modules of our method.",
    "table_content": "|| [BOLD] Method | [BOLD] F1-score ||\n|| HD + AT(seed abr. + comb.) | 88.6 ||\n|| - dstc2 | 86.2 ||\n|| - dstc3_seed | 84.5 ||\n|| - dstc2, - dstc3_seed | 84.3 ||\n|| - sentence generator | 74.0 ||\n|| - atomic templates | 70.5 ||",
    "claim": "Removing the sentence generator increases the F1-score by 10.3%, and replacing atomic templates with act-slot-value triples results in a sharp drop in F1-score.",
    "label": "refute"
  },
  {
    "id": "training_979_refute",
    "table_caption": "Table: Overall results on the MultiWOZ 2.0 dataset.",
    "table_content": "|| Dialog Act | Model | Inform | Success | BLEU | Combined Score ||\n|| Without Act | LSTM | 71.29 | 60.96 | 18.80 | 84.93 ||\n|| Without Act | Transformer | 71.10 | 59.90 | 19.10 | 84.60 ||\n|| Without Act | TokenMoE | 75.30 | 59.70 | 16.81 | 84.31 ||\n|| Without Act | Structured Fusion | 82.70 | 72.10 | 16.34 | 93.74 ||\n|| One-hot Act | SC-LSTM | 74.50 | 62.50 | 20.50 | 89.00 ||\n|| One-hot Act | HDSA (MarCo) | 76.50 | 62.30 | 21.85 | 91.25 ||\n|| One-hot Act | HDSA | 82.90 | 68.90 | [BOLD] 23.60 | 99.50 ||\n|| Sequential Act | MarCo | 90.30 | 75.20 | 19.45 | 102.20 ||\n|| Sequential Act | MarCo (BERT) | [BOLD] 92.30 | [BOLD] 78.60 | 20.02 | [BOLD] 105.47 ||",
    "claim": "MarCo (BERT) outperforms all other models in Request Success and Combined Score, but not in Inform Rate.",
    "label": "refute"
  },
  {
    "id": "training_1012_refute",
    "table_caption": "Table: BLEU scores for various deep features on the image description generation task using the system of Xu et al. [Xu et al.2015].",
    "table_content": "|| Network | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 ||\n|| VGG-19 | 58.2 | 31.4 | 18.5 | 11.3 ||\n|| ResNet-50 | 68.4 | 45.2 | 30.9 | 21.1 ||\n|| ResNet-152 | 68.3 | 44.9 | 30.7 | 21.1 ||",
    "claim": "ResNet-152 achieves the highest BLEU scores across all metrics compared to VGG-19 and ResNet-50.",
    "label": "refute"
  },
  {
    "id": "training_987_refute",
    "table_caption": "Table: Train and test set character error rate (CER) results for a deep neural network (DNN) without recurrence, recurrent deep neural network with forward temporal connections (RDNN), and a bi-directional recurrent deep neural network (BRDNN). All models have 5 hidden layers. The DNN and RDNN both have 2,048 hidden units in each hidden layer while the BRDNN has 1,824 hidden units per hidden layer to keep its total number of free parameters similar to the other models. For all models we choose the most likely character at each timestep and apply CTC collapsing to obtain a character-level transcript hypothesis.",
    "table_content": "|| Model | Parameters (M) | Train CER | Test CER ||\n|| DNN | 16.8 | 3.8 | 22.3 ||\n|| RDNN | 22.0 | 4.2 | 13.5 ||\n|| BRDNN | 20.9 | 2.8 | 10.7 ||",
    "claim": "BRDNN has the lowest train CER but not the lowest test CER compared to DNN and RDNN.",
    "label": "refute"
  },
  {
    "id": "training_1052_refute",
    "table_caption": "Table: BLEU and length ratio of models on Zh→En validation set. †indicates our own implementation.",
    "table_content": "|| Model | Train | Decode | BLEU | Len. ||\n|| Model | Beam | Beam | BLEU | Len. ||\n|| Seq2Seq† | - | 7 | 37.74 | 0.96 ||\n|| w/ Len. reward† | - | 7 | 38.28 | 0.99 ||\n|| BSO† | 4 | 3 | 36.91 | 1.03 ||\n|| BSO† | 8 | 7 | 35.57 | 1.07 ||\n|| This work | 4 | 3 | 38.41 | 1.00 ||\n|| This work | 8 | 7 | 39.51 | 1.00 ||",
    "claim": "The proposed model achieves lower BLEU scores than Seq2Seq in both beam configurations.",
    "label": "refute"
  },
  {
    "id": "training_1030_refute",
    "table_caption": "Table: Instance segmentation results.",
    "table_content": "|| net | [ITALIC] AP | [ITALIC] AP50 | [ITALIC] AP75 ||\n|| res101-mrcn (ours) | 30.7 | 52.3 | 32.4 ||\n|| res101-mrcn  | 32.7 | 54.2 | 34.0 ||",
    "claim": "The AP of the author's implementation is approximately 2 points higher than the other res101-mrcn model.",
    "label": "refute"
  },
  {
    "id": "training_999_refute",
    "table_caption": "Table: Comparison of the state-of-the-art methods with our single model performance on VQAv1.0 test-dev and test-standard server.",
    "table_content": "|| Methods | Test-dev Y/N | Test-dev No. | Test-dev Other | Test-dev All | Test-standard Y/N | Test-standard No. | Test-standard Other | Test-standard All ||\n|| RAF (Ours) | [BOLD] 85.9 | [BOLD] 41.3 | [BOLD] 58.7 | [BOLD] 68.0 | [BOLD] 85.8 | 41.4 | 58.9 | [BOLD] 68.2 ||\n|| ReasonNetIlievski and Feng ( 2017 ) | - | - | - | - | 84.0 | 38.7 | [BOLD] 60.4 | 67.9 ||\n|| MFB+CoAtt+Glove Yu et al. ( 2018 ) | 85.0 | 39.7 | 57.4 | 66.8 | 85.0 | 39.5 | 57.4 | 66.9 ||\n|| Dual-MFA Lu et al. ( 2017 ) | 83.6 | 40.2 | 56.8 | 66.0 | 83.4 | 40.4 | 56.9 | 66.1 ||\n|| MLB+VG Kim et al. ( 2016 ) | 84.1 | 38.0 | 54.9 | 65.8 | - | - | - | - ||\n|| MCB+Att+GloVe Fukui et al. ( 2016 ) | 82.3 | 37.2 | 57.4 | 65.4 | - | - | - | - ||\n|| MLAN Yu et al. ( 2017 ) | 81.8 | 41.2 | 56.7 | 65.3 | 81.3 | [BOLD] 41.9 | 56.5 | 65.2 ||\n|| MUTAN Ben-Younes et al. ( 2017 ) | 84.8 | 37.7 | 54.9 | 65.2 | - | - | - | - ||\n|| DAN (ResNet) Nam et al. ( 2016 ) | 83.0 | 39.1 | 53.9 | 64.3 | 82.8 | 38.1 | 54.0 | 64.2 ||\n|| HieCoAtt Lu et al. ( 2016 ) | 79.7 | 38.7 | 51.7 | 61.8 | - | - | - | 62.1 ||\n|| A+C+K+LSTMWu et al. ( 2016 ) | 81.0 | 38.4 | 45.2 | 59.2 | 81.1 | 37.1 | 45.8 | 59.4 ||\n|| VQA LSTM Q+I Antol et al. ( 2015 ) | 80.5 | 36.8 | 43.1 | 57.8 | 80.6 | 36.5 | 43.7 | 58.2 ||\n|| SANYang et al. ( 2016 ) | 79.3 | 36.6 | 46.1 | 58.7 | - | - | - | 58.9 ||\n|| AYN Malinowski et al. ( 2017 ) | 78.4 | 36.4 | 46.3 | 58.4 | 78.2 | 37.1 | 45.8 | 59.4 ||\n|| NMN Andreas et al. ( 2016 ) | 81.2 | 38.0 | 44.0 | 58.6 | - | - | - | 58.7 ||\n|| DMN+ Xiong et al. ( 2016 ) | 60.3 | 80.5 | 48.3 | 56.8 | - | - | - | 60.4 ||\n|| iBowling Zhou et al. ( 2015 ) | 76.5 | 35.0 | 42.6 | 55.7 | 76.8 | 35.0 | 42.6 | 55.9 ||",
    "claim": "The RAF model achieves the highest overall accuracy on the test-dev set, but not on the test-standard set.",
    "label": "refute"
  },
  {
    "id": "training_967_refute",
    "table_caption": "Table: F1 scores of each model for the claims with various context length values.",
    "table_content": "|| [EMPTY] | C [ITALIC] l=1 | C [ITALIC] l=2 | C [ITALIC] l=3 | C [ITALIC] l=4 ||\n|| BERT models | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| Claim only | 48.61±3.16 | 53.15±1.95 | 54.51±1.91 | 50.89±2.95 ||\n|| Claim + Parent | 51.49±2.63 | 54.78±2.95 | 54.94±2.72 | 51.94±2.59 ||\n|| Claim + Context [ITALIC] f(2) | 52.84±2.55 | 53.77±1.00 | 55.24±2.52 | 57.04±1.19 ||\n|| Claim + Context [ITALIC] f(3) | [BOLD] 54.88± [BOLD] 2.49 | 54.71±1.74 | 52.93±2.07 | [BOLD] 58.17± [BOLD] 1.89 ||\n|| Claim + Context [ITALIC] f(4) | 54.47±2.95 | [BOLD] 54.88± [BOLD] 1.53 | [BOLD] 57.11± [BOLD] 3.38 | 57.02±2.22 ||",
    "claim": "For claims with context length 1, the \"Claim only\" configuration achieves higher F1 scores than \"Claim + Context f(3)\" and \"Claim + Context f(4).\" For context lengths 3 and 4, \"Claim + Context f(4)\" and \"Claim + Context f(3)\" configurations achieve higher F1 scores than \"Claim only.\"",
    "label": "refute"
  },
  {
    "id": "training_1028_refute",
    "table_caption": "Table: Comparison of segmentation performance on RefCOCO, RefCOCO+, and our results on RefCOCOg.",
    "table_content": "|| RefCOCOg Model | RefCOCOg Backbone Net | RefCOCOg Split | RefCOCOg Pr@0.5 | RefCOCOg Pr@0.6 | RefCOCOg Pr@0.7 | RefCOCOg Pr@0.8 | RefCOCOg Pr@0.9 | RefCOCOg IoU ||\n|| MAttNet | res101-mrcn | val | 64.48 | 61.52 | 56.50 | 43.97 | 14.67 | 47.64 ||\n|| MAttNet | res101-mrcn | test | 65.60 | 62.92 | 57.31 | 44.44 | 12.55 | 48.61 ||",
    "claim": "MAttNet with res101-mrcn achieves a Precision@0.5 of 64.48 on the validation split and 64.60 on the test split of the RefCOCOg dataset.",
    "label": "refute"
  },
  {
    "id": "training_998_refute",
    "table_caption": "Table: Ablation Study on VQAv2 val-set.",
    "table_content": "|| Cat. | Methods | Val-set ||\n|| I | RAF-I(ResNet) | 53.9 ||\n|| [EMPTY] | HieCoAtt Lu et al. ( 2016 ); Goyal et al. ( 2016 ) | 54.6 ||\n|| [EMPTY] | RAF-I(ResNeXt) | 58.0 ||\n|| [EMPTY] | MCB Fukui et al. ( 2016 ); Goyal et al. ( 2016 ) | 59.1 ||\n|| [EMPTY] | MUTAN Ben-Younes et al. ( 2017 ) | 60.1 ||\n|| II | Up-DownAnderson et al. ( 2018 ) | 63.2 ||\n|| [EMPTY] | RAF-O(ResNet) | 63.9 ||\n|| III | RAF-IO(ResNet-ResNet) | 64.0 ||\n|| [EMPTY] | RAF-IO(ResNeXt-ResNet) | 64.2 ||",
    "claim": "In Category III, combining image and object-level features yields the best results with RAF-IO(ResNet-ResNet) achieving the highest score of 64.0.",
    "label": "refute"
  },
  {
    "id": "training_1006_refute",
    "table_caption": "Table: (a) TrecQA: raw and clean",
    "table_content": "|| [BOLD] Models | [BOLD] MAP | [BOLD] MRR ||\n|| [ITALIC]  [BOLD] Raw version | [ITALIC]  [BOLD] Raw version | [ITALIC]  [BOLD] Raw version ||\n|| aNMM  | 0.750 | 0.811 ||\n|| PWIM  | 0.758 | 0.822 ||\n|| MP CNN  | 0.762 | 0.830 ||\n|| HyperQA  | 0.770 | 0.825 ||\n|| PR+CNN  | 0.780 | 0.834 ||\n|| [BOLD] DRCN | [BOLD] 0.804 | [BOLD] 0.862 ||\n|| [ITALIC]  [BOLD] clean version | [ITALIC]  [BOLD] clean version | [ITALIC]  [BOLD] clean version ||\n|| HyperQA  | 0.801 | 0.877 ||\n|| PR+CNN  | 0.801 | 0.877 ||\n|| BiMPM  | 0.802 | 0.875 ||\n|| Comp.-Aggr.  | 0.821 | 0.899 ||\n|| IWAN  | 0.822 | 0.889 ||\n|| [BOLD] DRCN | [BOLD] 0.830 | [BOLD] 0.908 ||",
    "claim": "DRCN achieves the highest MAP and MRR scores only in the raw version.",
    "label": "refute"
  },
  {
    "id": "training_1004_refute",
    "table_caption": "Table: (b) SelQA",
    "table_content": "|| [BOLD] Models | [BOLD] MAP | [BOLD] MRR ||\n|| CNN-DAN  | 0.866 | 0.873 ||\n|| CNN-hinge  | 0.876 | 0.881 ||\n|| ACNN  | 0.874 | 0.880 ||\n|| AdaQA  | 0.891 | 0.898 ||\n|| [BOLD] DRCN | [BOLD] 0.925 | [BOLD] 0.930 ||",
    "claim": "AdaQA achieves the highest MAP and MRR scores among the models listed.",
    "label": "refute"
  },
  {
    "id": "training_832_refute",
    "table_caption": "Table: Human Evaluation on the MusicConvers dataset.",
    "table_content": "|| Models | Grammar | Context Relevance | Correctness ||\n|| S2SA | 1.76 | 0.87 | 0.16 ||\n|| GenQA | 1.28 | 0.95 | 0.41 ||\n|| GenQAD | 1.67 | 1.11 | 0.51 ||\n|| GenDS-Single | [BOLD] 2.16 | [BOLD] 1.67 | [BOLD] 1.18 ||\n|| GenDS-Static | 1.97 | 1.42 | 0.96 ||\n|| GenDS | 2.03 | 1.55 | 0.89 ||",
    "claim": "GenDS-Single achieves the best performance in terms of grammar and context relevance, while GenDS-Static achieves the best performance in information correctness.",
    "label": "refute"
  },
  {
    "id": "training_1013_refute",
    "table_caption": "Table: BLEU and METEOR scores of our NMT based submissions for Task 2.",
    "table_content": "|| System | Validation METEOR | Validation BLEU | Test METEOR | Test BLEU ||\n|| Monomodal | 36.3 | 24.0 | 35.1 | 23.8 ||\n|| Multimodal | 34.4 | 19.3 | 32.3 | 19.2 ||",
    "claim": "The Multimodal system outperforms the Monomodal system in both validation and test METEOR and BLEU scores.",
    "label": "refute"
  },
  {
    "id": "training_1074_refute",
    "table_caption": "Table: TempRelPro performances in terms of coverage (Cov), precision (P), recall (R) and F1-score (F1) for all domains, compared with systems in QA TempEval augmented with TREFL.",
    "table_content": "|| [BOLD] System | [BOLD] Cov | [BOLD] P | [BOLD] R | [BOLD] F1 ||\n|| TempRelPro | 0.53 | 0.65 | 0.34 | 0.45 ||\n|| [BOLD] TempRelPro + coref | 0.53 | 0.66 | [BOLD] 0.35 | [BOLD] 0.46 ||\n|| HLT-FBK + trefl | 0.48 | 0.61 | 0.29 | 0.39 ||\n|| HLT-FBK + coref + trefl | [BOLD] 0.67 | 0.51 | 0.34 | 0.40 ||\n|| HITSZ-ICRC + trefl | 0.15 | 0.58 | 0.09 | 0.15 ||\n|| CAEVO + trefl | 0.36 | 0.60 | 0.21 | 0.32 ||\n|| TIPSemB + trefl | 0.37 | 0.64 | 0.24 | 0.35 ||\n|| TIPSem + trefl | 0.40 | [BOLD] 0.68 | 0.27 | 0.38 ||",
    "claim": "TempRelPro + coref achieves the best performance with 35% recall and 48% F1-score.",
    "label": "refute"
  },
  {
    "id": "training_1055_refute",
    "table_caption": "Table: Efficiency of different search methods on PTB and WT-2.",
    "table_content": "|| [BOLD] Search Method | [BOLD] Search Cost (GPU days)  [BOLD] PTB | [BOLD] Search Cost (GPU days)  [BOLD] WT-2 | [BOLD] Method Class ||\n|| NAS Zoph and Le ( 2016 ) | 1,000 CPU days | n.a. | reinforcement ||\n|| ENAS Pham et al. ( 2018 ) | 0.5 | 0.5 | reinforcement ||\n|| DARTS (first order) Liu et al. ( 2018 ) | 0.5 | 1 | gradient descent ||\n|| DARTS (second order) Liu et al. ( 2018 ) | 1 | 1 | gradient descent ||\n|| BERT-CAS (Our) | [BOLD] 0.15 | [BOLD] 0.38 | greedy search ||\n|| GPT-CAS (Our) | 0.23 | 0.53 | greedy search ||",
    "claim": "ENAS has the lowest search cost compared to all other methods.",
    "label": "refute"
  },
  {
    "id": "training_1050_refute",
    "table_caption": "Table: Performance comparison between different suicide risk detection model, where Acc and F1 represent accuracy and F1-score respectively.",
    "table_content": "|| [EMPTY] | Full testset Acc | Full testset F1 | Harder sub-testset Acc | Harder sub-testset F1 ||\n|| SVM | 70.34 | 69.01 | 61.17 | 64.11 ||\n|| NB | 69.59 | 70.12 | 65.14 | 62.20 ||\n|| LSTM | 88.00 | 88.14 | 76.89 | 75.32 ||\n|| SDM | [BOLD] 91.33 | [BOLD] 90.92 | [BOLD] 85.51 | [BOLD] 84.77 ||",
    "claim": "LSTM achieves 3.33% higher accuracy and 2.78% higher F1-score than SDM on the full test set.",
    "label": "refute"
  },
  {
    "id": "training_969_refute",
    "table_caption": "Table: Number of claims, with at least 5 votes, above the given threshold of agreement percentage for 3-class and 5-class cases. When we combine the low impact and high impact classes, there are more claims with high agreement score.",
    "table_content": "|| [EMPTY] | 3-class case | 5-class case ||\n|| Agreement score | Number of claims | Number of claims ||\n|| >50% | 10,848 | 7,304 ||\n|| >60% | 7,386 | 4,329 ||\n|| >70% | 4,412 | 2,195 ||\n|| >80% | 2,068 | 840 ||",
    "claim": "The 3-class case has more claims with agreement scores over 50%, 60%, and 70%, but the 5-class case has more claims with agreement scores over 80%.",
    "label": "refute"
  },
  {
    "id": "training_1054_refute",
    "table_caption": "Table: Performance of Coordinate Architecture Search (CAS). ‘Val’ and ‘Test’ denote validation and test perplexity respectively.",
    "table_content": "|| [BOLD] Model | [BOLD] Datasets  [BOLD] PTB | [BOLD] Datasets  [BOLD] PTB | [BOLD] Datasets  [BOLD] WT-2 | [BOLD] Datasets  [BOLD] WT-2 | [BOLD] Datasets  [BOLD] WT-103 | [BOLD] Datasets  [BOLD] WT-103 ||\n|| [BOLD] Model | [BOLD] Val | [BOLD] Test | [BOLD] Val | [BOLD] Test | [BOLD] Val | [BOLD] Test ||\n|| AWD-LSTM-MoS-BERTVocab | 43.47 | 38.04 | 48.48 | 42.25 | 54.94 | 52.91 ||\n|| BERT | 72.99 | 62.40 | 79.76 | 69.32 | 109.54 | 107.30 ||\n|| BERT-CAS (Our) | 39.97 | 34.47 | 38.43 | 34.64 | 40.70 | 39.85 ||\n|| BERT-Large-CAS (Our) | [BOLD] 36.14 | [BOLD] 31.34 | [BOLD] 37.79 | [BOLD] 34.11 | [BOLD] 19.67 | [BOLD] 20.42 ||\n|| AWD-LSTM-MoS-GPTVocab | 50.20 | 44.92 | 55.03 | 49.77 | 52.90 | 51.88 ||\n|| GPT | 79.44 | 68.79 | 89.96 | 80.60 | 63.07 | 63.47 ||\n|| GPT-CAS (Our) | [BOLD] 46.24 | [BOLD] 40.87 | [BOLD] 50.41 | [BOLD] 46.62 | [BOLD] 35.75 | [BOLD] 34.24 ||",
    "claim": "GPT and BERT have lower validation and test scores (indicating better performance) than AWD-LSTM-MoS on the PTB, WT-2, and WT-103 datasets.",
    "label": "refute"
  },
  {
    "id": "training_919_refute",
    "table_caption": "Table: Adapted Results",
    "table_content": "|| Model | WER | WER 100 | PER ||\n|| wFST | 88.04 | 69.80 | 48.01 ||\n|| LangID-High | 74.99 | 46.18 | 42.64 ||\n|| LangID-Adapted | 75.06 | 46.39 | 41.77 ||\n|| LangID-All | [BOLD] 74.10 | [BOLD] 43.23 | [BOLD] 37.85 ||\n|| NoLangID-High | 82.14 | 50.17 | 54.05 ||\n|| NoLangID-Adapted | 85.11 | 48.24 | 55.93 ||\n|| NoLangID-All | 83.65 | 47.13 | 51.87 ||",
    "claim": "LangID-All achieves the best performance in WER, WER 100, and PER. Using a language ID token does not improve results considerably.",
    "label": "refute"
  },
  {
    "id": "training_1023_refute",
    "table_caption": "Table: Transfer learning with English reused either in source (encoder) or target (decoder). The column “Transfer” is our method, baselines correspond to training on one of the corpora only. Scores (BLEU) are always for the child language pair and they are comparable only within lines or when the child language pair is the same. “Unrelated” language pairs in bold. Upper part: parent larger, lower part: child larger. (“EN” lowercased just to stand out.)",
    "table_content": "|| Parent - Child | Transfer | Baselines: Only Child | Baselines: Only Parent ||\n|| enFI - enET | 19.74‡ | 17.03 | 2.32 ||\n|| FIen - ETen | 24.18‡ | 21.74 | 2.44 ||\n|| [BOLD] enCS - enET | 20.41‡ | 17.03 | 1.42 ||\n|| [BOLD] enRU - enET | 20.09‡ | 17.03 | 0.57 ||\n|| [BOLD] RUen - ETen | 23.54‡ | 21.74 | 0.80 ||\n|| enCS - enSK | 17.75‡ | 16.13 | 6.51 ||\n|| CSen - SKen | 22.42‡ | 19.19 | 11.62 ||\n|| enET - enFI | 20.07‡ | 19.50 | 1.81 ||\n|| ETen - FIen | 23.95 | 24.40 | 1.78 ||\n|| enSK - enCS | 22.99 | 23.48‡ | 6.10 ||\n|| SKen - CSen | 28.20 | 29.61‡ | 4.16 ||",
    "claim": "The BLEU score for ENET improves by 2.71 when the parent model is ENCS, compared to an improvement of 3.38 when the parent model is ENFI.",
    "label": "refute"
  },
  {
    "id": "training_1014_refute",
    "table_caption": "Table: Class distribution for discuss/stance classification.",
    "table_content": "|| [EMPTY] | [BOLD] Instances | [BOLD] Neutral | [BOLD] Stance ||\n|| [BOLD] Train | 13,427 | 8,909 | 4,518 ||\n|| [BOLD] Test | 7,064 | 4,464 | 2,600 ||",
    "claim": "The number of neutral documents is about the same as the number of stance documents in both the train and test datasets.",
    "label": "refute"
  },
  {
    "id": "training_1033_refute",
    "table_caption": "Table: Word error rate and summarization quality by the models with length count-down. Adaptation results in large gains. The model with length encoding outperforms the baseline and the one with length embedding.",
    "table_content": "|| [BOLD] Model | [BOLD] Ratio (output to desired length) | [BOLD] WER | [BOLD] R-1 | [BOLD] R-2 | [BOLD] R-L ||\n|| (1): Baseline (satisfy length) | 0.97 | 55.1 | 62.5 | 41.5 | 60.4 ||\n|| (2): + adapt | 0.96 | 39.9 | 74.6 | [BOLD] 57.0 | 72.6 ||\n|| (3): Length embedding | 1.00 | 57.8 | 61.9 | 40.5 | 59.8 ||\n|| (4): + adapt | 0.96 | 39.3 | 74.3 | 55.2 | 72.5 ||\n|| (5): Length encoding | 1.00 | 57.4 | 62.6 | 40.7 | 60.1 ||\n|| (6): + adapt | 0.96 | [BOLD] 38.6 | [BOLD] 75.1 | 56.4 | [BOLD] 73.2 ||",
    "claim": "The model with length embedding and adaptation achieves the lowest WER and the highest R-1 and R-L scores.",
    "label": "refute"
  },
  {
    "id": "training_1043_refute",
    "table_caption": "Table: Mean Average Precision on Wiki and APR. “∇” means the number is directly from the original paper.",
    "table_content": "|| [BOLD] Methods | [BOLD] Wiki MAP@10 | [BOLD] Wiki MAP@20 | [BOLD] Wiki MAP@50 | [BOLD] APR MAP@10 | [BOLD] APR MAP@20 | [BOLD] APR MAP@50 ||\n|| Egoset Rong et al. ( 2016 ) | 0.904 | 0.877 | 0.745 | 0.758 | 0.710 | 0.570 ||\n|| SetExpan Shen et al. ( 2017 ) | 0.944 | 0.921 | 0.720 | 0.789 | 0.763 | 0.639 ||\n|| SetExpander Mamou et al. ( 2018 ) | 0.499 | 0.439 | 0.321 | 0.287 | 0.208 | 0.120 ||\n|| CaSE Yu et al. ( 2019b ) | 0.897 | 0.806 | 0.588 | 0.619 | 0.494 | 0.330 ||\n|| MCTS Yan et al. ( 2019 ) | 0.980∇ | 0.930∇ | 0.790∇ | 0.960∇ | 0.900∇ | 0.810∇ ||\n|| CGExpan-NoCN | 0.968 | 0.945 | 0.859 | 0.909 | 0.902 | 0.787 ||\n|| CGExpan-NoFilter | 0.990 | 0.975 | 0.890 | 0.979 | 0.962 | 0.892 ||\n|| CGExpan-Comb | 0.991 | 0.974 | 0.895 | 0.983 | 0.984 | 0.937 ||\n|| CGExpan-MRR | [BOLD] 0.995 | [BOLD] 0.978 | [BOLD] 0.902 | [BOLD] 0.992 | [BOLD] 0.990 | [BOLD] 0.955 ||",
    "claim": "CGExpan-NoFilter achieves the highest MAP@50 scores on both the Wiki and APR datasets, outperforming all baseline methods.",
    "label": "refute"
  },
  {
    "id": "training_1049_refute",
    "table_caption": "Table: Performance comparison for different word embedding and different detection model, where “So-W2v”, “So-Glove” and “So-FastText” represent suicide-oriented word embeddings based on Word2vec, GloVe and FastText respectively. Acc and F1 represent accuracy and F1-score.",
    "table_content": "|| LSTM | Acc(%) | Word2vec 79.21 | GloVe 80.17 | FastText 82.59 | Bert 85.15 | So-W2v 86.00 | So-GloVe 86.45 | So-FastText  [BOLD] 88.00 ||\n|| LSTM | F1(%) | 78.58 | 79.98 | 82.18 | 85.69 | 86.17 | 86.69 | [BOLD] 88.14 ||\n|| SDM | Acc(%) | 86.54 | 86.55 | 87.08 | 88.89 | 90.83 | 91.00 | [BOLD] 91.33 ||\n|| SDM | F1(%) | 86.63 | 85.13 | 86.91 | 87.44 | 90.55 | 90.56 | [BOLD] 90.92 ||",
    "claim": "Bert does not outperform other word embeddings without the suicide-related dictionary, while suicide-oriented word embeddings based on FastText achieve the best performance with the dictionary.",
    "label": "refute"
  },
  {
    "id": "training_972_refute",
    "table_caption": "Table: Varying encoders from pretrained Transformers in our sequence labeling system. Training was done on data from training stage II only.",
    "table_content": "|| [BOLD] Encoder | [BOLD] CoNLL-2014 (test)  [BOLD] P | [BOLD] CoNLL-2014 (test)  [BOLD] R | [BOLD] CoNLL-2014 (test)  [BOLD] F0.5 | [BOLD] BEA-2019 (dev)  [BOLD] P | [BOLD] BEA-2019 (dev)  [BOLD] R | [BOLD] BEA-2019 (dev)  [BOLD] F0.5 ||\n|| LSTM | 51.6 | 15.3 | 35.0 | - | - | - ||\n|| ALBERT | 59.5 | 31.0 | 50.3 | 43.8 | 22.3 | 36.7 ||\n|| BERT | 65.6 | 36.9 | 56.8 | 48.3 | 29.0 | 42.6 ||\n|| GPT-2 | 61.0 | 6.3 | 22.2 | 44.5 | 5.0 | 17.2 ||\n|| RoBERTa | [BOLD] 67.5 | 38.3 | [BOLD] 58.6 | [BOLD] 50.3 | 30.5 | [BOLD] 44.5 ||\n|| XLNet | 64.6 | [BOLD] 42.6 | 58.5 | 47.1 | [BOLD] 34.2 | 43.8 ||",
    "claim": "RoBERTa achieves the highest F0.5 score on the CoNLL-2014 dataset, while XLNet achieves the highest F0.5 score on the BEA-2019 dataset.",
    "label": "refute"
  },
  {
    "id": "training_968_refute",
    "table_caption": "Table: Number of claims for the given range of number of votes. There are 19,512 claims in the dataset with 3 or more votes. Out of the claims with 3 or more votes, majority of them have 5 or more votes.",
    "table_content": "|| # impact votes | # claims ||\n|| [3,5) | 4,495 ||\n|| [5,10) | 5,405 ||\n|| [10,15) | 5,338 ||\n|| [15,20) | 2,093 ||\n|| [20,25) | 934 ||\n|| [25,50) | 992 ||\n|| [50,333) | 255 ||",
    "claim": "The majority of claims with 3 or more votes have fewer than 5 votes.",
    "label": "refute"
  },
  {
    "id": "training_1091_refute",
    "table_caption": "Table: Results on WMT’14 English-German translation.",
    "table_content": "|| Method | Model | BLEU ||\n|| Vaswani et al. | Trans.-Base | 27.30 ||\n|| Vaswani et al. | Trans.-Big | 28.40 ||\n|| Chen et al. | RNMT+ | 28.49 ||\n|| Ours | Trans.-Base | 28.34 ||\n|| Ours | Trans.-Big | [BOLD] 30.01 ||",
    "claim": "The \"Ours Trans.-Base\" model achieves the highest BLEU score of 30.01.",
    "label": "refute"
  },
  {
    "id": "training_1059_refute",
    "table_caption": "Table: The performance of non-autoregressive machine translation methods on the WMT14 EN-DE and WMT16 EN-RO test data. The Step columns indicate the average number of sequential transformer passes. Shaded results use a small transformer (d_{model}=d_{hidden}=512). Our EN-DE results show the scores after conventional compound splitting (luong-etal-2015-effective; Vaswani2017AttentionIA).",
    "table_content": "|| [BOLD] Model n: # rescored candidates | [BOLD] en\\rightarrowde Step | [BOLD] en\\rightarrowde BLEU | [BOLD] de\\rightarrowen Step | [BOLD] de\\rightarrowen BLEU | [BOLD] en\\rightarrowro Step | [BOLD] en\\rightarrowro BLEU | [BOLD] ro\\rightarrowen Step | [BOLD] ro\\rightarrowen BLEU ||\n|| Gu2017NonAutoregressiveNM (n=100) | 1 | 19.17 | 1 | 23.20 | 1 | 29.79 | 1 | 31.44 ||\n|| Wang2019NonAutoregressiveMT (n=9) | 1 | 24.61 | 1 | 28.90 | – | – | – | – ||\n|| Li2019HintBasedTF (n=9) | 1 | 25.20 | 1 | 28.80 | – | – | – | – ||\n|| Ma2019FlowSeqNC (n=30) | 1 | 25.31 | 1 | 30.68 | 1 | 32.35 | 1 | 32.91 ||\n|| Sun2019Fast (n=19) | 1 | 26.80 | 1 | 30.04 | – | – | – | – ||\n|| ReorderNAT | 1 | 26.51 | 1 | 31.13 | 1 | 31.70 | 1 | 31.99 ||\n|| Shu2019LatentVariableNN (n=50) | 1 | 25.1 | – | – | – | – | – | – ||\n|| [BOLD] Iterative NAT Models | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| Lee2018DeterministicNN | 10 | 21.61 | 10 | 25.48 | 10 | 29.32 | 10 | 30.19 ||\n|| Ghazvininejad2019MaskPredictPD (CMLM) | 4 | 25.94 | 4 | 29.90 | 4 | 32.53 | 4 | 33.23 ||\n|| [EMPTY] | 10 | 27.03 | 10 | 30.53 | 10 | 33.08 | 10 | 33.31 ||\n|| Gu2019LevenshteinT (LevT) | 7+ | 27.27 | – | – | – | – | 7+ | 33.26 ||\n|| [BOLD] Our Implementations | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| CMLM + Mask-Predict | 4 | 26.73 | 4 | 30.75 | 4 | 33.02 | 4 | 33.27 ||\n|| CMLM + Mask-Predict | 10 | [BOLD] 27.39 | 10 | 31.24 | 10 | [BOLD] 33.33 | 10 | [BOLD] 33.67 ||\n|| DisCo + Mask-Predict | 4 | 25.83 | 4 | 30.15 | 4 | 32.22 | 4 | 32.92 ||\n|| DisCo + Mask-Predict | 10 | 27.06 | 10 | 30.89 | 10 | 32.92 | 10 | 33.12 ||\n|| DisCo + Easy-First | 4.82 | 27.34 | 4.23 | [BOLD] 31.31 | 3.29 | 33.22 | 3.10 | 33.25 ||\n|| [BOLD] AT Models | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| Vaswani2017AttentionIA (base) | N | 27.3 | – | – | – | – | – | – ||\n|| Vaswani2017AttentionIA (large) | N | 28.4 | – | – | – | – | – | – ||\n|| [BOLD] Our Implementations | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| AT Transformer Base (EN-RO teacher) | N | 27.38 | N | 31.78 | N | 34.16 | N | 34.46 ||\n|| AT Transformer Base + Distillation | N | 28.24 | N | 31.54 | – | – | – | – ||\n|| AT Transformer Large (EN-DE teacher) | N | 28.60 | N | 31.71 | – | – | – | – ||",
    "claim": "Our re-implementations of CMLM + Mask-Predict achieve a BLEU score of 31.24 in de→en with 10 steps, which is lower than Ghazvininejad2019MaskPredictPD, which has a BLEU score of 30.53.",
    "label": "refute"
  },
  {
    "id": "training_1015_refute",
    "table_caption": "Table: Class distribution for discuss/stance classification.",
    "table_content": "|| [EMPTY] | [BOLD] Instances | [BOLD] Unrelated | [BOLD] Related ||\n|| [BOLD] Train | 49,972 | 36,545 | 13,427 ||\n|| [BOLD] Test | 25,413 | 18,349 | 7,064 ||",
    "claim": "In the train dataset, there are 13,427 related instances and 36,545 unrelated instances, while in the test dataset, there are 18,349 related instances and 7,064 unrelated instances.",
    "label": "refute"
  },
  {
    "id": "training_1072_refute",
    "table_caption": "Table: CauseRelPro-beta’s micro-averaged scores using different degrees of polynomial kernel.",
    "table_content": "|| [EMPTY] | [BOLD] P | [BOLD] R | [BOLD] F1 ||\n|| 2 [ITALIC] nd degree | 0.5031 | 0.2516 | 0.3354 ||\n|| 3 [ITALIC] rd degree | 0.5985 | 0.2484 | 0.3511 ||\n|| 4 [ITALIC] th degree | 0.6337 | 0.2013 | 0.3055 ||",
    "claim": "The best F1-score is achieved by using the 4th degree of polynomial kernel, while the best precision of 0.6337 is achieved with degree 4.",
    "label": "refute"
  },
  {
    "id": "training_1068_refute",
    "table_caption": "Table: Mean Average Precision across all queries on Wiki and APR.",
    "table_content": "|| Methods | [ITALIC] Wiki MAP@10 | [ITALIC] Wiki MAP@20 | [ITALIC] Wiki MAP@50 | [ITALIC] APR MAP@10 | [ITALIC] APR MAP@20 | [ITALIC] APR MAP@50 ||\n|| CaSE | 0.897 | 0.806 | 0.588 | 0.619 | 0.494 | 0.330 ||\n|| SetExpander | 0.499 | 0.439 | 0.321 | 0.287 | 0.208 | 0.120 ||\n|| SetExpan | 0.944 | 0.921 | 0.720 | 0.789 | 0.763 | 0.639 ||\n|| BERT | 0.970 | 0.945 | 0.853 | 0.890 | 0.896 | 0.777 ||\n|| Set-CoExpan (no aux.) | 0.964 | 0.950 | 0.861 | 0.900 | 0.893 | 0.793 ||\n|| Set-CoExpan (no flex.) | 0.973 | 0.961 | 0.886 | 0.927 | 0.908 | 0.823 ||\n|| Set-CoExpan | [BOLD] 0.976 | [BOLD] 0.964 | [BOLD] 0.905 | [BOLD] 0.933 | [BOLD] 0.915 | [BOLD] 0.830 ||",
    "claim": "BERT achieves the highest performance across all methods in both datasets.",
    "label": "refute"
  },
  {
    "id": "training_1009_refute",
    "table_caption": "Table: BLEU and METEOR scores for human description generation experiments.",
    "table_content": "|| Method | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR ||\n|| Image + sentences | 54.30 | 35.95 | 23.28 | 15.06 | 39.16 ||\n|| Image only | 51.26 | 34.74 | 22.63 | 15.01 | 38.06 ||\n|| Sentence only | 39.37 | 23.27 | 13.73 | 8.40 | 32.98 ||\n|| Our system | 60.61 | 44.35 | 31.65 | 21.95 | 33.59 ||",
    "claim": "Our system outperforms other methods on METEOR but not on BLEU metrics.",
    "label": "refute"
  },
  {
    "id": "training_1069_refute",
    "table_caption": "Table: Experiment using different fusion strategies.",
    "table_content": "|| [ITALIC] Embedding | RNN | Fusion | Training | Validation ||\n|| BERT | GRU | Mult | 78.28 | [BOLD] 58.75 ||\n|| BERT | GRU | Concat | 67.85 | 55.07 ||\n|| BERT | GRU | Sum | 68.21 | 54.93 ||",
    "claim": "Element-wise summation achieves the highest validation accuracy, while multiplication provides the lowest validation accuracy.",
    "label": "refute"
  },
  {
    "id": "training_1061_refute",
    "table_caption": "Table: Dev results from bringing training closer to inference.",
    "table_content": "|| Training Variant | [BOLD] en\\rightarrowde Step | [BOLD] en\\rightarrowde BLEU | [BOLD] ro\\rightarrowen Step | [BOLD] ro\\rightarrowen BLEU ||\n|| Random Sampling | 4.29 | [BOLD] 25.60 | 3.17 | [BOLD] 34.97 ||\n|| Easy-First Training | 4.03 | 24.76 | 2.94 | 34.96 ||",
    "claim": "Random Sampling achieves higher BLEU scores than Easy-First Training for the en→de task, while Easy-First Training achieves higher BLEU scores for the ro→en task.",
    "label": "refute"
  },
  {
    "id": "training_993_refute",
    "table_caption": "Table: Experimental results on CMRC 2018 and DRCD. † indicates unpublished works (some of the systems are using development set for training, which makes the results not directly comparable.). ♠ indicates zero-shot approach. We mark our system with an ID in the first column for reference simplicity.",
    "table_content": "|| [BOLD] # | [BOLD] System | [BOLD] CMRC 2018  [BOLD] Dev | [BOLD] CMRC 2018  [BOLD] Dev | [BOLD] CMRC 2018  [BOLD] Test | [BOLD] CMRC 2018  [BOLD] Test | [BOLD] CMRC 2018  [BOLD] Challenge | [BOLD] CMRC 2018  [BOLD] Challenge | [BOLD] DRCD  [BOLD] Dev | [BOLD] DRCD  [BOLD] Dev | [BOLD] DRCD  [BOLD] Test | [BOLD] DRCD  [BOLD] Test ||\n|| [BOLD] # | [BOLD] System | [BOLD] EM | [BOLD] F1 | [BOLD] EM | [BOLD] F1 | [BOLD] EM | [BOLD] F1 | [BOLD] EM | [BOLD] F1 | [BOLD] EM | [BOLD] F1 ||\n|| [EMPTY] | [ITALIC] Human Performance | [ITALIC] 91.1 | [ITALIC] 97.3 | [ITALIC] 92.4 | [ITALIC] 97.9 | [ITALIC] 90.4 | [ITALIC] 95.2 | - | - | [ITALIC] 80.4 | [ITALIC] 93.3 ||\n|| [EMPTY] | P-Reader (single model)† | 59.9 | 81.5 | 65.2 | 84.4 | 15.1 | 39.6 | - | - | - | - ||\n|| [EMPTY] | Z-Reader (single model)† | 79.8 | 92.7 | 74.2 | 88.1 | 13.9 | 37.4 | - | - | - | - ||\n|| [EMPTY] | MCA-Reader (ensemble)† | 66.7 | 85.5 | 71.2 | 88.1 | 15.5 | 37.1 | - | - | - | - ||\n|| [EMPTY] | RCEN (ensemble)† | 76.3 | 91.4 | 68.7 | 85.8 | 15.3 | 34.5 | - | - | - | - ||\n|| [EMPTY] | r-net (single model)† | - | - | - | - | - | - | - | - | 29.1 | 44.4 ||\n|| [EMPTY] | DA (Yang et al.,  2019 ) | 49.2 | 65.4 | - | - | - | - | 55.4 | 67.7 | - | - ||\n|| 1 | GNMT+BERT [ITALIC] SQ− [ITALIC] Ben♠ | 15.9 | 40.3 | 20.8 | 45.4 | 4.2 | 20.2 | 28.1 | 50.0 | 26.6 | 48.9 ||\n|| 2 | GNMT+BERT [ITALIC] SQ− [ITALIC] Len♠ | 16.8 | 42.1 | 21.7 | 47.3 | 5.2 | 22.0 | 28.9 | 52.0 | 28.7 | 52.1 ||\n|| 3 | GNMT+BERT [ITALIC] SQ− [ITALIC] Len+SimpleMatch♠ | 26.7 | 56.9 | 31.3 | 61.6 | 9.1 | 35.5 | 36.9 | 60.6 | 37.0 | 61.2 ||\n|| 4 | GNMT+BERT [ITALIC] SQ− [ITALIC] Len+Aligner | 46.1 | 66.4 | 49.8 | 69.3 | 16.5 | 40.9 | 60.1 | 70.5 | 59.5 | 70.7 ||\n|| 5 | GNMT+BERT [ITALIC] SQ− [ITALIC] Len+Verifier | 64.7 | 84.7 | 68.9 | 86.8 | 20.0 | 45.6 | 83.5 | 90.1 | 82.6 | 89.6 ||\n|| 6 | BERT [ITALIC] Bcn | 63.6 | 83.9 | 67.8 | 86.0 | 18.4 | 42.1 | 83.4 | 90.1 | 81.9 | 89.0 ||\n|| 7 | BERT [ITALIC] Bmul | 64.1 | 84.4 | 68.6 | 86.8 | 18.6 | 43.8 | 83.2 | 89.9 | 82.4 | 89.5 ||\n|| 8 | [BOLD] Dual BERT | 65.8 | 86.3 | 70.4 | 88.1 | 23.8 | 47.9 | 84.5 | 90.8 | 83.7 | 90.3 ||\n|| 9 | BERT [ITALIC] SQ− [ITALIC] Bmul♠ | 56.5 | 77.5 | 59.7 | 79.9 | 18.6 | 41.4 | 66.7 | 81.0 | 65.4 | 80.1 ||\n|| 10 | BERT [ITALIC] SQ− [ITALIC] Bmul + Cascade Training | 66.6 | 87.3 | 71.8 | 89.4 | 25.6 | 52.3 | 85.2 | 91.4 | 84.4 | 90.8 ||\n|| 11 | BERT [ITALIC] Bmul + Mixed Training | 66.8 | 87.5 | 72.6 | 89.8 | 26.7 | 53.4 | 85.3 | 91.6 | 84.7 | 91.2 ||\n|| 12 | [BOLD] Dual BERT (w/ SQuAD) | 68.0 | 88.1 | 73.6 | 90.2 | 27.8 | 55.2 | 86.0 | 92.1 | 85.4 | 91.6 ||",
    "claim": "The application of SimpleMatch (#3) improves performance over the baseline systems (#1 and #2), and SimpleMatch further enhances performance beyond the Answer Aligner (#4). The Answer Verifier (#5) surpasses the multi-lingual BERT baseline (#7).",
    "label": "refute"
  },
  {
    "id": "training_990_refute",
    "table_caption": "Table: Performances on sparse and plentiful learning data with different numbers of interlocutor’s dialogue turns, where the test data is divided into different intervals according to the number of dialogue turns in training dataset said by target addressee (named as interlocutor’s dialogue turns).",
    "table_content": "|| [BOLD] Interlocutor’s | [BOLD] Persona Model | [BOLD] Persona Model | [BOLD] ICRED (ours) | [BOLD] ICRED (ours) ||\n|| [BOLD] Dialogue Turns | [BOLD]  BLEU | [BOLD] ROUGE | [BOLD] BLEU | [BOLD] ROUGE ||\n|| [0, 100] | 8.47 | 6.72 | 10.63 | 8.60 ||\n|| (100, 1000] | 8.87 | 7.14 | 10.50 | 8.61 ||\n|| (1000, 5000] | 9.48 | 7.74 | [BOLD] 10.77 | [BOLD] 8.90 ||\n|| (5000, +∞) | [BOLD] 9.51 | [BOLD] 7.80 | 10.60 | 8.79 ||",
    "claim": "The Persona Model achieves higher BLEU and ROUGE scores than ICRED across all dialogue turn ranges.",
    "label": "refute"
  },
  {
    "id": "training_1031_refute",
    "table_caption": "Table: Evaluation results.",
    "table_content": "|| [BOLD] Model | [BOLD] BLEU | [BOLD] TER | [BOLD] Adq. | [BOLD] Flu. ||\n|| GNMT | 2.44 | 75.09 | 0.90 | 1.12 ||\n|| CMT1 | 15.09 | 58.04 | 3.18 | 3.57 ||\n|| CMT2 | 16.47 | 55.45 | 3.19 | 3.97 ||",
    "claim": "CMT2 outperforms GNMT by 13.34 BLEU and 18.34 TER, and token reordering in CMT2 reduces fluency compared to CMT1.",
    "label": "refute"
  },
  {
    "id": "training_1029_refute",
    "table_caption": "Table: Object detection results.",
    "table_content": "|| net | [ITALIC] APbb | [ITALIC] APbb50 | [ITALIC] APbb75 ||\n|| res101-frcn | 34.1 | 53.7 | 36.8 ||\n|| res101-mrcn | 35.8 | 55.3 | 38.6 ||",
    "claim": "Faster R-CNN (res101-frcn) has higher AP than Mask R-CNN (res101-mrcn).",
    "label": "refute"
  },
  {
    "id": "training_1058_refute",
    "table_caption": "Table: Results on domains A, B and C for the proposed pretrained DNN method and the baseline CRF/MaxEnt method during experimental early stages of domain development. * denotes statistically significant SER difference between proposed and baseline",
    "table_content": "|| Train Set | Size | Method | [ITALIC] F1 [ITALIC] intent | [ITALIC] F1 [ITALIC] slot | [ITALIC] SER ||\n|| Domain A (5 intents, 36 slots) | Domain A (5 intents, 36 slots) | Domain A (5 intents, 36 slots) | Domain A (5 intents, 36 slots) | Domain A (5 intents, 36 slots) | Domain A (5 intents, 36 slots) ||\n|| Core* | 500 | Baseline | 85.0 | 63.9 | 51.9 ||\n|| data | 500 | Proposed | 86.6 | 66.6 | 48.2 ||\n|| Bootstrap | 18K | Baseline | 86.1 | 72.8 | 49.6 ||\n|| data* | 18K | Proposed | 86.9 | 73.8 | 47.0 ||\n|| Core + | 3.5K | Baseline | 90.4 | 74.3 | 40.5 ||\n|| user data* | 3.5K | Proposed | 90.1 | 75.8 | 37.9 ||\n|| Core + | 43K | Baseline | 92.1 | 80.6 | 33.4 ||\n|| bootstrap + | 43K | Proposed | 91.9 | 80.8 | 32.8 ||\n|| user data | 43K | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| Domain B (2 intents, 17 slots) | Domain B (2 intents, 17 slots) | Domain B (2 intents, 17 slots) | Domain B (2 intents, 17 slots) | Domain B (2 intents, 17 slots) | Domain B (2 intents, 17 slots) ||\n|| Bootstrap | 2K | Baseline | 97.0 | 94.7 | 10.1 ||\n|| data* | 2K | Proposed | 97.8 | 95.3 | 6.3 ||\n|| User data | 2.5K | Baseline | 97.0 | 94.7 | 8.2 ||\n|| [EMPTY] | 2.5K | Proposed | 97.1 | 96.4 | 7.1 ||\n|| Bootstrap + | 52K | Baseline | 96.7 | 95.2 | 8.2 ||\n|| user data* | 52K | Proposed | 97.0 | 96.6 | 6.4 ||\n|| Domain C (22 intents, 43 slots) | Domain C (22 intents, 43 slots) | Domain C (22 intents, 43 slots) | Domain C (22 intents, 43 slots) | Domain C (22 intents, 43 slots) | Domain C (22 intents, 43 slots) ||\n|| Core* | 300 | Baseline | 77.9 | 47.8 | 64.2 ||\n|| data | 300 | Proposed | 85.6 | 46.6 | 51.8 ||\n|| Bootstrap | 26K | Baseline | 46.1 | 65.8 | 64.0 ||\n|| data* | 26K | Proposed | 49.1 | 68.9 | 62.8 ||\n|| Core + | 126K | Baseline | 92.3 | 78.3 | 28.1 ||\n|| bootstrap. + | 126K | Proposed | 92.7 | 72.7 | 31.9 ||\n|| user data* | 126K | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||",
    "claim": "The proposed method consistently achieves lower F1 scores for intent and slot, and higher Slot Error Rate (SER) compared to the baseline across all domains and dataset sizes.",
    "label": "refute"
  },
  {
    "id": "training_1051_refute",
    "table_caption": "Table: Ablation test for SDM with different inputs.",
    "table_content": "|| Inputs | Accuracy | F1-score ||\n|| Text | 88.56 | 87.99 ||\n|| Text+Image | 89.22 | 89.22 ||\n|| Text+User’s feature | 90.66 | 90.17 ||\n|| Text+Image +User’s feature | [BOLD] 91.33 | [BOLD] 90.92 ||",
    "claim": "The combination of Text and Image achieves the highest accuracy and F1-score.",
    "label": "refute"
  },
  {
    "id": "training_980_refute",
    "table_caption": "Table: English→German performance on WMT test sets. Our pretrained model outperforms all other models. Note that the model without pretraining uses the LM objective.",
    "table_content": "|| [ITALIC] System | [ITALIC] ensemble? | [ITALIC] BLEU  [ITALIC] newstest2014 | [ITALIC] BLEU  [ITALIC] newstest2015 ||\n|| Phrase Based MT (Williams et al.,  2016 ) | - | 21.9 | 23.7 ||\n|| Supervised NMT (Jean et al.,  2015 ) | single | - | 22.4 ||\n|| Edit Distance Transducer NMT (Stahlberg et al.,  2016 ) | single | 21.7 | 24.1 ||\n|| Edit Distance Transducer NMT (Stahlberg et al.,  2016 ) | ensemble 8 | 22.9 | 25.7 ||\n|| Backtranslation (Sennrich et al.,  2015a ) | single | 22.7 | 25.7 ||\n|| Backtranslation (Sennrich et al.,  2015a ) | ensemble 4 | 23.8 | 26.5 ||\n|| Backtranslation (Sennrich et al.,  2015a ) | ensemble 12 | [BOLD] 24.7 | 27.6 ||\n|| No pretraining | single | 21.3 | 24.3 ||\n|| Pretrained seq2seq | single | [BOLD] 24.0 | [BOLD] 27.0 ||\n|| Pretrained seq2seq | ensemble 5 | [BOLD] 24.7 | [BOLD] 28.1 ||",
    "claim": "The best single model does not outperform the previous state-of-the-art ensemble of 4 models, and the ensemble of 5 models matches or exceeds the previous best ensemble of 12 models.",
    "label": "refute"
  },
  {
    "id": "training_1116_refute",
    "table_caption": "Table: Recall of set-follow operation with joint training vs. single task.",
    "table_content": "|| [EMPTY] | k=1 | k=10 | k=100 | k=1000 ||\n|| recall (single task) | 9.7 | 49.2 | 78.1 | 91.8 ||\n|| recall (multi task) | 10.3 | 52.1 | 81.7 | 93.0 ||\n|| max recall | 10.3 | 53.1 | 84.3 | 94.1 ||",
    "claim": "Single-task training improves recall performance at all values of k compared to joint training.",
    "label": "refute"
  },
  {
    "id": "training_1035_refute",
    "table_caption": "Table: NLI modeling experiments with RoBERTa, reporting results on the validation sets for MNLI and for the task used for training each model (Self), and the GLUE diagnostic set. We compare the two-class Contrast with a two-class version of MNLI.",
    "table_content": "|| [BOLD] Training Data | [BOLD] Self | [BOLD] MNLI | [BOLD] GLUE Diag. ||\n|| Base | 84.8 | 81.5 | 40.5 ||\n|| Paragraph | 78.3 | 78.2 | 31.7 ||\n|| EditPremise | 82.9 | 79.8 | 35.5 ||\n|| EditOther | 82.5 | 82.6 | 33.9 ||\n|| MNLI8.5k | 87.5 | 87.5 | 44.6 ||\n|| MNLIGov8.5k | 87.7 | 85.4 | 40.7 ||\n|| ANLI8.5k | 35.7 | 85.6 | 39.8 ||\n|| MNLI | 90.4 | [BOLD] 90.4 | 49.2 ||\n|| ANLI | 61.5 | 90.1 | 49.7 ||\n|| MNLI (two-class) | 94.0 | [BOLD] 94.0 | – ||\n|| MNLI8.5k (two-class) | 92.4 | 92.4 | – ||\n|| Contrast | 91.6 | 80.6 | – ||",
    "claim": "The first three interventions, especially EditPremise, show lower hypothesis-only performance than the Base model. MNLI and its two-class variant do not achieve the highest scores on the Self dataset.",
    "label": "refute"
  },
  {
    "id": "training_971_refute",
    "table_caption": "Table: Number of claims for the given range of context length, for claims with more than 5 votes and an agreement score greater than 60%.",
    "table_content": "|| Context length | # claims ||\n|| 1 | 1,524 ||\n|| 2 | 1,977 ||\n|| 3 | 1,181 ||\n|| [4,5] | 1,436 ||\n|| (5,10] | 1,115 ||\n|| >10 | 153 ||",
    "claim": "More than half of the claims have a context length of 4 or higher.",
    "label": "refute"
  },
  {
    "id": "training_1056_refute",
    "table_caption": "Table: Compare model parameter size and results with GPT-2. The GPT-2 model size and results are from Radford et al. (2019).",
    "table_content": "|| [BOLD] Model | [BOLD] Parameters | [BOLD] Datasets PTB | [BOLD] Datasets WT-2 | [BOLD] Datasets WT-103 ||\n|| GPT-2 | 345M | 47.33 | 22.76 | 26.37 ||\n|| GPT-2 | 762M | 40.31 | 19.93 | 22.05 ||\n|| GPT-2 | 1542M | 35.76 | [BOLD] 18.34 | [BOLD] 17.48 ||\n|| BERT-Large-CAS | 395M | [BOLD] 31.34 | 34.11 | 20.42 ||",
    "claim": "On WT-103, GPT-2 (762M) performs better than BERT-Large-CAS.",
    "label": "refute"
  },
  {
    "id": "training_1108_refute",
    "table_caption": "Table: Post-discharge mortality prediction performance",
    "table_content": "|| [BOLD] Model | [BOLD] 30-day | [BOLD] 1-year ||\n|| [BOLD] Model | [BOLD] A.R. | [BOLD] A.R. ||\n||  | 0.80 | 0.77 ||\n||  | 0.82 | 0.81 ||\n|| (retrospective) | 0.82 | 0.81 ||\n||  | 0.858 | 0.853 ||\n|| LSTM | 0.823 | 0.820 ||\n|| CC-LSTM | 0.839 | 0.837 ||",
    "claim": "The LSTM model has higher accuracy rates than the CC-LSTM model for both 30-day and 1-year predictions.",
    "label": "refute"
  },
  {
    "id": "training_1041_refute",
    "table_caption": "Table: Ablations on R2R Validation Seen and Validation Unseen sets, showing results in VLN for different combinations of pre-training tasks. SPL and SR are reported as percentages and NE and PL in meters.",
    "table_content": "|| Method | cma | nvs | [BOLD] Validation Seen PL | [BOLD] Validation Seen NE ↓ | [BOLD] Validation Seen SR ↑ | [BOLD] Validation Seen SPL ↑ | [BOLD] Validation Unseen PL | [BOLD] Validation Unseen NE ↓ | [BOLD] Validation Unseen SR ↑ | [BOLD] Validation Unseen SPL ↑ ||\n|| Speaker-Follower  | - | - | - | 3.36 | 66.4 | - | - | 6.62 | 35.5 | - ||\n|| RCM | - | - | 12.1 | 3.25 | 67.6 | - | 15.0 | 6.01 | 40.6 | - ||\n|| Speaker-Follower (Ours) | ✗ | ✗ | 15.9 | 4.90 | 51.9 | 43.0 | 15.6 | 6.40 | 36.0 | 29.0 ||\n|| Speaker-Follower (Ours) | ✓ | ✗ | 14.9 | 5.04 | 50.2 | 39.2 | 16.8 | 5.85 | 39.1 | 26.8 ||\n|| Speaker-Follower (Ours) | ✗ | ✓ | 16.5 | 5.12 | 48.7 | 34.9 | 18.0 | 6.30 | 34.9 | 20.9 ||\n|| Speaker-Follower (Ours) | ✓ | ✓ | 11.3 | 4.06 | 60.8 | 55.9 | 14.6 | 6.06 | 40.0 | 31.2 ||\n|| RCM (Ours) | ✗ | ✗ | 13.7 | 4.48 | 55.3 | 47.9 | 14.8 | 6.00 | 41.1 | 32.7 ||\n|| RCM (Ours) | ✓ | ✗ | 10.2 | 5.10 | 51.8 | 49.0 | 9.5 | 5.81 | 44.8 | 42.0 ||\n|| RCM (Ours) | ✗ | ✓ | 19.5 | 6.53 | 34.6 | 20.8 | 18.8 | 6.79 | 33.7 | 20.6 ||\n|| RCM (Ours) | ✓ | ✓ | 13.2 | 4.68 | 55.8 | 52.7 | 9.8 | 5.61 | 46.1 | 43.0 ||",
    "claim": "Pre-training with CMA and NVS jointly results in an 11-12% improvement in SR and improved SPL for both SF and RCM agents, while pre-training with NVS only leads to an improvement in performance.",
    "label": "refute"
  },
  {
    "id": "training_958_refute",
    "table_caption": "Table: Results of m-RNN-shared model after applying consensus reranking using nearest neighbors as references (m-RNN-shared-NNref), compared with those of the original m-RNN model on our validation set and MS COCO test server.",
    "table_content": "|| MS COCO val for consensus reranking | MS COCO val for consensus reranking B1 | MS COCO val for consensus reranking B2 | MS COCO val for consensus reranking B3 | MS COCO val for consensus reranking B4 | MS COCO val for consensus reranking CIDEr | MS COCO val for consensus reranking ROUGE_L | MS COCO val for consensus reranking METEOR ||\n|| m-RNN-shared | 0.686 | 0.511 | 0.375 | 0.280 | 0.842 | 0.500 | 0.228 ||\n|| m-RNN-shared-NNref-BLEU | 0.718 | 0.550 | 0.409 | 0.305 | 0.909 | 0.519 | 0.235 ||\n|| m-RNN-shared-NNref-CIDEr | 0.714 | 0.543 | 0.406 | 0.304 | 0.938 | 0.519 | 0.239 ||\n|| m-RNN-shared-NNref-BLEU-Orcale | 0.792 | 0.663 | 0.543 | 0.443 | 1.235 | 0.602 | 0.287 ||\n|| m-RNN-shared-NNref-CIDEr-Oracle | 0.784 | 0.648 | 0.529 | 0.430 | 1.272 | 0.593 | 0.287 ||\n|| MS COCO 2014 test server | MS COCO 2014 test server | MS COCO 2014 test server | MS COCO 2014 test server | MS COCO 2014 test server | MS COCO 2014 test server | MS COCO 2014 test server | MS COCO 2014 test server ||\n|| [EMPTY] | B1 | B2 | B3 | B4 | CIDEr | ROUGE_L | METEOR ||\n|| m-RNN-shared | 0.685 | 0.512 | 0.376 | 0.279 | 0.819 | 0.504 | 0.229 ||\n|| m-RNN-shared-NNref-BLEU | 0.720 | 0.553 | 0.410 | 0.302 | 0.886 | 0.524 | 0.238 ||\n|| m-RNN-shared-NNref-CIDEr | 0.716 | 0.545 | 0.404 | 0.299 | 0.917 | 0.521 | 0.242 ||",
    "claim": "For BLEU-based consensus reranking, there is a decrease of 3.5 points on the validation set and 3.3 points on the MS COCO test 2014 set in terms of BLEU4 score. For CIDEr-based consensus reranking, there is a decrease of 9.4 points on the validation set and 9.8 points on the MS COCO test 2014 set in terms of CIDEr.",
    "label": "refute"
  },
  {
    "id": "training_962_refute",
    "table_caption": "Table: Performance comparison on resplited AAPD, whose test set contains 2000 samples whose label sets occur in the training set (Seen test set) and 2000 samples are not (Unseen test set). OCD (correct prefix) means we only sample correct labels in the training phase.",
    "table_content": "|| Models | Seen test set miF1 | Seen test set ebF1 | Unseen test set miF1 | Unseen test set ebF1 ||\n|| Seq2seq | 0.730 | 0.749 | 0.508 | 0.503 ||\n|| Seq2seq + SS | 0.736 | 0.754 | 0.517 | 0.515 ||\n|| Order-free RNN | 0.732 | 0.746 | 0.496 | 0.494 ||\n|| Order-free RNN + SS | 0.724 | 0.740 | 0.520 | 0.517 ||\n|| OCD (correct prefix) | 0.726 | 0.741 | 0.513 | 0.515 ||\n|| OCD | [BOLD] 0.746 | [BOLD] 0.771 | [BOLD] 0.521 | [BOLD] 0.530 ||",
    "claim": "OCD achieves the highest performance on both seen and unseen test sets, while SS does not improve the performance on the unseen test set for the Seq2seq model.",
    "label": "refute"
  },
  {
    "id": "training_1121_refute",
    "table_caption": "Table: Results on the English-German task. The evaluation metric is case-sensitive tokenized BLEU. The translations of the top three single MT systems are the inputs of the bottom three system combination methods. “††”: significantly better than “Transformer{}_{\\mathrm{big}}-fb” (p<0.01). “‡‡”: significantly better than “Jane” (p<0.01). “**”: significantly better than “Hier” (p<0.01).",
    "table_content": "|| Method | newstest2014 ||\n|| Transformer{}_{\\mathrm{big}} [Vaswani2017AttentionIA] | 28.72 ||\n|| DynamicConv [Wu2019PayLA] | 29.74 ||\n|| Transformer{}_{\\mathrm{big}}-fb [Ott2018ScalingNM] | 29.76 ||\n|| Jane [Freitag2014JaneOS] | 29.62 ||\n|| Hier [Zhou2017NeuralSC] | 29.95 ||\n|| Ours | [BOLD] 30.52{}^{{\\dagger}{\\dagger}{\\ddagger}{\\ddagger}**} ||",
    "claim": "The \"Hier\" method achieves the highest BLEU score of 30.52 on the newstest2014 dataset.",
    "label": "refute"
  },
  {
    "id": "training_957_refute",
    "table_caption": "Table: Results of R@K and median rank (Med r) for Flickr8K dataset. “-AlexNet” denotes the image representation based on AlexNet extracted from the whole image frame. “-RCNN” denotes the image representation extracted from possible objects detected by the RCNN algorithm.",
    "table_content": "|| [EMPTY] | Sentence Retrival (Image to Text) R@1 | Sentence Retrival (Image to Text) R@5 | Sentence Retrival (Image to Text) R@10 | Sentence Retrival (Image to Text) Med r | Image Retrival (Text to Image) R@1 | Image Retrival (Text to Image) R@5 | Image Retrival (Text to Image) R@10 | Image Retrival (Text to Image) Med r ||\n|| Random | 0.1 | 0.5 | 1.0 | 631 | 0.1 | 0.5 | 1.0 | 500 ||\n|| SDT-RNN-AlexNet | 4.5 | 18.0 | 28.6 | 32 | 6.1 | 18.5 | 29.0 | 29 ||\n|| Socher-avg-RCNN | 6.0 | 22.7 | 34.0 | 23 | 6.6 | 21.6 | 31.7 | 25 ||\n|| DeViSE-avg-RCNN | 4.8 | 16.5 | 27.3 | 28 | 5.9 | 20.1 | 29.6 | 29 ||\n|| DeepFE-AlexNet | 5.9 | 19.2 | 27.3 | 34 | 5.2 | 17.6 | 26.5 | 32 ||\n|| DeepFE-RCNN | 12.6 | 32.9 | 44.0 | 14 | 9.7 | 29.6 | [BOLD] 42.5 | [BOLD] 15 ||\n|| Ours-m-RNN-AlexNet | [BOLD] 14.5 | [BOLD] 37.2 | [BOLD] 48.5 | [BOLD] 11 | [BOLD] 11.5 | [BOLD] 31.0 | 42.4 | [BOLD] 15 ||",
    "claim": "\"Ours-m-RNN-AlexNet\" achieves the highest performance across all sentence retrieval (Image to Text) metrics but does not achieve the highest performance in any image retrieval (Text to Image) metrics compared to other models.",
    "label": "refute"
  },
  {
    "id": "training_1044_refute",
    "table_caption": "Table: Experimental results in semantic matching between e-commerce concepts and items.",
    "table_content": "|| Model | AUC | F1 | P@10 ||\n|| BM25 | - | - | 0.7681 ||\n|| DSSM (huang2013learning) | 0.7885 | 0.6937 | 0.7971 ||\n|| MatchPyramid (pang2016text) | 0.8127 | 0.7352 | 0.7813 ||\n|| RE2 (yang2019simple) | 0.8664 | 0.7052 | 0.8977 ||\n|| Ours | 0.8610 | 0.7532 | 0.9015 ||\n|| Ours + Knowledge | [BOLD] 0.8713 | [BOLD] 0.7769 | [BOLD] 0.9048 ||",
    "claim": "Our knowledge-aware model outperforms all baselines in terms of AUC and Precision at 10, but not in terms of F1.",
    "label": "refute"
  },
  {
    "id": "training_1063_refute",
    "table_caption": "Table: Extremely large Q&A dataset results.",
    "table_content": "|| [BOLD] Performance (ACC)  [BOLD] BERTlarge | [BOLD] Performance (ACC)  [BOLD] KD | [BOLD] Performance (ACC)  [BOLD] MKD | [BOLD] Performance (ACC)  [BOLD] TMKD ||\n|| 77.00 | 73.22 | 77.32 | [BOLD] 79.22 ||",
    "claim": "BERTlarge exceeds the performance of TMKD (ACC: 77.00 vs 79.22).",
    "label": "refute"
  },
  {
    "id": "training_1001_refute",
    "table_caption": "Table: Ablation analysis on the development set for the DSTC7 Ubuntu dataset.",
    "table_content": "|| [BOLD] Sub | [BOLD] Models | [BOLD] R@1 | [BOLD] R@10 | [BOLD] R@50 | [BOLD] MRR ||\n|| 1 | ESIM | 0.534 | 0.854 | 0.985 | 0.6401 ||\n|| 1 | -CtxDec | 0.508 | 0.845 | 0.982 | 0.6210 ||\n|| 1 | -CtxDec & -Rev | 0.504 | 0.840 | 0.982 | 0.6174 ||\n|| 1 | Ensemble | 0.573 | 0.887 | 0.989 | 0.6790 ||\n|| 2 | Sent-based | 0.021 | 0.082 | 0.159 | 0.0416 ||\n|| 2 | Ensemble1 | 0.023 | 0.091 | 0.168 | 0.0475 ||\n|| 2 | ESIM | 0.043 | 0.125 | 0.191 | 0.0713 ||\n|| 2 | -CtxDec | 0.034 | 0.117 | 0.191 | 0.0620 ||\n|| 2 | Ensemble2 | 0.048 | 0.134 | 0.194 | 0.0770 ||\n|| 4 | ESIM | 0.515 | 0.887 | 0.988 | 0.6434 ||\n|| 4 | -CtxDec | 0.492 | 0.877 | 0.987 | 0.6277 ||\n|| 4 | -CtxDec & -Rev | 0.490 | 0.875 | 0.986 | 0.6212 ||\n|| 4 | Ensemble | 0.551 | 0.909 | 0.992 | 0.6771 ||\n|| 5 | ESIM | 0.534 | 0.854 | 0.985 | 0.6401 ||\n|| 5 | +W2V | 0.530 | 0.858 | 0.986 | 0.6394 ||\n|| 5 | Ensemble | 0.575 | 0.890 | 0.989 | 0.6817 ||",
    "claim": "For Ubuntu subtask 1, ESIM achieved 0.854 R@10 and 0.6401 MRR. Removing context’s local matching and matching composition (“-CtxDec”) increased R@10 to 0.855 and MRR to 0.6220. Further discarding the last words for the context (“-CtxDec & -Rev”) degraded R@10 to 0.840 and MRR to 0.6174. The ensemble model achieved 0.887 R@10 and 0.6790 MRR.",
    "label": "refute"
  },
  {
    "id": "training_1036_refute",
    "table_caption": "Table: Annotated relations between entities. Relations appear within a sentence (intra-sentential) or across sentences (inter-sentential)",
    "table_content": "|| Type of Relation case has condition | Intra-sentential 28 | Intra-sentential 18.1% | Inter-sentential 127 | Inter-sentential 81.9% | Total 155 | Total 4.0% ||\n|| case has finding | 169 | 7.2% | 2180 | 92.8% | 2349 | 61.0% ||\n|| case has factor | 153 | 52.9% | 136 | 47.1% | 289 | 7.5% ||\n|| modifier modifies finding | 994 | 98.5% | 15 | 1.5% | 1009 | 26.2% ||\n|| condition causes finding | 44 | 3.6% | 3 | 6.4% | 47 | 1.2% ||",
    "claim": "There is a high number of inter-sentential relations in the corpus, and the most frequently annotated relation is the \"modifier modifies finding\" relation.",
    "label": "refute"
  },
  {
    "id": "training_1079_refute",
    "table_caption": "Table: Impact of increased training data, using EMM-clusters and propagated (prop.) CLINKs, on system performances (micro-averaged scores). TP: true positives, FP: false positives and FN: false negatives.",
    "table_content": "|| [EMPTY] | [BOLD] TP | [BOLD] FP | [BOLD] FN | [BOLD] P | [BOLD] R | [BOLD] F1 ||\n|| Causal-TimeBank | 79 | 53 | 239 | 0.5985 | 0.2484 | 0.3511 ||\n|| Causal-TimeBank + EMM-clusters | 95 | 54 | 223 | 0.6376 | 0.2987 | 0.4069 ||\n|| [BOLD] Causal-TimeBank + EMM-clusters + prop. CLINKs | 108 | 74 | 210 | [BOLD] 0.5934 | [BOLD] 0.3396 | [BOLD] 0.4320 ||",
    "claim": "Causal-TimeBank + EMM-clusters + prop. CLINKs achieves the highest precision and recall among the systems, despite a drop in F1-score.",
    "label": "refute"
  },
  {
    "id": "training_1007_refute",
    "table_caption": "Table: Accuracy (%) of Linguistic correctness on MultiNLI dev sets.",
    "table_content": "|| [BOLD] Category | [BOLD] ESIM | [BOLD] DIIN | [BOLD] CAFE | [BOLD] DRCN ||\n|| [BOLD] Matched | [BOLD] Matched | [BOLD] Matched | [BOLD] Matched | [BOLD] Matched ||\n|| Conditional | [BOLD] 100 | 57 | 70 | 65 ||\n|| Word overlap | 50 | 79 | 82 | [BOLD] 89 ||\n|| Negation | 76 | 78 | 76 | [BOLD] 80 ||\n|| Antonym | 67 | [BOLD] 82 | [BOLD] 82 | [BOLD] 82 ||\n|| Long Sentence | 75 | 81 | 79 | [BOLD] 83 ||\n|| Tense Difference | 73 | [BOLD] 84 | 82 | 82 ||\n|| Active/Passive | 88 | 93 | [BOLD] 100 | 87 ||\n|| Paraphrase | 89 | 88 | 88 | [BOLD] 92 ||\n|| Quantity/Time | 33 | 53 | 53 | [BOLD] 73 ||\n|| Coreference | [BOLD] 83 | 77 | 80 | 80 ||\n|| Quantifier | 69 | 74 | 75 | [BOLD] 78 ||\n|| Modal | 78 | [BOLD] 84 | 81 | 81 ||\n|| Belief | 65 | [BOLD] 77 | [BOLD] 77 | 76 ||\n|| Mean | 72.8 | 77.46 | 78.9 | [BOLD] 80.6 ||\n|| Stddev | 16.6 | 10.75 | 10.2 | [BOLD] 6.7 ||\n|| [BOLD] Mismatched | [BOLD] Mismatched | [BOLD] Mismatched | [BOLD] Mismatched | [BOLD] Mismatched ||\n|| Conditional | 60 | 69 | 85 | [BOLD] 89 ||\n|| Word overlap | 62 | [BOLD] 92 | 87 | 89 ||\n|| Negation | 71 | 77 | [BOLD] 80 | 78 ||\n|| Antonym | 58 | [BOLD] 80 | [BOLD] 80 | [BOLD] 80 ||\n|| Long Sentence | 69 | 73 | 77 | [BOLD] 84 ||\n|| Tense Difference | 79 | 78 | [BOLD] 89 | 83 ||\n|| Active/Passive | 91 | 70 | 90 | [BOLD] 100 ||\n|| Paraphrase | 84 | [BOLD] 100 | 95 | 90 ||\n|| Quantity/Time | 54 | 69 | 62 | [BOLD] 80 ||\n|| Coreference | 75 | 79 | 83 | [BOLD] 87 ||\n|| Quantifier | 72 | 78 | 80 | [BOLD] 82 ||\n|| Modal | 76 | 75 | 81 | [BOLD] 87 ||\n|| Belief | 67 | 81 | 83 | [BOLD] 85 ||\n|| Mean | 70.6 | 78.53 | 82.5 | [BOLD] 85.7 ||\n|| Stddev | 10.2 | 8.55 | 7.6 | [BOLD] 5.5 ||",
    "claim": "DRCN shows the highest mean and the lowest standard deviation for both matched and mismatched problems, but it does not outperform in the Quantity/Time category.",
    "label": "refute"
  },
  {
    "id": "training_1062_refute",
    "table_caption": "Table: Dev results with different decoding strategies.",
    "table_content": "|| [BOLD] Inference Strategy | [BOLD] en\\rightarrowde Step | [BOLD] en\\rightarrowde BLEU | [BOLD] ro\\rightarrowen Step | [BOLD] ro\\rightarrowen BLEU ||\n|| Left-to-Right Order | 6.80 | 21.25 | 4.86 | 33.87 ||\n|| Right-to-Left Order | 6.79 | 20.75 | 4.67 | 34.38 ||\n|| All-But-Itself | 6.90 | 20.72 | 4.80 | 33.35 ||\n|| Parallel Easy-First | 4.29 | [BOLD] 25.60 | 3.17 | [BOLD] 34.97 ||\n|| Mask-Predict | 10 | 25.34 | 10 | 34.54 ||",
    "claim": "Parallel Easy-First achieves the highest BLEU score for en→de translation but not for ro→en translation.",
    "label": "refute"
  },
  {
    "id": "training_1084_refute",
    "table_caption": "Table: Precision, recall and F1-score averaged on all slots and on all intents of an in-house dataset, run in June 2017.",
    "table_content": "|| NLU provider | train size | precision | recall | F1-score ||\n|| Luis | 70 | 0.909 | 0.537 | 0.691 ||\n|| Luis | 2000 | 0.954 | 0.917 | [BOLD] 0.932 ||\n|| Wit | 70 | 0.838 | 0.561 | 0.725 ||\n|| Wit | 2000 | 0.877 | 0.807 | 0.826 ||\n|| API.ai | 70 | 0.770 | 0.654 | 0.704 ||\n|| API.ai | 2000 | 0.905 | 0.881 | 0.884 ||\n|| Alexa | 70 | 0.680 | 0.495 | 0.564 ||\n|| Alexa | 2000 | 0.720 | 0.592 | 0.641 ||\n|| Snips | 70 | 0.795 | 0.769 | [BOLD] 0.790 ||\n|| Snips | 2000 | 0.946 | 0.921 | 0.930 ||",
    "claim": "Snips NLU achieves the highest F1-score for the 70-query training set and the highest for the 2000-query training set.",
    "label": "refute"
  },
  {
    "id": "training_1115_refute",
    "table_caption": "Table: Precision, recall and F1 for model jointly trained on all four reasoning tasks.",
    "table_content": "|| Set | R | k=1 34.7 | k=10 94.7 | k=100 98.9 | k=1000 99.8 ||\n|| Set | P | 100.0 | 100.0 | 100.0 | 100.0 ||\n|| Set | F1 | 49.6 | 96.5 | 99.2 | 99.9 ||\n|| Intersect | R | 71.0 | 91.7 | 97.7 | 99.7 ||\n|| Intersect | P | 99.8 | 99.4 | 99.2 | 98.9 ||\n|| Intersect | F1 | 74.7 | 92.7 | 98.8 | 99.3 ||\n|| Union | R | 9.6 | 39.6 | 59.7 | 79.0 ||\n|| Union | P | 100.0 | 99.7 | 99.2 | 98.8 ||\n|| Union | F1 | 16.1 | 47.0 | 65.8 | 84.3 ||\n|| Follow | R | 10.3 | 52.1 | 81.7 | 93.0 ||\n|| Follow | P | 100.0 | 100.0 | 100.0 | 100.0 ||\n|| Follow | F1 | 16.4 | 59.2 | 84.7 | 94.9 ||",
    "claim": "Precision is inconsistent and often below 99% across all sets and k values, and good recall is achieved with relatively small k values.",
    "label": "refute"
  },
  {
    "id": "training_1021_refute",
    "table_caption": "Table: SLU performances of different systems on the DSTC3 evaluation set.",
    "table_content": "|| [BOLD] SLU | [BOLD] Augmentation | [BOLD] F1-score ||\n|| ZS | w/o | 68.3 ||\n|| HD | w/o | 78.5 ||\n|| HD | Naive | 82.9 ||\n|| HD | AT (seed abridgement) | 85.5 ||\n|| HD | AT (combination) | 87.9 ||\n|| HD | AT (seed abr. + comb.) | [BOLD] 88.6 ||\n|| HD | Human Zhu et al. ( 2014 ) | 90.4 ||\n|| HD | Oracle | 96.9 ||",
    "claim": "The hierarchical decoding (HD) model achieves better performance than the zero-shot learning (ZS) method, and augmentation methods improve the HD model's F1-score, with the naive augmentation achieving the highest score among the augmentations.",
    "label": "refute"
  },
  {
    "id": "training_1123_refute",
    "table_caption": "Table: Performance of the proposed method and state-of-the-art systems.",
    "table_content": "|| Model | ACC | BLEU ||\n|| CEA | 71.96 | 2.77 ||\n|| MAE | 74.59 | 5.45 ||\n|| [BOLD] SMAE | [BOLD] 76.64 (+2.05) | [BOLD] 24.00 (+18.55) ||",
    "claim": "SMAE achieves the highest BLEU score but not the highest accuracy compared to CEA and MAE.",
    "label": "refute"
  },
  {
    "id": "training_1053_refute",
    "table_caption": "Table: Ablation study. Compare CAS with not adding LSTM layers (CAS-Subset) and not updating Transformer block parameters (CAS-LSTM).",
    "table_content": "|| [BOLD] Model | [BOLD] Datasets  [BOLD] PTB | [BOLD] Datasets  [BOLD] PTB | [BOLD] Datasets  [BOLD] WT-2 | [BOLD] Datasets  [BOLD] WT-2 | [BOLD] Datasets  [BOLD] WT-103 | [BOLD] Datasets  [BOLD] WT-103 ||\n|| [BOLD] Model | [BOLD] Val | [BOLD] Test | [BOLD] Val | [BOLD] Test | [BOLD] Val | [BOLD] Test ||\n|| BERT-CAS-Subset | 42.53 | 36.57 | [BOLD] 51.15 | [BOLD] 44.96 | [BOLD] 44.34 | [BOLD] 43.33 ||\n|| BERT-CAS-LSTM | [BOLD] 40.22 | [BOLD] 35.32 | 53.82 | 47.00 | 53.66 | 51.60 ||\n|| GPT-CAS-Subset | 47.58 | 41.85 | 54.58 | 50.08 | [BOLD] 35.49 | [BOLD] 35.48 ||\n|| GPT-CAS-LSTM | [BOLD] 47.24 | [BOLD] 41.61 | [BOLD] 50.55 | [BOLD] 46.62 | 36.68 | 36.61 ||",
    "claim": "On the PTB dataset, BERT-CAS-LSTM achieves the best validation and test scores, while on the WT-2 dataset, BERT-CAS-LSTM achieves the best validation and test scores. On the WT-103 dataset, BERT-CAS-Subset achieves the best validation and test scores.",
    "label": "refute"
  },
  {
    "id": "training_1114_refute",
    "table_caption": "Table: Precision of retrieval for the set-follow operation with various count-min sketches.",
    "table_content": "|| [EMPTY] | k=1 | k=10 | k=100 | k=1000 ||\n|| no sketch | 99.8 | 89.8 | 46.1 | 23.6 ||\n|| width=200 | 99.9 | 97.5 | 65.5 | 23.8 ||\n|| width=2000 | 100.0 | 100.0 | 100.0 | 99.98 ||",
    "claim": "Precision improves with the use of sketches, especially for larger k values, with width=2000 achieving 100% precision for k=1 and k=100, but not for k=10.",
    "label": "refute"
  },
  {
    "id": "training_1082_refute",
    "table_caption": "Table: Comparison of speed and memory performance of nnet-256 and nnet-768. RTF refers to real time ratio.",
    "table_content": "|| [BOLD] Model | [BOLD] Num. Params (M) | [BOLD] Size (MB) | [BOLD] RTF (Raspberry Pi 3) ||\n|| nnet-256 | 2.6 | 10 | <1 ||\n|| nnet-768 | 15.4 | 59 | >1 ||",
    "claim": "The nnet-768 is 6 to 10 times faster than the nnet-256.",
    "label": "refute"
  },
  {
    "id": "training_991_refute",
    "table_caption": "Table: Ablation Experiments by removing the main components.",
    "table_content": "|| [BOLD] Model | [BOLD] Referenced | [BOLD] Referenced | [BOLD] Unreferenced | [BOLD] Unreferenced ||\n|| [BOLD] Model | [BOLD] BLEU | [BOLD] ROUGE | [BOLD] Length | [BOLD] #Noun ||\n|| [BOLD] ICRED | [BOLD] 10.63 | [BOLD] 8.73 | [BOLD] 11.34 | [BOLD] 1.68 ||\n|| w/o Adr_Mem | 10.25 | 8.23 | 10.73 | 1.27 ||\n|| w/o Ctx_Spk_Vec | 10.13 | 8.22 | 10.86 | 1.59 ||\n|| w/o Ctx_Adr_Vec | 9.95 | 8.18 | 10.93 | 1.26 ||",
    "claim": "Removing any component from the ICRED model results in performance degradation, with \"w/o Adr_Mem\" performing the worst across most metrics.",
    "label": "refute"
  },
  {
    "id": "training_1086_refute",
    "table_caption": "Table: Precision, recall and F1-score averaged on all slots in an in-house dataset, run in June 2017.",
    "table_content": "|| intent | NLU provider | train size | precision | recall | F1-score ||\n|| AddToPlaylist | Luis | 70 | 0.759 | 0.575 | 0.771 ||\n|| AddToPlaylist | Luis | 2000 | 0.971 | 0.938 | 0.953 ||\n|| AddToPlaylist | Wit | 70 | 0.647 | 0.478 | 0.662 ||\n|| AddToPlaylist | Wit | 2000 | 0.862 | 0.761 | 0.799 ||\n|| AddToPlaylist | API.ai | 70 | 0.830 | 0.740 | 0.766 ||\n|| AddToPlaylist | API.ai | 2000 | 0.943 | 0.951 | 0.947 ||\n|| AddToPlaylist | Alexa | 70 | 0.718 | 0.664 | 0.667 ||\n|| AddToPlaylist | Alexa | 2000 | 0.746 | 0.704 | 0.724 ||\n|| AddToPlaylist | Snips | 70 | 0.787 | 0.788 | 0.785 ||\n|| AddToPlaylist | Snips | 2000 | 0.914 | 0.891 | 0.900 ||\n|| RateBook | Luis | 70 | 0.993 | 0.843 | 0.887 ||\n|| RateBook | Luis | 2000 | 1.000 | 0.997 | 0.999 ||\n|| RateBook | Wit | 70 | 0.987 | 0.922 | 0.933 ||\n|| RateBook | Wit | 2000 | 0.990 | 0.950 | 0.965 ||\n|| RateBook | API.ai | 70 | 0.868 | 0.830 | 0.840 ||\n|| RateBook | API.ai | 2000 | 0.976 | 0.983 | 0.979 ||\n|| RateBook | Alexa | 70 | 0.873 | 0.743 | 0.798 ||\n|| RateBook | Alexa | 2000 | 0.867 | 0.733 | 0.784 ||\n|| RateBook | Snips | 70 | 0.966 | 0.962 | 0.964 ||\n|| RateBook | Snips | 2000 | 0.997 | 0.997 | 0.997 ||\n|| SearchScreeningEvent | Luis | 70 | 0.995 | 0.721 | 0.826 ||\n|| SearchScreeningEvent | Luis | 2000 | 1.000 | 0.961 | 0.979 ||\n|| SearchScreeningEvent | Wit | 70 | 0.903 | 0.773 | 0.809 ||\n|| SearchScreeningEvent | Wit | 2000 | 0.849 | 0.849 | 0.840 ||\n|| SearchScreeningEvent | API.ai | 70 | 0.859 | 0.754 | 0.800 ||\n|| SearchScreeningEvent | API.ai | 2000 | 0.974 | 0.959 | 0.966 ||\n|| SearchScreeningEvent | Alexa | 70 | 0.710 | 0.515 | 0.560 ||\n|| SearchScreeningEvent | Alexa | 2000 | 0.695 | 0.541 | 0.585 ||\n|| SearchScreeningEvent | Snips | 70 | 0.881 | 0.840 | 0.858 ||\n|| SearchScreeningEvent | Snips | 2000 | 0.965 | 0.971 | 0.967 ||\n|| BookRestaurant | Luis | 70 | 0.859 | 0.336 | 0.473 ||\n|| BookRestaurant | Luis | 2000 | 0.906 | 0.891 | 0.892 ||\n|| BookRestaurant | Wit | 70 | 0.901 | 0.436 | 0.597 ||\n|| BookRestaurant | Wit | 2000 | 0.841 | 0.739 | 0.736 ||\n|| BookRestaurant | API.ai | 70 | 0.705 | 0.548 | 0.606 ||\n|| BookRestaurant | API.ai | 2000 | 0.874 | 0.853 | 0.834 ||\n|| BookRestaurant | Alexa | 70 | 0.598 | 0.364 | 0.504 ||\n|| BookRestaurant | Alexa | 2000 | 0.760 | 0.575 | 0.689 ||\n|| BookRestaurant | Snips | 70 | 0.727 | 0.700 | 0.719 ||\n|| BookRestaurant | Snips | 2000 | 0.919 | 0.891 | 0.903 ||",
    "claim": "Snips NLU achieves F1-scores that are lower than other providers for slot filling across different intents and training sizes.",
    "label": "refute"
  },
  {
    "id": "training_1137_refute",
    "table_caption": "Table: The percentage of responses in manual evaluation with the score of Content-Emotion. For instance, 2-1 means content score is 2 and emotion score is 1.",
    "table_content": "|| Method (%) | 2-1 | 1-1 | 0-1 | 2-0 | 1-0 | 0-0 ||\n|| Seq2Seq | 9.0 | 5.1 | 1.1 | 37.6 | 28.0 | 19.2 ||\n|| Emb | 22.8 | 9.3 | 4.3 | 27.1 | 19.1 | 17.4 ||\n|| ECM | [BOLD] 27.2 | [BOLD] 10.8 | 4.4 | 24.2 | 15.5 | 17.9 ||",
    "claim": "27.2% of the responses generated by ECM have a Content score of 2 and an Emotion score of 1, which is lower than Emb and Seq2Seq.",
    "label": "refute"
  },
  {
    "id": "training_1119_refute",
    "table_caption": "Table: Generalization ability evaluation. We report the results of single MT systems and system combination methods (i.e. Hier and Ours) on the Chinese-English task. N denotes the number of single MT systems. Note that both system combination methods were trained on the outputs of three single MT systems (i.e., N=3) and tested on various number of single systems (i.e., N=2,3,4,5). “†” and “††”: significantly better than the best system among inputs (p<0.05 and p<0.01). “‡” and “‡‡”: significantly better than “Hier” (p<0.05 and p<0.01).",
    "table_content": "|| N 2 | Single MT Systems 47.44 | Single MT Systems 46.18 | Single MT Systems - | Single MT Systems - | Single MT Systems - | Training No | Test Yes | Hier 47.35 | Ours  [BOLD] 47.76{}^{{\\dagger}{\\ddagger}} | \\Delta +0.41 ||\n|| 3 | 47.44 | 46.18 | 45.18 | - | - | Yes | Yes | 48.26 | [BOLD] 49.00{}^{{\\dagger}{\\dagger}{\\ddagger}{\\ddagger}} | +0.74 ||\n|| 4 | 47.44 | 46.18 | 45.18 | 47.09 | - | No | Yes | 48.59 | [BOLD] 49.56{}^{{\\dagger}{\\dagger}{\\ddagger}{\\ddagger}} | +0.97 ||\n|| 5 | 47.44 | 46.18 | 45.18 | 47.09 | 45.97 | No | Yes | 48.79 | [BOLD] 49.80{}^{{\\dagger}{\\dagger}{\\ddagger}{\\ddagger}} | +1.01 ||",
    "claim": "\"Ours\" achieves the highest performance across all tested configurations, with decreasing performance gains as the number of single MT systems increases.",
    "label": "refute"
  },
  {
    "id": "training_1078_refute",
    "table_caption": "Table: CauseRelPro-beta’s micro-averaged scores.",
    "table_content": "|| [BOLD] System | [BOLD] P | [BOLD] R | [BOLD] F1 ||\n|| mirza-tonelli:2014:Coling | 0.6729 | 0.2264 | 0.3388 ||\n|| [BOLD] CauseRelPro-beta | [BOLD] 0.5985 | [BOLD] 0.2484 | [BOLD] 0.3511 ||",
    "claim": "CauseRelPro-beta has higher recall and F1 score, while it also has higher precision than mirza-tonelli:2014:Coling.",
    "label": "refute"
  },
  {
    "id": "training_1096_refute",
    "table_caption": "Table: Comparison with the state-of-the-art systems of SemEval 2016 task 6 on sentiment dataset.",
    "table_content": "|| [BOLD] Models | [BOLD] Sentiment (F-score) ||\n|| UWB  | 42.02 ||\n|| INF-UFRGS-OPINION-MINING  | 42.32 ||\n|| LitisMind | 44.66 ||\n|| pkudblab  | 56.28 ||\n|| SVM + n-grams + sentiment  | 78.90 ||\n|| M2 (proposed) | [BOLD] 82.10 ||",
    "claim": "The M2 model surpasses the existing best system by 5 F-score points for sentiment analysis.",
    "label": "refute"
  },
  {
    "id": "training_1131_refute",
    "table_caption": "Table: Translation performance on WMT14 English⇒German translation task. “# Para.” denotes the number of parameters, and “Train” and “Decode” respectively denote the training (steps/second) and decoding (sentences/second) speeds.",
    "table_content": "|| [BOLD] # | [BOLD] Model | [BOLD] # Para. | [BOLD] Train | [BOLD] Decode | [BOLD] BLEU | △ ||\n|| 1 | Transformer-Base | 88.0M | 1.79 | 1.43 | 27.31 | – ||\n|| 2 | + Linear Combination  | +14.7M | 1.57 | 1.36 | 27.73 | +0.42 ||\n|| 3 | + Dynamic Combination | +25.2M | 1.50 | 1.30 | 28.33 | +1.02 ||\n|| 4 | + Dynamic Routing | +37.8M | 1.37 | 1.24 | 28.22 | +0.91 ||\n|| 5 | + EM Routing | +56.8M | 1.10 | 1.15 | 28.81 | +1.50 ||",
    "claim": "The linear combination model improves translation performance by +1.02 BLEU points compared to the Transformer-Base model.",
    "label": "refute"
  },
  {
    "id": "training_1127_refute",
    "table_caption": "Table: Results of our model and the baselines (directly reported in the referred articles) on the Chinese-English translation. “-” means that the studies did not test the models on the corresponding datasets.",
    "table_content": "|| Model | MT-02 | MT-03 | MT-04 | MT-05 | MT-06 | MT-08 | All ||\n|| Moses (Su et al.,  2016 ) | 33.19 | 32.43 | 34.14 | 31.47 | 30.81 | 23.85 | 31.04 ||\n|| RNNSearch (Su et al.,  2016 ) | 34.68 | 33.08 | 35.32 | 31.42 | 31.61 | 23.58 | 31.76 ||\n|| Lattice (Su et al.,  2016 ) | 35.94 | 34.32 | 36.50 | 32.40 | 32.77 | 24.84 | 32.95 ||\n|| CPR (Zhang et al.,  2017 ) | 33.84 | 31.18 | 33.26 | 30.67 | 29.63 | 22.38 | 29.72 ||\n|| POSTREG (Zhang et al.,  2017 ) | 34.37 | 31.42 | 34.18 | 30.99 | 29.90 | 22.87 | 30.20 ||\n|| PKI (Zhang et al.,  2017 ) | 36.10 | 33.64 | 36.48 | 33.08 | 32.90 | 24.63 | 32.51 ||\n|| Bi-Tree-LSTM (Chen et al.,  2017 ) | 36.57 | 35.64 | 36.63 | 34.35 | 30.57 | - | - ||\n|| Mixed RNN (Li et al.,  2017 ) | 37.70 | 34.90 | 38.60 | 35.50 | 35.60 | - | - ||\n|| Seq2Seq+Attn (our implementation) | 34.71 | 33.15 | 35.26 | 32.36 | 32.45 | 23.96 | 31.96 ||\n|| [BOLD] +Bag-of-Words (this paper) | [BOLD] 39.77 | [BOLD] 38.91 | [BOLD] 40.02 | [BOLD] 36.82 | [BOLD] 35.93 | [BOLD] 27.61 | [BOLD] 36.51 ||",
    "claim": "The \"+Bag-of-Words\" model achieves the highest BLEU score of 36.51 across all test sets, but the Seq2Seq+Attention model outperforms it by a BLEU score of 4.55.",
    "label": "refute"
  },
  {
    "id": "training_1136_refute",
    "table_caption": "Table: Objective evaluation with perplexity and accuracy.",
    "table_content": "|| Method | Perplexity | Accuracy ||\n|| Seq2Seq | 68.0 | 0.179 ||\n|| Emb | 62.5 | 0.724 ||\n|| ECM | 65.9 | [BOLD] 0.773 ||\n|| w/o Emb | 66.1 | 0.753 ||\n|| w/o IMem | 66.7 | 0.749 ||\n|| w/o EMem | [BOLD] 61.8 | 0.731 ||",
    "claim": "ECM obtains the best performance in emotion accuracy and has better perplexity than Emb but worse than Seq2Seq.",
    "label": "refute"
  },
  {
    "id": "training_1000_refute",
    "table_caption": "Table: Comparison of the state-of-the-art methods with our single model performance on VQAv2.0 test-dev and test-standard server.",
    "table_content": "|| Methods | Test-dev Y/N | Test-dev No. | Test-dev Other | Test-dev All | Test-standard Y/N | Test-standard No. | Test-standard Other | Test-standard All ||\n|| RAF (Ours) | [BOLD] 84.1 | [BOLD] 44.9 | [BOLD] 57.8 | [BOLD] 67.2 | [BOLD] 84.2 | [BOLD] 44.4 | [BOLD] 58.0 | [BOLD] 67.4 ||\n|| BU, adaptive K Teney et al. ( 2017 ) | 81.8 | 44.2 | 56.1 | 65.3 | 82.2 | 43.9 | 56.3 | 65.7 ||\n|| MFB Yu et al. ( 2018 ) | - | - | - | 64.9 | - | - | - | - ||\n|| ResonNetIlievski and Feng ( 2017 ) | - | - | - | - | 78.9 | 42.0 | 57.4 | 64.6 ||\n|| MUTANBen-Younes et al. ( 2017 ) | 80.7 | 39.4 | 53.7 | 63.2 | 80.9 | 38.6 | 54.0 | 63.5 ||\n|| MCB Fukui et al. ( 2016 ); Goyal et al. ( 2016 ) | - | - | - | - | 77.4 | 36.7 | 51.2 | 59.1 ||\n|| HieCoAtt Lu et al. ( 2016 ); Goyal et al. ( 2016 ) | - | - | - | - | 71.8 | 36.5 | 46.3 | 54.6 ||\n|| Language onlyGoyal et al. ( 2016 ) | - | - | - | - | 67.1 | 31.6 | 27.4 | 44.3 ||\n|| Common answerGoyal et al. ( 2016 ) | - | - | - | - | 61.2 | 0.4 | 1.8 | 26.0 ||",
    "claim": "RAF achieves the highest performance across all question categories in the test-dev dataset but not in the test-standard dataset.",
    "label": "refute"
  },
  {
    "id": "training_1130_refute",
    "table_caption": "Table: Retrieval results, comparing the performance of querying with the full question against segmented question (gold segments and predicted segments)",
    "table_content": "|| [BOLD] Method | [BOLD] MRR ||\n|| Full Question | 0.292 ||\n|| Segmented Question - Gold | 0.300 ||\n|| Segmented Question - Predicted | [BOLD] 0.298 ||",
    "claim": "Segmented Question - Predicted achieves the highest MRR, followed by Segmented Question - Gold, both outperforming the Full Question method.",
    "label": "refute"
  },
  {
    "id": "training_1080_refute",
    "table_caption": "Table: Macro-averaged F1 score for multi-class author classification on the large datasets, using either no discourse (None), grammatical relations (GR), or RST relations (RST).",
    "table_content": "|| Disc. Type | Model | novel-50 | IMDB62 ||\n|| None | SVM2 | 92.9 | 90.4 ||\n|| None | CNN2 | 95.3 | 91.5 ||\n|| GR | SVM2-PV | 93.3 | 90.4 ||\n|| GR | CNN2-PV | 95.1 | 90.5 ||\n|| GR | CNN2-DE (local) | 96.9 | 90.8 ||\n|| GR | CNN2-DE (global) | 97.5 | 90.9 ||\n|| RST | SVM2-PV | 93.8 | 90.9 ||\n|| RST | CNN2-PV | 95.5 | 90.7 ||\n|| RST | CNN2-DE (local) | 97.7 | 91.4 ||\n|| RST | CNN2-DE (global) | [BOLD] 98.8 | [BOLD] 92.0 ||",
    "claim": "On novel-50, the CNN2-DE (global) model achieves the highest F1 score of 98.8, and on IMDB62, the CNN2-DE (local) model achieves the highest F1 score of 92.0.",
    "label": "refute"
  },
  {
    "id": "training_1163_refute",
    "table_caption": "Table: Comparisons on the Stanford sentiment treebank dataset. Scores are as reported by socher and paragraphvector.",
    "table_content": "|| [BOLD] Method | [BOLD] Acc. ||\n|| Naïve Bayes | 81.8 ||\n|| SVM | 79.4 ||\n|| Vector average | 80.1 ||\n|| Recursive neural networks | 82.4 ||\n|| [BOLD] LR (this work) | 82.4 ||\n|| Matrix-vector RNN | 82.9 ||\n|| Recursive neural tensor networks | 85.4 ||\n|| Paragraph vector | 87.8 ||",
    "claim": "The logistic regression model achieves lower accuracy than the SVM baseline.",
    "label": "refute"
  },
  {
    "id": "training_1085_refute",
    "table_caption": "Table: Precision, recall and F1-score averaged on all slots in an in-house dataset, run in June 2017.",
    "table_content": "|| intent | NLU provider | train size | precision | recall | F1-score ||\n|| SearchCreativeWork | Luis | 70 | 0.993 | 0.746 | 0.849 ||\n|| SearchCreativeWork | Luis | 2000 | 1.000 | 0.995 | 0.997 ||\n|| SearchCreativeWork | Wit | 70 | 0.959 | 0.569 | 0.956 ||\n|| SearchCreativeWork | Wit | 2000 | 0.974 | 0.955 | 0.964 ||\n|| SearchCreativeWork | API.ai | 70 | 0.915 | 0.711 | 0.797 ||\n|| SearchCreativeWork | API.ai | 2000 | 1.000 | 0.968 | 0.983 ||\n|| SearchCreativeWork | Alexa | 70 | 0.492 | 0.323 | 0.383 ||\n|| SearchCreativeWork | Alexa | 2000 | 0.464 | 0.375 | 0.413 ||\n|| SearchCreativeWork | Snips | 70 | 0.864 | 0.908 | 0.885 ||\n|| SearchCreativeWork | Snips | 2000 | 0.983 | 0.976 | 0.980 ||\n|| GetWeather | Luis | 70 | 0.781 | 0.271 | 0.405 ||\n|| GetWeather | Luis | 2000 | 0.985 | 0.902 | 0.940 ||\n|| GetWeather | Wit | 70 | 0.790 | 0.411 | 0.540 ||\n|| GetWeather | Wit | 2000 | 0.847 | 0.874 | 0.825 ||\n|| GetWeather | API.ai | 70 | 0.666 | 0.513 | 0.530 ||\n|| GetWeather | API.ai | 2000 | 0.826 | 0.751 | 0.761 ||\n|| GetWeather | Alexa | 70 | 0.764 | 0.470 | 0.572 ||\n|| GetWeather | Alexa | 2000 | 0.818 | 0.701 | 0.746 ||\n|| GetWeather | Snips | 70 | 0.791 | 0.703 | 0.742 ||\n|| GetWeather | Snips | 2000 | 0.964 | 0.926 | 0.943 ||\n|| PlayMusic | Luis | 70 | 0.983 | 0.265 | 0.624 ||\n|| PlayMusic | Luis | 2000 | 0.816 | 0.737 | 0.761 ||\n|| PlayMusic | Wit | 70 | 0.677 | 0.336 | 0.580 ||\n|| PlayMusic | Wit | 2000 | 0.773 | 0.518 | 0.655 ||\n|| PlayMusic | API.ai | 70 | 0.549 | 0.486 | 0.593 ||\n|| PlayMusic | API.ai | 2000 | 0.744 | 0.701 | 0.716 ||\n|| PlayMusic | Alexa | 70 | 0.603 | 0.384 | 0.464 ||\n|| PlayMusic | Alexa | 2000 | 0.690 | 0.518 | 0.546 ||\n|| PlayMusic | Snips | 70 | 0.546 | 0.482 | 0.577 ||\n|| PlayMusic | Snips | 2000 | 0.876 | 0.792 | 0.823 ||",
    "claim": "Snips NLU achieves the highest F1-score for the \"SearchCreativeWork\" intent with a training size of 2000 queries.",
    "label": "refute"
  },
  {
    "id": "training_1139_refute",
    "table_caption": "Table: Evaluation on CoNLL 2003 Test-b and TAC-2010",
    "table_content": "|| [BOLD] models | [BOLD] In-KB acc% ||\n|| [BOLD] local | [BOLD] local ||\n|| (He et al.,  2013 ) | 85.6 ||\n|| (Francis-Landau et al.,  2016 ) | 85.5 ||\n|| (Sil and Florian,  2016 ) | 86.2 ||\n|| (Nevena Lazic and Pereira,  2015 ) | 86.4 ||\n|| (Yamada et al.,  2016 ) | 90.9 ||\n|| (Sil et al.,  2018 ) | 94.0 ||\n|| [BOLD] global | [BOLD] global ||\n|| (Hoffart et al.,  2011 ) | 82.5 ||\n|| (Pershina et al.,  2015 ) | 91.8 ||\n|| (Globerson et al.,  2016 ) | 92.7 ||\n|| (Yamada et al.,  2016 ) | 93.1 ||\n|| [BOLD] our model | [BOLD] our model ||\n|| local | 90.89 ||\n|| global | [BOLD] 94.44 ||",
    "claim": "The global model achieves the highest accuracy compared to other global models, while the local model outperforms the best local model.",
    "label": "refute"
  },
  {
    "id": "training_1143_refute",
    "table_caption": "Table: CoNLL 2003 Test-b",
    "table_content": "|| [BOLD] Models | Heuristic | Depth ( [ITALIC] τ) | Beam Size (b) | In-KB acc % ||\n|| Global + LDS | [ITALIC] h1 | [ITALIC] τ=25% [ITALIC] n | 1 | 93.88 ||\n|| Global + LDS | [ITALIC] h1 | [ITALIC] τ=25% [ITALIC] n | 5 | 94.12 ||\n|| Global + LDS | [ITALIC] h1 | [ITALIC] τ=50% [ITALIC] n | 5 | 94.23 ||\n|| Global + LDS | [ITALIC] h2 | flexible | 5 | 94.44 ||",
    "claim": "Using a beam size of 5 improves accuracy compared to a beam size of 1, and heuristic h1 with τ=50% n achieves the highest accuracy.",
    "label": "refute"
  },
  {
    "id": "training_1040_refute",
    "table_caption": "Table: Comparison on R2R Leaderboard Test Set. Our navigation model benefits from transfer learned representations and outperforms the known SOTA on SPL. SPL and SR are reported as percentages and NE and PL in meters.",
    "table_content": "|| Model | PL | NE ↓ | SR ↑ | SPL ↑ ||\n|| Random  | 9.89 | 9.79 | 13.2 | 12.0 ||\n|| Seq-to-Seq  | 8.13 | 7.85 | 20.4 | 18.0 ||\n|| Look Before You Leap  | 9.15 | 7.53 | 25.3 | 23.0 ||\n|| Speaker-Follower  | 14.8 | 6.62 | 35.0 | 28.0 ||\n|| Self-Monitoring  | 18.0 | 5.67 | [BOLD] 48.0 | 35.0 ||\n|| Reinforced Cross-Modal  | 12.0 | 6.12 | 43.1 | 38.0 ||\n|| The Regretful Agent  | 13.7 | 5.69 | [BOLD] 48.0 | 40.0 ||\n|| ALTR (Ours) | 10.3 | [BOLD] 5.49 | [BOLD] 48.0 | [BOLD] 45.0 ||",
    "claim": "ALTR has the lowest navigation error (NE), ties for the highest success rate (SR), and achieves the highest success weighted by path length (SPL). It also has a longer path length (PL) compared to the Reinforced Cross-Modal model.",
    "label": "refute"
  },
  {
    "id": "training_1128_refute",
    "table_caption": "Table: Results for experiments between using Word2Vec and fastText embeddings. Also includes results of using attention on top of the model with Word2Vec. Since attention results were not promising, we did not repeat them with fastText.",
    "table_content": "|| [BOLD] Model | [BOLD] P | [BOLD] R | [BOLD] F1 ||\n|| Word2Vec (w/o Attn) | 65.20 | 58.59 | 61.72 ||\n|| + weighted Attn. | 62.34 | 57.0 | 59.55 ||\n|| + un-weighted Attn. | 69.21 | 56.15 | 62.0 ||\n|| fastText | 74.57 | 75.51 | 75.04 ||",
    "claim": "fastText achieves higher recall and F1 score compared to Word2Vec models, but a Word2Vec model achieves higher precision.",
    "label": "refute"
  },
  {
    "id": "training_1133_refute",
    "table_caption": "Table: Performance with different encoder.",
    "table_content": "|| Model | Params | [ITALIC] Sen-Making | [ITALIC] Explanation ||\n|| Random | - | 49.52 | 32.77 ||\n|| BERT [ITALIC] base | 117M | 88.56 | 85.32 ||\n|| BERT [ITALIC] large | 340M | 86.55 | 90.12 ||\n|| XLNet | 340M | 90.33 | 91.07 ||\n|| SpanBERT | 340M | 89.46 | 90.47 ||\n|| RoBERTa | 355M | 93.56 | 92.37 ||\n|| ALBERT [ITALIC] base | 12M | 86.63 | 84.37 ||\n|| ALBERT [ITALIC] large | 18M | 88.01 | 89.72 ||\n|| ALBERT [ITALIC] xlarge | 60M | 92.03 | 92.45 ||\n|| Ours(ALBERT [ITALIC] xxlarge) | 235M | 95.68 | 95.48 ||\n|| Our-ensemble | - | 95.91 | 96.39 ||",
    "claim": "RoBERTa achieves the highest performance in both Sen-Making and Explanation tasks.",
    "label": "refute"
  },
  {
    "id": "training_1066_refute",
    "table_caption": "Table: Compare different number of transformer layer.",
    "table_content": "|| [BOLD] Dataset | [BOLD] Metrics | [BOLD] Layer Number  [BOLD] 1 | [BOLD] Layer Number  [BOLD] 3 | [BOLD] Layer Number  [BOLD] 5 ||\n|| [BOLD] DeepQA | ACC | 74.59 | 78.46 | 79.54 ||\n|| [BOLD] MNLI | ACC | 61.23 | 71.13 | 72.76 ||\n|| [BOLD] SNLI | ACC | 70.21 | 77.67 | 78.20 ||\n|| [BOLD] QNLI | ACC | 70.60 | 82.04 | 84.94 ||\n|| [BOLD] RTE | ACC | 54.51 | 65.70 | 66.07 ||\n|| [EMPTY] | QPS | 511 | 217 | 141 ||",
    "claim": "As the layer number increases from 1 to 3, the ACC gains for the datasets are significant, and the gains remain significant from 3 to 5, accompanied by a significant decrease in QPS.",
    "label": "refute"
  },
  {
    "id": "training_1126_refute",
    "table_caption": "Table: Evaluation results for models trained on Daily Mail and their generated texts",
    "table_content": "|| Model | [ITALIC] BN | [ITALIC] BNc | [ITALIC] GR | [ITALIC] Ppl. | [ITALIC] CB| [ITALIC] o | [ITALIC] CB| [ITALIC] g | [ITALIC] EBd ||\n|| Dataset | 0.340 | 0.213 | [EMPTY] | - | - | - | - ||\n|| Baseline | 0.531 | 0.282 | 1.415 | 117.845 | 1.447 | 97.762 | 0.528 ||\n|| REG | 0.381 | 0.329 | 1.028 | [BOLD] 114.438 | 1.861 | 108.740 | 0.373 ||\n|| CDA | 0.208 | 0.149 | 1.037 | 117.976 | 0.703 | 56.82 | 0.268 ||\n|| [ITALIC] λ0.01 | 0.492 | 0.245 | 1.445 | 118.585 | 0.111 | 9.306 | 0.077 ||\n|| [ITALIC] λ0.1 | 0.459 | 0.208 | 1.463 | 118.713 | 0.013 | 2.326 | 0.018 ||\n|| [ITALIC] λ0.5 | 0.312 | 0.173 | 1.252 | 120.344 | [BOLD] 0.000 | 1.159 | 0.006 ||\n|| [ITALIC] λ0.8 | 0.226 | 0.151 | 1.096 | 119.792 | 0.001 | 1.448 | 0.002 ||\n|| [ITALIC] λ1 | 0.218 | 0.153 | 1.049 | 120.973 | [BOLD] 0.000 | 0.999 | 0.002 ||\n|| [ITALIC] λ2 | 0.221 | 0.157 | 1.020 | 123.248 | [BOLD] 0.000 | 0.471 | [BOLD] 0.000 ||\n|| [ITALIC]  [BOLD] λ [BOLD] 0.5 + CDA | [BOLD] 0.205 | [BOLD] 0.145 | [BOLD] 1.012 | 117.971 | [BOLD] 0.000 | [BOLD] 0.153 | [BOLD] 0.000 ||",
    "claim": "The λ0.5 + CDA combination achieves the lowest values for GR and Ppl., indicating effective bias reduction.",
    "label": "refute"
  },
  {
    "id": "training_1112_refute",
    "table_caption": "Table: Classification accuracy (%) for natural language inference on SNLI dataset. Results with “*” are obtained from the official SNLI leaderboard (https://nlp.stanford.edu/projects/snli/).",
    "table_content": "|| [BOLD] Model | Dev | Test ||\n|| GPT Radford et al. ( 2018 ) | - | 89.9∗ ||\n|| Kim et al. ( 2018 ) | - | 90.1∗ ||\n|| BERT \\textsc  [ITALIC] BASE | 90.94 | 90.66 ||\n|| [BOLD] BERT-Attention | 91.12 | 90.70 ||\n|| [BOLD] BERT-LSTM | [BOLD] 91.18 | [BOLD] 90.79 ||\n|| MT-DNN Liu et al. ( 2019 ) | 91.35 | [BOLD] 91.00 ||\n|| [BOLD] MT-DNN-Attention | 91.41 | 90.95 ||\n|| [BOLD] MT-DNN-LSTM | [BOLD] 91.50 | 90.91 ||",
    "claim": "BERT-Attention and BERT-LSTM perform better than BERT BASE, while MT-DNN performs better than MT-DNN-Attention and MT-DNN-LSTM on the Dev set.",
    "label": "refute"
  },
  {
    "id": "training_1129_refute",
    "table_caption": "Table: Results comparing the models using various pre-trained embeddings. The en data source is the downloaded pre-trained ELMo model. For simple concatenation, we present the results for the best model at each n combinations of data sources. For example, when concatenating any 2 datasources, the en + config combination gives the best performance.",
    "table_content": "|| [BOLD] Model | [BOLD] P | [BOLD] R | [BOLD] F1 ||\n|| No Pretraining | 74.57 | 75.51 | 75.04 ||\n|| Simple Concat - 1 (en) | 76.88 | 74.49 | 75.67 ||\n|| Simple Concat - 2 (en + config) | 77.67 | 76.12 | 76.89 ||\n|| Simple Concat - 3 (en + code + config) | 79.64 | 77.72 | 78.67 ||\n|| Simple Concat - 4 (ALL) | 76.05 | 76.65 | 76.35 ||\n|| DME | 77.42 | 75.82 | 76.61 ||\n|| CDME | 78.30 | 79.29 | [BOLD] 78.80 ||",
    "claim": "Simple Concat - 3 (en + code + config) achieves the highest F1 score among the models listed in the table.",
    "label": "refute"
  },
  {
    "id": "training_1165_refute",
    "table_caption": "Table: Comparisons on the U.S. congressional vote dataset. SVM-link exploits link structures [Thomas et al.2006]; the min-cut result is from bansal; and SVM-SLE result is reported by ainur.",
    "table_content": "|| [BOLD] Method | [BOLD] Acc. ||\n|| SVM-link | 71.28 ||\n|| Min-cut | 75.00 ||\n|| SVM-SLE | 77.67 ||\n|| [BOLD] LR (this work) | 78.59 ||",
    "claim": "SVM-SLE achieves the highest accuracy among the methods listed.",
    "label": "refute"
  },
  {
    "id": "training_1146_refute",
    "table_caption": "Table: Accuracies of normalization (Id-En, Tr-De) and POS tagging (Tr-De) on test data, comparing the baselines to the best two normalization models.",
    "table_content": "|| Dataset | Task | LAI | Multilingual | Language-aware | Gold ||\n|| Id-En | norm. | 74.03 | 94.27 | 94.32 | 100.00 ||\n|| Tr-De | norm. | 67.02 | 78.28 | 77.83 | 100.00 ||\n|| Tr-De | pos | 59.60 | 62.86 | 62.72 | 66.47 ||",
    "claim": "The language-aware model slightly outperforms the multilingual model on the Tr-De dataset for both normalization and POS tasks. For the Id-En dataset, the performances of the multilingual and language-aware models for normalization are very close.",
    "label": "refute"
  },
  {
    "id": "training_1057_refute",
    "table_caption": "Table: Results for around 200 custom developer domains. For F1, higher values are better, while for SER lower values are better. * denotes statistically significant SER difference compared to both baselines.",
    "table_content": "|| Approach | [ITALIC] F1 [ITALIC] Intent Mean | [ITALIC] F1 [ITALIC] Intent Median | [ITALIC] F1 [ITALIC] Slot Mean | [ITALIC] F1 [ITALIC] Slot Median | [ITALIC] SER Mean | [ITALIC] SER Median ||\n|| Baseline CRF/MaxEnt | 94.6 | 96.6 | 80.0 | 91.5 | 14.5 | 9.2 ||\n|| Baseline DNN | 91.9 | 95.9 | 85.1 | 92.9 | 14.7 | 9.2 ||\n|| Proposed Pretrained DNN * | [BOLD] 95.2 | [BOLD] 97.2 | [BOLD] 88.6 | [BOLD] 93.0 | [BOLD] 13.1 | [BOLD] 7.9 ||",
    "claim": "The Proposed Pretrained DNN achieves higher scores across all metrics compared to the Baseline CRF/MaxEnt and Baseline DNN, including a higher SER Mean than the Baseline CRF/MaxEnt.",
    "label": "refute"
  },
  {
    "id": "training_1169_refute",
    "table_caption": "Table: Results for the explicit classifier.",
    "table_content": "|| [EMPTY] | Precision | Recall | [ITALIC] F1 ||\n|| GS + no EP | 86.77 | 86.77 | 86.77 ||\n|| GS + EP | 83.19 | 82.65 | 82.92 ||\n|| Auto + EP | 81.19 | 80.04 | 80.61 ||",
    "claim": "The F1 score is 86.77 for GS + no EP, 82.92 for GS + EP, and 81.61 for Auto + EP.",
    "label": "refute"
  },
  {
    "id": "training_1083_refute",
    "table_caption": "Table: Precision, recall and F1-score on Braun et al. corpora. *Benchmark run in August 2017 by the authors of bench17 . **Benchmark run in January 2018 by the authors of this paper.",
    "table_content": "|| corpus | NLU provider | precision | recall | F1-score ||\n|| chatbot | Luis* | 0.970 | 0.918 | 0.943 ||\n|| chatbot | IBM Watson* | 0.686 | 0.8 | 0.739 ||\n|| chatbot | API.ai* | 0.936 | 0.532 | 0.678 ||\n|| chatbot | Rasa* | 0.970 | 0.918 | 0.943 ||\n|| chatbot | Rasa** | 0.933 | 0.921 | 0.927 ||\n|| chatbot | Snips** | 0.963 | 0.899 | 0.930 ||\n|| web apps | Luis* | 0.828 | 0.653 | 0.73 ||\n|| web apps | IBM Watson* | 0.828 | 0.585 | 0.686 ||\n|| web apps | API.ai* | 0.810 | 0.382 | 0.519 ||\n|| web apps | Rasa* | 0.466 | 0.724 | 0.567 ||\n|| web apps | Rasa** | 0.593 | 0.613 | 0.603 ||\n|| web apps | Snips** | 0.655 | 0.655 | 0.655 ||\n|| ask ubuntu | Luis* | 0.885 | 0.842 | 0.863 ||\n|| ask ubuntu | IBM Watson* | 0.807 | 0.825 | 0.816 ||\n|| ask ubuntu | API.ai* | 0.815 | 0.754 | 0.783 ||\n|| ask ubuntu | Rasa* | 0.791 | 0.823 | 0.807 ||\n|| ask ubuntu | Rasa** | 0.796 | 0.768 | 0.782 ||\n|| ask ubuntu | Snips** | 0.812 | 0.828 | 0.820 ||\n|| overall | Luis* | 0.945 | 0.889 | 0.916 ||\n|| overall | IBM Watson* | 0.738 | 0.767 | 0.752 ||\n|| overall | API.ai* | 0.871 | 0.567 | 0.687 ||\n|| overall | Rasa* | 0.789 | 0.855 | 0.821 ||\n|| overall | Rasa** | 0.866 | 0.856 | 0.861 ||\n|| overall | Snips** | 0.896 | 0.858 | 0.877 ||",
    "claim": "Snips NLU ranks third highest overall in F1-score.",
    "label": "refute"
  },
  {
    "id": "training_1168_refute",
    "table_caption": "Table: Overall results for the argument extractor.",
    "table_content": "|| Partial | GS + no EP | Arg1  [ITALIC] F1 86.67 | Arg2  [ITALIC] F1 99.13 | Rel  [ITALIC] F1 86.24 ||\n|| Partial | GS + EP | 83.62 | 94.98 | 83.52 ||\n|| Partial | Auto + EP | 81.72 | 92.64 | 80.96 ||\n|| Exact | GS + no EP | 59.15 | 82.23 | 53.85 ||\n|| Exact | GS + EP | 57.64 | 79.80 | 52.29 ||\n|| Exact | Auto + EP | 47.68 | 70.27 | 40.37 ||",
    "claim": "The GS + EP setting gives an F1 score of 86.24% for partial matching on the relation level.",
    "label": "refute"
  },
  {
    "id": "training_1140_refute",
    "table_caption": "Table: Evaluation on CoNLL 2003 Test-b and TAC-2010",
    "table_content": "|| [BOLD] models | [BOLD] In-KB acc% ||\n|| [BOLD] local | [BOLD] local ||\n|| (Sil and Florian,  2016 ) | 78.6 ||\n|| (He et al.,  2013 ) | 81.0 ||\n|| (Sun et al.,  2015 ) | 83.9 ||\n|| (Yamada et al.,  2016 ) | 84.6 ||\n|| (Sil et al.,  2018 ) | 87.4 ||\n|| [BOLD] global | [BOLD] global ||\n|| (Yamada et al.,  2016 ) | 85.2 ||\n|| (Globerson et al.,  2016 ) | 87.2 ||\n|| [BOLD] our model | [BOLD] our model ||\n|| local | 85.73 ||\n|| global | [BOLD] 87.9 ||",
    "claim": "Our global model outperforms all competitors, while our local model is superior to the state-of-the-art local model.",
    "label": "refute"
  },
  {
    "id": "training_1047_refute",
    "table_caption": "Table: Experimental results in shopping concept tagging.",
    "table_content": "|| Model | Precision | Recall | F1 ||\n|| Baseline | 0.8573 | 0.8474 | 0.8523 ||\n|| +Fuzzy CRF | 0.8731 | 0.8665 | 0.8703 ||\n|| +Fuzzy CRF & Knowledge | [BOLD] 0.8796 | [BOLD] 0.8748 | [BOLD] 0.8772 ||",
    "claim": "Adding Fuzzy CRF improves the F1 score from 0.8523 to 0.8703, but further adding external knowledge decreases it to 0.8772.",
    "label": "refute"
  },
  {
    "id": "training_1152_refute",
    "table_caption": "Table: Comparison of the BiLSTM baseline with SentModel which encode the document sentence by sentence. We report the F1 score and AUC on the Dev set here.",
    "table_content": "|| Model | F1 | AUC ||\n|| BiLSTM | 50.94 | 50.26 ||\n|| SentModel | 50.97 | 49.31 ||",
    "claim": "The SentModel achieves significantly different F1 and AUC scores compared to the BiLSTM model.",
    "label": "refute"
  },
  {
    "id": "training_1045_refute",
    "table_caption": "Table: Experimental results of different sampling strategy in hypernym discovery.",
    "table_content": "|| Strategy | Labeled Size | MRR | MAP | P@1 | Reduce ||\n|| Random | 500k | 58.97 | 45.30 | 45.50 | - ||\n|| US | 375k | 59.66 | 45.73 | 46.00 | 150k ||\n|| CS | 400k | 58.96 | 45.22 | 45.30 | 100k ||\n|| UCS | 325k | 59.87 | 46.32 | 46.00 | 175k ||",
    "claim": "UCS is the most economical sampling strategy, requiring only 325k samples and reducing 25% of samples compared to the random strategy.",
    "label": "refute"
  },
  {
    "id": "training_1161_refute",
    "table_caption": "Table: Macro averaged F1-Scores of our submissions and the other experiments on test data",
    "table_content": "|| [BOLD] Model | [BOLD] Arabic | [BOLD] Greek | [BOLD] Turkish | [BOLD] Average ||\n|| [BOLD] SVM with TF-IDF | 0.772 | 0.823 | 0.685 | 0.760 ||\n|| [BOLD] Multilingual BERT | 0.808 | 0.807 | 0.774 | 0.796 ||\n|| [BOLD] Bi-LSTM | 0.822 | 0.826 | 0.755 | 0.801 ||\n|| [BOLD] CNN-Text | 0.840 | 0.825 | 0.751 | 0.805 ||\n|| [BOLD] BERT | 0.884 | 0.822 | [BOLD] 0.816 | 0.841 ||\n|| [BOLD] BERT-CNN (Ours)44footnotemark: 4 | [BOLD] 0.897 | [BOLD] 0.843 | 0.814 | [BOLD] 0.851 ||",
    "claim": "BERT achieves the highest average macro-averaged F1-Score across Arabic, Greek, and Turkish languages.",
    "label": "refute"
  },
  {
    "id": "training_1081_refute",
    "table_caption": "Table: Decoding accuracy of neural networks of different sizes (Word Error Rate, %)",
    "table_content": "|| [BOLD] Model | [BOLD] dev-clean | [BOLD] dev-other | [BOLD] test-clean | [BOLD] test-other ||\n|| nnet-256 | 7.3 | 19.2 | 7.6 | 19.6 ||\n|| nnet-512 | 6.4 | 17.1 | 6.6 | 17.6 ||\n|| nnet-768 | 6.4 | 16.8 | 6.6 | 17.5 ||\n|| KALDI | 4.3 | 11.2 | 4.8 | 11.5 ||",
    "claim": "nnet-512 achieves the lowest WER across all datasets compared to the KALDI model.",
    "label": "refute"
  },
  {
    "id": "training_1167_refute",
    "table_caption": "Table: Results for the connective classifier.",
    "table_content": "|| [EMPTY] | P&N Acc. | P&N  [ITALIC] F1 | +new Acc. | +new  [ITALIC] F1 ||\n|| GS | 95.30 | 92.75 | 97.34 | 95.76 ||\n|| Auto | 94.21 | 91.00 | 96.02 | 93.62 ||",
    "claim": "Introducing new features decreases accuracy and F1 scores under both GS and Auto settings.",
    "label": "refute"
  },
  {
    "id": "training_1153_refute",
    "table_caption": "Table: Comparison of the BERT model with other baselines. We report F1 score on the Dev and Test set.",
    "table_content": "|| Model | Dev | Test ||\n|| CNN | 43.45 | 42.26 ||\n|| LSTM | 50.68 | 50.07 ||\n|| BiLSTM | 50.94 | 51.06 ||\n|| Context-Aware | 51.09 | 50.70 ||\n|| BERT | 54.16 | 53.20 ||\n|| BERT-Two-Step | [BOLD] 54.42 | [BOLD] 53.92 ||",
    "claim": "BERT achieves the highest F1 scores on both the development and test datasets.",
    "label": "refute"
  },
  {
    "id": "training_1170_refute",
    "table_caption": "Table: Results for the non-explicit classifier.",
    "table_content": "|| [EMPTY] | Precision | Recall | [ITALIC] F1 | Baseline  [ITALIC] F1 ||\n|| GS + no EP | 39.63 | 39.63 | 39.63 | 21.34 ||\n|| GS + EP | 26.21 | 27.63 | 26.90 | 20.30 ||\n|| Auto + EP | 24.54 | 26.45 | 25.46 | 19.31 ||",
    "claim": "The F1 scores for GS + EP and Auto + EP are lower than the baseline F1 score.",
    "label": "refute"
  },
  {
    "id": "training_1117_refute",
    "table_caption": "Table: Hits@1 of WebQuestionsSP and MetaQA (2-hop and 3-hop) datasets. (We re-run GRAFT-Net and PullNet on WebQuestionsSP with oracle entities.)",
    "table_content": "|| [EMPTY] | MetaQA2 | MetaQA3 | WebQSP ||\n|| KV-Mem | 82.7 | 48.9 | 46.7 ||\n|| ReifKB | 81.1 | 72.3 | 52.7 ||\n|| GRAFT-Net | 94.8 | 77.7 | 70.3 ||\n|| PullNet | [BOLD] 99.9 | 91.4 | 69.7 ||\n|| EmQL | 98.6 | [BOLD] 99.1 | [BOLD] 75.5 ||\n|| EmQL (no-sketch) | 70.3 | 60.9 | 53.2 ||\n|| EmQL (no-constr) | – | – | 65.2 ||",
    "claim": "EmQL achieves the highest performance on the MetaQA 2-hop and WebQSP datasets.",
    "label": "refute"
  },
  {
    "id": "training_1151_refute",
    "table_caption": "Table: Model performance with the different method for computing node representation.",
    "table_content": "|| [BOLD] Model | dev MAP | dev MRR | train MAP | train MRR ||\n|| [BOLD] PS- [ITALIC] USD_T | 0.651 | 0.795 | 0.693 | 0.830 ||\n|| [BOLD] PS- [ITALIC] avg-glove | 0.617 | 0.753 | 0.876 | 0.945 ||\n|| [BOLD] PS- [ITALIC] avg-elmo-s | 0.471 | 0.611 | 0.483 | 0.625 ||\n|| [BOLD] PS- [ITALIC] rnn-glove | 0.700 | 0.822 | [BOLD] 0.919 | [BOLD] 0.971 ||\n|| [BOLD] PS- [ITALIC] rnn-elmo-s | 0.716 | 0.841 | 0.813 | 0.916 ||\n|| [BOLD] PS- [ITALIC] rnn-elmo | [BOLD] 0.734 | [BOLD] 0.853 | 0.863 | 0.945 ||\n|| [BOLD] PS- [ITALIC] rnn-bert | 0.667 | 0.806 | 0.708 | 0.841 ||",
    "claim": "Average pooling (-avg) performs better than RNN encoding (-rnn) in all cases, and average pooling with ELMo representation (PS-avg-elmo-s) performs worse than with GloVe representation (PS-avg-glove).",
    "label": "refute"
  },
  {
    "id": "training_1177_refute",
    "table_caption": "Table: Effect of number of permutations P on S-RAILS+NAWE performance on the development set, for signature length b=1024 and beamwidth B=2,000.",
    "table_content": "|| [ITALIC] P | [BOLD] Median Example  [BOLD] FOM | [BOLD] Median Example  [BOLD] OTWV | [BOLD] Median Example  [BOLD] P@10 | [BOLD] Best Example  [BOLD] FOM | [BOLD] Best Example  [BOLD] OTWV | [BOLD] Best Example  [BOLD] P@10 ||\n|| 4 | 48.8 | 33.2 | 45.2 | 75.2 | 59.0 | 83.0 ||\n|| 8 | 60.9 | 41.0 | 50.3 | 80.3 | 63.8 | 85.0 ||\n|| 16 | 69.1 | 46.5 | 54.5 | 84.1 | 66.7 | 84.8 ||",
    "claim": "As the number of permutations increases from 4 to 16, the Median Example FOM improves, but the OTWV score decreases.",
    "label": "refute"
  },
  {
    "id": "training_1060_refute",
    "table_caption": "Table: Effects of distillation across different models and inference. All results are BLEU scores from the dev data. T and b denote the max number of iterations and beam size respectively.",
    "table_content": "|| Model | T | [BOLD] en\\rightarrowde raw | [BOLD] en\\rightarrowde dist. | [BOLD] en\\rightarrowde \\Delta | [BOLD] ro\\rightarrowen raw | [BOLD] ro\\rightarrowen dist. | [BOLD] ro\\rightarrowen \\Delta ||\n|| CMLM + MaskP | 4 | 22.7 | 25.5 | 2.8 | 33.2 | 34.8 | 1.6 ||\n|| CMLM + MaskP | 10 | 24.5 | 25.9 | 1.4 | 34.5 | 34.9 | 0.4 ||\n|| DisCo + MaskP | 4 | 21.4 | 24.6 | 3.2 | 32.3 | 34.1 | 1.8 ||\n|| DisCo + MaskP | 10 | 23.6 | 25.3 | 1.7 | 33.4 | 34.3 | 0.9 ||\n|| DisCo + EasyF | 10 | 23.9 | 25.6 | 1.7 | 34.0 | 35.0 | 1.0 ||\n|| AT Base (b=1) | N | 25.5 | 26.4 | 0.9 | – | – | – ||\n|| AT Base (b=5) | N | 26.1 | 26.8 | 0.7 | – | – | – ||",
    "claim": "The CMLM transformer benefits more from distillation compared to the DisCo under the same mask-predict inference conditions.",
    "label": "refute"
  },
  {
    "id": "training_1106_refute",
    "table_caption": "Table: Readmission prediction performance",
    "table_content": "|| [BOLD] Model | [BOLD] Acc | [BOLD] Pre-0 | [BOLD] Pre-1 | [BOLD] Re-0 | [BOLD] Re-1 | [BOLD] A.R. | [BOLD] A.P. ||\n||  | 0.698 | 0.916 | 0.367 | 0.687 | 0.742 | 0.791 | 0.513 ||\n|| LSTM | 0.840 | 0.956 | 0.366 | 0.859 | 0.704 | 0.794 | 0.600 ||\n|| CC-LSTM | 0.848 | 0.978 | 0.321 | 0.854 | 0.786 | [BOLD] 0.804 | 0.613 ||",
    "claim": "CC-LSTM achieves the highest recall for class 0 (Re-0) among the models listed.",
    "label": "refute"
  },
  {
    "id": "training_1162_refute",
    "table_caption": "Table: Comparisons on the 20 Newsgroups dataset for classifying documents into all topics. The disriminative RBM result is from drbm; compressive feature learning and LR-5-grams results are from compressive, and the distributed structured output result is from srikumar.",
    "table_content": "|| [BOLD] Method | [BOLD] Acc. ||\n|| Discriminative RBM | 76.20 ||\n|| Compressive feature learning | 83.00 ||\n|| LR-{1,2,3,4,5}-grams | 82.80 ||\n|| Distributed structured output | 84.00 ||\n|| [BOLD] LR (this work) | 87.84 ||",
    "claim": "The \"Distributed structured output\" method achieves an accuracy of 87.84, which is the highest among the methods listed.",
    "label": "refute"
  },
  {
    "id": "training_1182_refute",
    "table_caption": "Table: Evaluation results of baselines and our models. CLMN− refers to CLMN without using pre-trained BERT.",
    "table_content": "|| [BOLD] Model | [ITALIC] R2@1 | [ITALIC] R10@1 | [ITALIC] R10@2 | [ITALIC] R10@5 | [ITALIC] MRR ||\n|| Unsupervised models | Unsupervised models | Unsupervised models | Unsupervised models | Unsupervised models | Unsupervised models ||\n|| TbTQT | 86.0 | 40.1 | 64.7 | 94.5 | 61.7 ||\n|| BWE-AGG | 64.1 | 20.4 | 33.6 | 65.5 | 40.3 ||\n|| BWE-IDF | 62.0 | 19.0 | 32.6 | 66.4 | 39.3 ||\n|| Bigram | 64.0 | 19.4 | 33.4 | 65.3 | 39.6 ||\n|| Translation+Monolingual information retrieval models | Translation+Monolingual information retrieval models | Translation+Monolingual information retrieval models | Translation+Monolingual information retrieval models | Translation+Monolingual information retrieval models | Translation+Monolingual information retrieval models ||\n|| SMN | 94.7 | 73.6 | 88.8 | 98.4 | 84.2 ||\n|| DAM | 95.8 | 78.5 | 91.5 | 98.8 | 87.4 ||\n|| CSRAN | 96.0 | 79.8 | 92.5 | 99.0 | 88.2 ||\n|| Cross-lingual information retrieval model | Cross-lingual information retrieval model | Cross-lingual information retrieval model | Cross-lingual information retrieval model | Cross-lingual information retrieval model | Cross-lingual information retrieval model ||\n|| ML-BERT | 97.2 | 83.8 | 95.0 | 99.3 | 90.8 ||\n|| POSIT-DRMM | 95.4 | 74.3 | 90.7 | 99.3 | 85.1 ||\n|| CLMN− | 97.1 | 83.5 | 94.7 | 99.6 | 90.5 ||\n|| [BOLD] CLMN | [BOLD] 97.8 | [BOLD] 86.8 | [BOLD] 95.5 | [BOLD] 99.8 | [BOLD] 92.3 ||",
    "claim": "The CLMN model achieves the best performance on four evaluation metrics, but ML-BERT outperforms it on the R2@1 metric.",
    "label": "refute"
  },
  {
    "id": "training_1147_refute",
    "table_caption": "Table: Word level accuracies for language identification (10-fold).",
    "table_content": "|| [EMPTY] | Indonesian-English MarMoT | Indonesian-English Bilty | Indonesian-English MaChAmp | Turkish-German MarMoT | Turkish-German Bilty | Turkish-German MaChAmp ||\n|| Lang1 (Id/Tr) | 96.79 | 97.29 | 97.53 | 96.90 | 96.91 | 97.54 ||\n|| Lang2 (En/De) | 92.11 | 93.57 | 94.81 | 86.65 | 90.88 | 93.71 ||\n|| Unspecified | 85.68 | 87.75 | 91.11 | 88.86 | 91.29 | 92.75 ||\n|| Total | 92.71 | 93.86 | 95.17 | 92.76 | 94.26 | 95.57 ||",
    "claim": "MaChAmp consistently outperforms MarMoT and Bilty across all categories, except in the Turkish-German Lang1 category.",
    "label": "refute"
  },
  {
    "id": "training_1173_refute",
    "table_caption": "Table: Hyper-parameters for training.",
    "table_content": "|| [BOLD] hyper-parameter Word embeddings | [BOLD] value English: 100 Chinese: 80 | [BOLD] hyper-parameter Word LSTM layers | [BOLD] value 2 ||\n|| Word LSTM hidden units | 200 | Character embeddings | 20 ||\n|| Character LSTM layers | 1 | Character LSTM hidden units | 25 ||\n|| Tree-LSTM hidden units | 200 | POS tag embeddings | 32 ||\n|| Constituent label embeddings | 32 | Label LSTM layers | 1 ||\n|| Label LSTM hidden units | 200 | Last output layer hidden units | 128 ||\n|| Maximum training epochs | 50 | Dropout | English: 0.5, Chinese 0.3 ||\n|| Trainer | SGD | Initial learning rate | 0.1 ||\n|| Per-epoch decay | 0.05 | [ITALIC] ϕ | ELU  ||",
    "claim": "The model is optimized using stochastic gradient descent (SGD) with an initial learning rate of 0.1, and the dropout probability for Chinese is set to 0.5.",
    "label": "refute"
  },
  {
    "id": "training_1110_refute",
    "table_caption": "Table: Results of VQA3+, WUPS-acm, WUPS–mcm, MaSSeS and its components on four VQA datasets.",
    "table_content": "|| [BOLD] dataset | [BOLD] metric  [BOLD] VQA3+ | [BOLD] metric  [BOLD] WUPS | [BOLD] metric  [BOLD] WUPS | [BOLD] metric  [BOLD] MaSSeS | [BOLD] metric  [BOLD] MaSSeS | [BOLD] metric  [BOLD] MaSSeS | [BOLD] metric  [BOLD] MaSSeS | [BOLD] metric  [BOLD] MaSSeS | [BOLD] metric  [BOLD] MaSSeS | [BOLD] metric  [BOLD] MaSSeS ||\n|| [EMPTY] | [EMPTY] | acm0.9 | mcm0.9 | Ma | S | SeS0.7 | SeS0.9 | MaS | MaSSeS0.7 | MaSSeS0.9 ||\n|| [ITALIC] VQA 1.0 | 0.542 | 0.479 | 0.642 | 0.523 | 0.731 | 0.922 | 0.786 | 0.425 | 0.567 | 0.458 ||\n|| [ITALIC] VQA 2.0 | 0.516 | 0.441 | 0.634 | 0.495 | 0.705 | 0.907 | 0.760 | 0.384 | 0.545 | 0.418 ||\n|| [ITALIC] VQA-abstract | 0.602 | 0.532 | 0.685 | 0.582 | 0.780 | 0.944 | 0.818 | 0.482 | 0.618 | 0.507 ||\n|| [ITALIC] VizWiz | 0.448 | 0.163 | 0.441 | 0.444 | 0.460 | 0.705 | 0.541 | 0.207 | 0.292 | 0.227 ||",
    "claim": "Accuracies obtained with both versions of MaSSeS are lower compared to those of VQA3+, with the drop being particularly accentuated for VQA-abstract.",
    "label": "refute"
  },
  {
    "id": "training_1109_refute",
    "table_caption": "Table: Examples from the validation splits of VQA 1.0 (top) and VizWiz (bottom). For each example, we report the pattern of answers provided by annotators (unique answer: frequency), the prediction of the model, and the scores (note that acm, SeS, MaSSeS are computed using threshold 0.9). Answers that are grouped together by SeS are included in square brackets.",
    "table_content": "|| [BOLD] dataset | [BOLD] n. | [BOLD] answers | [BOLD] prediction | [BOLD] VQA3+ | [BOLD] acm | [BOLD] Ma | [BOLD] S | [BOLD] SeS | [BOLD] MaSSeS ||\n|| [ITALIC] VQA 1.0 | 1 | [yellow: 5, orange: 4, light orange: 1] | [ITALIC] yellow | 1.0 | 0.53 | 1.0 | 0.44 | 1.0 | 1.0 ||\n|| [ITALIC] VQA 1.0 | 2 | [refrigerator: 6, fridge: 4] | [ITALIC] refrigerator | 1.0 | 0.98 | 1.0 | 0.55 | 1.0 | 1.0 ||\n|| [ITALIC] VQA 1.0 | 3 | [tennis rackets: 4, tennis racket: 2, tennis racquet: 1], racket: 2, racquets: 1 | [ITALIC] tennis rackets | 1.0 | 0.98 | 1.0 | 0.33 | 0.67 | 0.67 ||\n|| [ITALIC] VQA 1.0 | 4 | [hot dogs: 5, hot dog: 2, hot dogs and fries: 1, hot dog fries: 1, hot dog and onion rings: 1] | [ITALIC] hot dog | 0.60 | 0.70 | 0.4 | 0.44 | 1.0 | 1.0 ||\n|| [ITALIC] VizWiz | 1 | [christmas tree: 6, tree: 1, chritmas tree shaped santaclauses: 1, christmas tree santas: 1], santas: 1 | [ITALIC] christmas tree | 1.0 | 0.70 | 1.0 | 0.55 | 0.89 | 0.89 ||\n|| [ITALIC] VizWiz | 2 | white: 6, [green: 2, light green: 1, very light green: 1] | [ITALIC] white | 1.0 | 0.62 | 1.0 | 0.55 | 0.55 | 0.55 ||\n|| [ITALIC] VizWiz | 3 | [ginger peach: 5, ginger peach tea: 2, ginger peach herbal tea: 1], unanswerable: 2 | [ITALIC] unanswerable | 0.60 | 0.20 | 0.4 | 0.44 | 0.77 | 0.19 ||\n|| [ITALIC] VizWiz | 4 | [beef: 5, beef flavored broth: 2, beef flavored: 1, beef flavor: 1, this beef flavor: 1] | [ITALIC] unanswerable | 0.0 | 0.0 | 0.0 | 0.44 | 1.0 | 0.0 ||",
    "claim": "Examples 1 and 2 from VQA 1.0 are considered 100% correct by both VQA3+ and MaSSeS. MaSSeS assigns a lower score (0.67) for example 3 compared to VQA3+ (1.0). For example 4, MaSSeS assigns a lower score (0.6) than VQA3+ (0.6) and acm (0.7).",
    "label": "refute"
  },
  {
    "id": "training_1154_refute",
    "table_caption": "Table: The effect of GCN.",
    "table_content": "|| Models | Restaurant Acc | Restaurant Macro-F1 | Laptop Acc | Laptop Macro-F1 ||\n|| Att | 81.43 | 72.40 | 72.12 | 68.67 ||\n|| Att+GCN | 82.77 | 74.33 | 74.61 | 70.33 ||\n|| BiAtt | 81.61 | 73.49 | 73.51 | 69.73 ||\n|| BiAtt+GCN (SDGCN) | 82.95 | 75.79 | 75.55 | 71.35 ||",
    "claim": "Models with GCN (Att+GCN and BiAtt+GCN) achieve higher performance in both accuracy and macro-F1 scores compared to models without GCN (Att and BiAtt) in the restaurant domain, but models without GCN perform better in the laptop domain.",
    "label": "refute"
  },
  {
    "id": "training_1092_refute",
    "table_caption": "Table: Comparison with baseline methods trained on different backbone models (second column). * indicates the method trained using an extra corpus.",
    "table_content": "|| Method | Model | MT06 | MT02 | MT03 | MT04 | MT05 | MT08 ||\n|| Vaswani:17 | Trans.-Base | 44.59 | 44.82 | 43.68 | 45.60 | 44.57 | 35.07 ||\n|| Miyato:17 | Trans.-Base | 45.11 | 45.95 | 44.68 | 45.99 | 45.32 | 35.84 ||\n|| Sennrich:16c | Trans.-Base | 44.96 | 46.03 | 44.81 | 46.01 | 45.69 | 35.32 ||\n|| Wang:18 | Trans.-Base | 45.47 | 46.31 | 45.30 | 46.45 | 45.62 | 35.66 ||\n|| Cheng:18 | RNMT [ITALIC] lex. | 43.57 | 44.82 | 42.95 | 45.05 | 43.45 | 34.85 ||\n|| Cheng:18 | RNMT [ITALIC] feat. | 44.44 | 46.10 | 44.07 | 45.61 | 44.06 | 34.94 ||\n|| Cheng:18 | Trans.-Base [ITALIC] feat. | 45.37 | 46.16 | 44.41 | 46.32 | 45.30 | 35.85 ||\n|| Cheng:18 | Trans.-Base [ITALIC] lex. | 45.78 | 45.96 | 45.51 | 46.49 | 45.73 | 36.08 ||\n|| Sennrich:16b* | Trans.-Base | 46.39 | 47.31 | 47.10 | 47.81 | 45.69 | 36.43 ||\n|| Ours | Trans.-Base | 46.95 | 47.06 | 46.48 | 47.39 | 46.58 | 37.38 ||\n|| Ours + BackTranslation* | Trans.-Base | [BOLD] 47.74 | [BOLD] 48.13 | [BOLD] 47.83 | [BOLD] 49.13 | [BOLD] 49.04 | [BOLD] 38.61 ||",
    "claim": "\"Ours + BackTranslation*\" achieves the highest scores across most datasets, but Sennrich:16b* outperforms it on MT05.",
    "label": "refute"
  },
  {
    "id": "training_1183_refute",
    "table_caption": "Table: Ablation study of cross-lingual alignment, where mono refers to without cross-lingual alignment learning, en2ch refers to calculating the matching score in the Chinese bilingual space, and ch2en is to compute the relevance score in the English Bilingual Space.",
    "table_content": "|| [BOLD] Model | [ITALIC] R2@1 | [ITALIC] R10@1 | [ITALIC] R10@2 | [ITALIC] R10@5 | [ITALIC] MRR ||\n|| Initialize with word embedding | Initialize with word embedding | Initialize with word embedding | Initialize with word embedding | Initialize with word embedding | Initialize with word embedding ||\n|| CLMN−-mono | 95.8 | 78.3 | 91.6 | 99.2 | 87.3 ||\n|| CLMN−-en2ch | 96.3 | 81.5 | 93.1 | 99.5 | 89.2 ||\n|| CLMN−-ch2en | 96.7 | 81.8 | 93.4 | 99.5 | 89.5 ||\n|| CLMN− | 97.1 | 83.5 | 94.7 | 99.6 | 90.5 ||\n|| Initialize with BERT | Initialize with BERT | Initialize with BERT | Initialize with BERT | Initialize with BERT | Initialize with BERT ||\n|| CLMN-mono | 96.5 | 80.7 | 93.8 | 99.8 | 89.1 ||\n|| CLMN-en2ch | 97.2 | 84.6 | 95.4 | 99.7 | 91.4 ||\n|| CLMN-ch2en | 97.5 | 85.5 | 94.8 | 99.8 | 91.7 ||\n|| [BOLD] CLMN | [BOLD] 97.8 | [BOLD] 86.8 | [BOLD] 95.5 | [BOLD] 99.8 | [BOLD] 92.3 ||",
    "claim": "The CLMN model initialized with word embedding achieves the highest performance across all metrics (R2@1, R10@1, R10@2, R10@5, MRR).",
    "label": "refute"
  },
  {
    "id": "training_1038_refute",
    "table_caption": "Table: Results for Validation Seen and Validation Unseen, when trained with a small fraction of Fried-Augmented ordered by scores given by model trained on cma. SPL and SR are reported as percentages and NE and PL in meters.",
    "table_content": "|| Dataset size | Strategy | [BOLD] Validation Seen PL | [BOLD] Validation Seen NE ↓ | [BOLD] Validation Seen SR ↑ | [BOLD] Validation Seen SPL ↑ | [BOLD] Validation Unseen PL | [BOLD] Validation Unseen NE ↓ | [BOLD] Validation Unseen SR ↑ | [BOLD] Validation Unseen SPL ↑ ||\n|| 1% | Top | 11.1 | 8.5 | 21.2 | 17.6 | 11.2 | 8.5 | 20.4 | 16.6 ||\n|| 1% | Bottom | 10.7 | 9.0 | 16.3 | 13.1 | 10.8 | 8.9 | 15.4 | 14.1 ||\n|| 2% | Top | 11.7 | 7.9 | 25.5 | 21.0 | 11.3 | 8.2 | 22.3 | 18.5 ||\n|| 2% | Bottom | 14.5 | 9.1 | 17.7 | 12.7 | 11.4 | 8.4 | 17.5 | 14.1 ||",
    "claim": "Using low-quality examples results in better performance compared to high-quality examples, as indicated by higher SR and SPL values for the \"Bottom\" strategy.",
    "label": "refute"
  },
  {
    "id": "training_1211_refute",
    "table_caption": "Table: VQA2.0 accuracy on Validation set for DCN and DAN",
    "table_content": "|| [BOLD] Models | [BOLD] All | [BOLD] Yes/No | [BOLD] Number | [BOLD] others ||\n|| SAN-2 | 52.82 | - | - | - ||\n|| DAN(K=1) +LQIA | [BOLD] 52.96 | [BOLD] 70.08 | [BOLD] 34.06 | [BOLD] 44.20 ||\n|| DCN Add_v1(K=1)+LQIA | 53.01 | 70.13 | 33.98 | 44.27 ||\n|| DCN Add_v2(K=1) +LQIA | 53.07 | 70.46 | 34.30 | 44.10 ||\n|| DCN Mul_v1(K=1) +LQIA | 53.18 | 70.24 | 34.53 | 44.24 ||\n|| DCN Mul_v2(K=1)+LQIA | [BOLD] 53.26 | [BOLD] 70.57 | [BOLD] 34.61 | [BOLD] 44.39 ||\n|| DCN Add_v1(K=4)+MCB | 65.30 | 81.89 | 42.93 | 55.56 ||\n|| DCN Add_v2(K=4)+MCB | 65.41 | 81.90 | 42.88 | 55.99 ||\n|| DCN Mul_v1(K=4)+MCB | 65.52 | 82.07 | 42.91 | 55.97 ||\n|| DCN Mul_v2(K=4)+MCB | [BOLD] 65.90 | [BOLD] 82.40 | [BOLD] 43.18 | [BOLD] 56.81 ||",
    "claim": "DCN Mul_v2(K=4)+MCB achieves an accuracy of 65.80% in the \"All\" category.",
    "label": "refute"
  },
  {
    "id": "training_1197_refute",
    "table_caption": "Table: Personalization task 2 (Updating API calls). The model correctly learns to focus on the original API call and the changes demanded by the user. It does not require multiple hops to solve this task.",
    "table_content": "|| [BOLD] Time | [BOLD] Locutor | [BOLD] Dialog History | [BOLD] Hop #1 | [BOLD] Hop #2 | [BOLD] Hop #3 ||\n|| 1 | [EMPTY] | female middle-aged | 0.004 | 0 | 0 ||\n|| 2 | User | good morning | 0.008 | 0 | 0 ||\n|| 3 | Bot | hello maam how can i help you | 0 | 0 | 0 ||\n|| 4 | User | may i have a table in bombay with spanish food for four in a expensive price range | 0.001 | 0 | 0.999 ||\n|| 5 | Bot | give me a second for processing the reservation | 0.001 | 0 | 0 ||\n|| 6 | User |  | 0.082 | 0 | 0 ||\n|| 7 | Bot | sure maam i’m finding some options for you | 0 | 0 | 0 ||\n|| 8 | User |  | 0.05 | 0 | 0 ||\n|| 9 | Bot | api_call spanish bombay four expensive | 0.214 | 0.002 | 0 ||\n|| 10 | User | actually i would prefer with french food | 0.635 | 0.997 | 0 ||\n|| 11 | Bot | great is there any other thing to modify | 0 | 0 | 0 ||\n|| 12 | User | no | 0.001 | 0 | 0 ||\n|| 13 | Bot | sure maam i’m finding some options for you | 0 | 0 | 0 ||\n|| [BOLD] User input | [BOLD] User input |  |  |  |  ||\n|| [BOLD] Correct answer | [BOLD] Correct answer | api_call french bombay four expensive | api_call french bombay four expensive | api_call french bombay four expensive | api_call french bombay four expensive ||\n|| [BOLD] Predicted answer | [BOLD] Predicted answer | api_call french bombay four expensive | api_call french bombay four expensive | api_call french bombay four expensive | api_call french bombay four expensive ||",
    "claim": "They completed PT1 with a very high degree of accuracy, but PT2 was not completed accurately.",
    "label": "refute"
  },
  {
    "id": "training_1160_refute",
    "table_caption": "Table: Experiments on choice of K, the number of captions retrieved for each image. B-1 / B-4 / M / R / C / S refers to BLEU1/ BLEU4 / METEOR / ROUGE-L / CIDEr / SPICE scores. Experiments are conducted on MSCOCO Karpathy validation set.",
    "table_content": "|| Top  [ITALIC] K | Cross-Entropy Loss B-1 | Cross-Entropy Loss B-4 | Cross-Entropy Loss M | Cross-Entropy Loss R | Cross-Entropy Loss C | Cross-Entropy Loss S ||\n|| [ITALIC] K=1 | 77.1 | 36.3 | 27.8 | 56.9 | 115.8 | 21.2 ||\n|| [ITALIC] K=5 | 77.1 | 36.5 | 28.0 | 57.0 | [BOLD] 116.7 | 21.3 ||\n|| [ITALIC] K=15 | 77.0 | 36.3 | 27.9 | 56.6 | 115.6 | 21.0 ||",
    "claim": "Retrieving the top 1 captions results in the highest cross-entropy loss C value compared to retrieving 5 or 15 captions.",
    "label": "refute"
  },
  {
    "id": "training_1113_refute",
    "table_caption": "Table: Test performance (classification error) as polarity classifiers. LL stands for LibLinear (SVM), μi, γu.γi, fA, fT are the recommender systems as in table 2. LL + fA and LL + fT are two hybrid opinion classification models combining the SVM classifier and fA and fT recommender systems.",
    "table_content": "|| Subsets | LL | [ITALIC] μi | [ITALIC] γu. [ITALIC] γi | [ITALIC] fA | [ITALIC] fT | LL +  [ITALIC] fA | LL +  [ITALIC] fT ||\n|| RB_U50_I200 | 5.35 | 5.12 | 6.01 | 5.57 | 5.57 | [BOLD] 3.79 | [BOLD] 3.79 ||\n|| RB_U500_I2k | 7.18 | 10.67 | 9.73 | 8.55 | 8.55 | [BOLD] 6.52 | 6.92 ||\n|| RB_U5k_I20k | 8.44 | 11.80 | 10.04 | 9.17 | 9.17 | [BOLD] 8.33 | [BOLD] 8.35 ||\n|| A_U200_I120 | [BOLD] 10.00 | 15.83 | 22.50 | 20.00 | 20.83 | [BOLD] 10.00 | [BOLD] 10.00 ||\n|| A_U2k_I1k | 7.89 | 15.25 | 12.85 | 12.62 | 12.62 | [BOLD] 7.54 | [BOLD] 7.54 ||\n|| A_U20k_I12k | [BOLD] 6.34 | 13.99 | 12.79 | 12.38 | 12.37 | [BOLD] 6.29 | [BOLD] 6.29 ||\n|| A_U210k_I120k | [BOLD] 6.25 | 14.04 | 14.40 | 13.32 | 13.31 | [BOLD] 6.22 | [BOLD] 6.22 ||",
    "claim": "The hybrid models LL + fA and LL + fT have higher classification errors than all other recommender systems across all subsets.",
    "label": "refute"
  },
  {
    "id": "training_1188_refute",
    "table_caption": "Table: AIC values of five fitted models with corresponding degrees of freedom (df), fitted with Maximum Likelihood. AIC of Select is not listed because it was not fitted with ML; AIC of Select fitted with REML is, however, similar to Excluded (=1,008.46 vs. 1008.54).",
    "table_content": "|| [EMPTY] | df | AIC ||\n|| Full | 108.94 | 1018.38 ||\n|| Modified | 88.06 | 1031.03 ||\n|| Excluded | 71.51 | 1008.20 ||\n|| Linear | 101.00 | 1036.04 ||\n|| Linear excluded | 78.00 | 1007.06 ||",
    "claim": "The Linear model has the lowest AIC score.",
    "label": "refute"
  },
  {
    "id": "training_1166_refute",
    "table_caption": "Table: Results for identifying the Arg1 and Arg2 subtree nodes for the SS case under the GS + no EP setting for the three categories.",
    "table_content": "|| [EMPTY] | Arg1  [ITALIC] F1 | Arg2  [ITALIC] F1 | Rel  [ITALIC] F1 ||\n|| Subordinating | 88.46 | 97.93 | 86.98 ||\n|| Coordinating | 90.34 | 90.34 | 82.39 ||\n|| Discourse adverbial | 46.88 | 62.50 | 37.50 ||\n|| All | 86.63 | 93.41 | 82.60 ||",
    "claim": "Subordinating connectives have a high Arg2 F1 score of 97.93%, and coordinating connectives have different Arg1 and Arg2 F1 scores, with Arg1 being higher.",
    "label": "refute"
  },
  {
    "id": "training_1149_refute",
    "table_caption": "Table: Model performance on the HotpotQA dataset (top scores marked in bold). Models [1-5] are from (Shen et al.,2017a; Tran et al., 2018; Wang and Jiang, 2016; Bian et al.,2017; Yoon et al., 2019), respectively.",
    "table_content": "|| [BOLD] Model | dev MAP | dev MRR | train MAP | train MRR ||\n|| [BOLD] IWAN [1] | 0.526 | 0.680 | 0.605 | 0.775 ||\n|| [BOLD] sCARNN [2] | 0.534 | 0.698 | 0.620 | 0.792 ||\n|| [BOLD] CompAggr [3] | 0.659 | 0.812 | 0.796 | 0.911 ||\n|| [BOLD] CompAggr-kMax [4] | 0.670 | 0.825 | 0.767 | 0.901 ||\n|| [BOLD] CompClip-LM-LC [5] | 0.702 | 0.848 | 0.757 | 0.884 ||\n|| [BOLD] PS- [ITALIC] rnn-elmo-s | 0.716 | 0.841 | 0.813 | 0.916 ||\n|| [BOLD] PS- [ITALIC] rnn-elmo | [BOLD] 0.734 | [BOLD] 0.853 | [BOLD] 0.863 | [BOLD] 0.945 ||",
    "claim": "PS-rnn-elmo achieves the highest dev MAP, dev MRR, and train MAP, but CompAggr-kMax achieves the highest train MRR among all the models.",
    "label": "refute"
  },
  {
    "id": "training_1187_refute",
    "table_caption": "Table: Coefficients of a beta regression generalized additive model with ratio of maximum intensity ([s] vs. vowel) as the dependent variable.",
    "table_content": "|| A. parametric coefficients | Estimate | Std. Error | t-value | p-value ||\n|| (Intercept) =  [ITALIC] z11 | -0.0571 | 0.0156 | -3.6505 | 0.0003 ||\n|| [ITALIC] z5 | -0.0404 | 0.0123 | -3.2820 | 0.0011 ||\n|| [ITALIC] z14 | -0.0011 | 0.0144 | -0.0753 | 0.9400 ||\n|| [ITALIC] z26 | -0.0097 | 0.0115 | -0.8444 | 0.3989 ||\n|| [ITALIC] z29 | -0.0590 | 0.0113 | -5.2131 | < 0.0001 ||\n|| [ITALIC] z49 | 0.0074 | 0.0112 | 0.6595 | 0.5100 ||\n|| [ITALIC] z74 | -0.0741 | 0.0121 | -6.1071 | < 0.0001 ||\n|| B. smooth terms | edf | Ref.df | F-value | p-value ||\n|| s(zValuePerc): [ITALIC] z5 | 1.0002 | 1.0000 | 11.8417 | 0.0006 ||\n|| s(zValuePerc): [ITALIC] z11 | 4.1696 | 4.8546 | 14.1190 | < 0.0001 ||\n|| s(zValuePerc): [ITALIC] z14 | 5.3322 | 6.1117 | 36.6899 | < 0.0001 ||\n|| s(zValuePerc): [ITALIC] z26 | 1.0003 | 1.0002 | 12.5952 | 0.0004 ||\n|| s(zValuePerc): [ITALIC] z29 | 1.0002 | 1.0000 | 12.0036 | 0.0006 ||\n|| s(zValuePerc): [ITALIC] z49 | 4.2002 | 4.8650 | 19.1225 | < 0.0001 ||\n|| s(zValuePerc): [ITALIC] z74 | 3.2768 | 3.7863 | 1.2326 | 0.2479 ||\n|| fs(zValuePerc,sameValues,m=1,k=5) | 110.6863 | 143.0000 | 6.1542 | < 0.0001 ||\n|| fs(zValuePerc,trajectoryZ,m=1,k=5) | 558.2060 | 728.0000 | 56.9670 | < 0.0001 ||",
    "claim": "All smooth terms, except for z5, are significantly different from 0.",
    "label": "refute"
  },
  {
    "id": "training_1215_refute",
    "table_caption": "Table: Results on MathQA dataset testing set",
    "table_content": "|| [BOLD] MODEL | [BOLD] Operation Accuracy(%) | [BOLD] Execution Accuracy(%) ||\n|| SEQ2PROG-orig | 59.4 | 51.9 ||\n|| SEQ2PROG-best | 66.97 | 54.0 ||\n|| LSTM2TP (ours) | 68.21 | 54.61 ||\n|| TP2LSTM (ours) | 68.84 | 54.61 ||\n|| [BOLD] TP-N2F (ours) | [BOLD] 71.89 | [BOLD] 55.95 ||",
    "claim": "SEQ2PROG-best achieves the highest operation accuracy and execution accuracy among the models compared.",
    "label": "refute"
  },
  {
    "id": "training_1118_refute",
    "table_caption": "Table: Ablated study on WebQuestionsSP",
    "table_content": "|| [EMPTY] | WebQuestionsSP ||\n|| EmQL | [BOLD] 75.5 ||\n|| EmQL (no-sketch) | 53.2 ||\n|| EmQL (no-constr) | 65.2 ||\n|| EmQL (approx. MIPS) | 73.4 ||\n|| EmQL (no-bert) | 74.2 ||",
    "claim": "The performance of EmQL (no-bert) on the WebQuestionsSP dataset is 1.3% higher than the full EmQL model.",
    "label": "refute"
  },
  {
    "id": "training_1209_refute",
    "table_caption": "Table: Analysis network parameter for DAN",
    "table_content": "|| [BOLD] Models | VQA1.0 Open-Ended (test-dev)  [BOLD] All | VQA1.0 Open-Ended (test-dev)  [BOLD] Yes/No | VQA1.0 Open-Ended (test-dev)  [BOLD] Number | VQA1.0 Open-Ended (test-dev)  [BOLD] others | HAT val dataset  [BOLD] Rank-correlation ||\n|| LSTM Q+I+ Attention(LQIA) | 56.1 | 80.3 | 37.4 | 40.46 | 0.2142 ||\n|| DAN(K=1)+LQIA | 59.2 | 80.1 | 36.1 | 46.6 | 0.2959 ||\n|| DAN(K=2)+LQIA | 59.5 | 80.9 | 36.6 | 47.1 | 0.3090 ||\n|| DAN(K=3)+LQIA | 59.9 | 80.6 | 37.2 | [BOLD] 47.5 | 0.3100 ||\n|| DAN(K=4)+LQIA | [BOLD] 60.2 | [BOLD] 80.9 | [BOLD] 37.4 | 47.2 | [BOLD] 0.3206 ||\n|| DAN(K=1)+MCB | 64.8 | 82.4 | 38.1 | 54.2 | 0.3284 ||\n|| DAN(K=2)+MCB | 64.8 | 82.9 | 38.0 | 54.3 | 0.3298 ||\n|| DAN(K=3)+MCB | 64.9 | 82.6 | 38.2 | 54.6 | 0.3316 ||\n|| DAN(K=4)+MCB | [BOLD] 65.0 | 83.1 | [BOLD] 38.4 | [BOLD] 54.9 | [BOLD] 0.3326 ||\n|| DAN(K=5)+LQIA | 58.1 | 79.4 | 36.9 | 45.7 | 0.2157 ||\n|| DAN(K=1,Random)+LQIA | 56.4 | 79.3 | 37.1 | 44.6 | 0.2545 ||",
    "claim": "Using 3 nearest neighbors in the DAN network yields the highest accuracy, and increasing k beyond 4 results in reduced accuracy.",
    "label": "refute"
  },
  {
    "id": "training_1150_refute",
    "table_caption": "Table: Model performance with different typologies. The connection strategies between nodes for each type are illustrated in figure 4.",
    "table_content": "|| [BOLD] Model | dev MAP | dev MRR | train MAP | train MRR ||\n|| [BOLD] PS- [ITALIC] rnn-elmo-s | [BOLD] 0.716 | [BOLD] 0.841 | [BOLD] 0.813 | [BOLD] 0.916 ||\n|| [BOLD] Type-1 ( [ITALIC] rnn-elmo-s) | 0.694 | 0.834 | 0.807 | 0.915 ||\n|| [BOLD] Type-2 ( [ITALIC] rnn-elmo-s) | 0.705 | 0.836 | 0.792 | 0.903 ||\n|| [BOLD] Type-3 ( [ITALIC] rnn-elmo-s) | 0.658 | 0.796 | 0.729 | 0.857 ||",
    "claim": "All model variations (Type-1, Type-2, Type-3) show lower dev MAP, train MAP, and train MRR compared to the original model PS-rnn-elmo-s, but Type-2 has a higher dev MRR.",
    "label": "refute"
  },
  {
    "id": "training_1124_refute",
    "table_caption": "Table: Word similarity Results",
    "table_content": "|| Method | [ITALIC] rs  [BOLD] Gigaword | [ITALIC] rs  [BOLD] Wikipedia ||\n|| none | 0.385 | 0.368 ||\n|| CDA | 0.381 | 0.363 ||\n|| gCDA | 0.381 | 0.363 ||\n|| nCDA | 0.380 | 0.365 ||\n|| gCDS | 0.382 | 0.366 ||\n|| nCDS | 0.380 | 0.362 ||\n|| WED40 | 0.386 | 0.371 ||\n|| WED70 | 0.395 | 0.375 ||\n|| nWED70 | 0.384 | 0.367 ||",
    "claim": "WED40 and WED70 methods outperform the unmitigated embedding on both Gigaword and Wikipedia, while nWED70 performs better than the unmitigated embedding.",
    "label": "refute"
  },
  {
    "id": "training_1003_refute",
    "table_caption": "Table: Ablation study on R52 and Reuters21578. It is clear that LSTM plays an important role by alleviating over-smoothing problem, especially in multi-label classification, which is more prone to over-smoothing.",
    "table_content": "|| Model | R52 | Reuters21578 ||\n|| w/o LSTM | 84.74 | 43.82 ||\n|| w/o Attention | 94.39 | 81.31 ||\n|| w/o Global node | 93.85 | 76.81 ||\n|| Proposal | 95.29 | 82.01 ||",
    "claim": "Removing any component from the proposed model decreases accuracy, with the absence of Attention causing the largest drop.",
    "label": "refute"
  },
  {
    "id": "training_1034_refute",
    "table_caption": "Table: Model performance on the SuperGLUE validation and diagnostic sets. The Avg. column shows the overall SuperGLUE score—an average across the eight tasks, weighting each task equally—as a mean and standard deviation across three restarts.",
    "table_content": "|| [BOLD] Intermediate-  [BOLD] Training Data | [BOLD] Avg.  [ITALIC] μ ( [ITALIC] σ) | [BOLD] BoolQ  [BOLD] Acc. | [BOLD] CB  [BOLD] F1/Acc. | [BOLD] CB  [BOLD] F1/Acc. | [BOLD] COPA  [BOLD] Acc. | [BOLD] MultiRC  [BOLD] F1 [ITALIC] a/EM | [BOLD] MultiRC  [BOLD] F1 [ITALIC] a/EM | [BOLD] ReCoRD  [BOLD] F1/EM | [BOLD] ReCoRD  [BOLD] F1/EM | [BOLD] RTE  [BOLD] Acc. | [BOLD] WiC  [BOLD] Acc. | [BOLD] WSC  [BOLD] Acc. ||\n|| [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) | [BOLD] RoBERTa (large) ||\n|| None | 67.3 (1.2) | 84.3 | 83.1 / | 89.3 | 90.0 | 70.0 / | 27.3 | 86.5 / | 85.9 | 85.2 | [BOLD] 71.9 | 64.4 ||\n|| Base | [BOLD] 72.2 (0.1) | 84.4 | [BOLD] 97.4 / | [BOLD] 96.4 | [BOLD] 94.0 | 71.9 / | 33.3 | 86.1 / | 85.5 | 88.4 | 70.8 | [BOLD] 76.9 ||\n|| Paragraph | 70.3 (0.1) | 84.7 | [BOLD] 97.4 / | [BOLD] 96.4 | 90.0 | 70.4 / | 29.9 | [BOLD] 86.7 / | [BOLD] 86.0 | 86.3 | 70.2 | 67.3 ||\n|| EditPremise | 69.6 (0.6) | 83.0 | 92.3 / | 92.9 | 89.0 | 71.2 / | 31.2 | 86.4 / | 85.7 | 85.6 | 71.0 | 65.4 ||\n|| EditOther | 70.3 (0.1) | 84.2 | 91.8 / | 94.6 | 91.0 | 70.7 / | 31.3 | 86.2 / | 85.6 | 87.4 | 71.5 | 68.3 ||\n|| Contrast | 69.2 (0.0) | 84.1 | 93.1 / | 94.6 | 87.0 | 71.4 / | 29.5 | 84.8 / | 84.1 | 84.5 | 71.5 | 67.3 ||\n|| MNLI8.5k | 71.0 (0.6) | 84.7 | 96.1 / | 94.6 | 92.0 | 71.7 / | 32.3 | 86.4 / | 85.7 | 87.4 | 74.0 | 68.3 ||\n|| MNLIGov8.5k | 70.9 (0.5) | [BOLD] 84.8 | [BOLD] 97.4 / | [BOLD] 96.4 | 92.0 | 71.4 / | 32.0 | 86.2 / | 85.6 | 86.3 | 71.6 | 70.2 ||\n|| ANLI8.5k | 70.5 (0.3) | 84.7 | 96.1 / | 94.6 | 89.0 | 71.6 / | 31.8 | 85.7 / | 85.0 | 85.9 | [BOLD] 71.9 | 70.2 ||\n|| MNLI | 70.0 (0.0) | 85.3 | 89.0 / | 92.9 | 88.0 | [BOLD] 72.2 / | 35.4 | 84.7 / | 84.1 | 89.2 | 71.8 | 66.3 ||\n|| ANLI | 70.4 (0.9) | 85.4 | 92.4 / | 92.9 | 90.0 | 72.0 / | 33.5 | 85.5 / | 84.8 | [BOLD] 91.0 | 71.8 | 66.3 ||\n|| [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) | [BOLD] XLNet (large cased) ||\n|| None | 62.7 (1.3) | 82.0 | 83.1 / | 89.3 | 76.0 | 69.9 / | 26.8 | 80.9 / | 80.1 | 69.0 | 65.2 | 63.5 ||\n|| Base | 67.7 (0.0) | 83.1 | 90.5 / | 92.9 | [BOLD] 89.0 | 70.5 / | 28.2 | 78.2 / | 77.4 | 85.9 | 68.7 | 64.4 ||\n|| Paragraph | 67.3 (0.0) | 82.5 | [BOLD] 90.8 / | [BOLD] 94.6 | 85.0 | 69.8 / | 28.1 | 79.4 / | 78.6 | 83.8 | 69.7 | 64.4 ||\n|| EditPremise | 67.0 (0.4) | 82.8 | 82.8 / | 91.1 | 83.0 | 69.8 / | 28.6 | 79.3 / | 78.5 | 85.2 | [BOLD] 70.2 | [BOLD] 65.4 ||\n|| EditOther | 67.2 (0.1) | 82.9 | 84.4 / | 91.1 | 87.0 | 70.2 / | 29.1 | 79.4 / | 78.6 | 85.6 | 69.7 | 63.5 ||\n|| Contrast | 66.3 (0.6) | 83.0 | 82.5 / | 89.3 | 83.0 | 69.8 / | 28.3 | 80.2 / | 79.5 | 85.9 | 68.2 | 58.7 ||\n|| MNLI8.5k | 67.6 (0.1) | 83.5 | 89.5 / | 92.9 | 88.0 | 69.4 / | 28.3 | 79.5 / | 78.6 | 86.3 | 69.3 | 62.5 ||\n|| MNLIGov8.5k | 67.5 (0.3) | 82.5 | 89.5 / | [BOLD] 94.6 | 85.0 | 70.0 / | 28.1 | 79.8 / | 79.0 | 87.4 | 68.7 | 62.5 ||\n|| ANLI8.5k | 67.2 (0.3) | 83.4 | 86.3 / | 91.1 | 83.0 | 69.3 / | 28.9 | [BOLD] 81.2 / | [BOLD] 80.4 | 85.9 | 70.1 | 63.5 ||\n|| MNLI | 67.7 (0.1) | [BOLD] 84.0 | 85.5 / | 91.1 | [BOLD] 89.0 | [BOLD] 71.5 / | [BOLD] 31.0 | 79.1 / | 78.3 | 87.7 | 68.5 | 63.5 ||\n|| ANLI | [BOLD] 68.1 (0.4) | 83.7 | 82.8 / | 91.1 | 86.0 | 71.3 / | 30.0 | 80.1 / | 79.3 | [BOLD] 89.5 | 69.6 | 66.3 ||",
    "claim": "The Base data yields models that perform worse than the plain RoBERTa or XLNet baseline.",
    "label": "refute"
  },
  {
    "id": "training_989_refute",
    "table_caption": "Table: Performances over different memory types.",
    "table_content": "|| [BOLD] Memory Type | [BOLD] Referenced | [BOLD] Referenced | [BOLD] Unreferenced | [BOLD] Unreferenced ||\n|| [BOLD] Memory Type | [BOLD] BLEU | [BOLD] ROUGE | [BOLD] Length | [BOLD] #Noun ||\n|| [BOLD] addressee memory | [BOLD] 10.63 | 8.73 | 11.34 | [BOLD] 1.68 ||\n|| all utterance memory | 10.39 | [BOLD] 8.78 | [BOLD] 11.38 | 1.37 ||\n|| latest memory | 10.43 | 8.40 | 10.16 | 1.28 ||\n|| speaker memory | 10.03 | 8.28 | 10.72 | 1.66 ||\n|| w/o memory | 10.25 | 8.23 | 10.73 | 1.27 ||",
    "claim": "The addressee memory achieves the best or near-best performances on all metrics, particularly excelling in the ROUGE metric.",
    "label": "refute"
  },
  {
    "id": "training_1107_refute",
    "table_caption": "Table: Performance on language-inferable and non-language-inferable knowledge",
    "table_content": "|| [EMPTY] | [BOLD] # of examples | [BOLD] Hits@10 (%) ||\n|| LI | 76 | 77.6 ||\n|| Non-LI | 24 | 20.8 ||\n|| Total | 100 | 64.0 ||",
    "claim": "The model achieves a Hits@10 of 20.8% on Language-inferable examples and 77.6% on Non-language-inferable examples.",
    "label": "refute"
  },
  {
    "id": "training_1256_refute",
    "table_caption": "Table: Our results on Subtask B.1: Clustering Arguments of Verbs to Frame-Specific Slots. Purity F1-score is denoted as Pu F1, B-Cubed F1-score is denoted as B3 F1. denotes our final submission (# 535483), denotes a supervised Logistic Regression submission that does not comply to the task rules, denotes our post-competition result, denotes a baseline, and denotes the submission of the winning team.",
    "table_content": "|| [BOLD] Method  [ITALIC] Agglomerative Clustering | [BOLD] Method  [ITALIC] Agglomerative Clustering | [BOLD] Pu F1  [ITALIC] Agglomerative Clustering | [BOLD] B3 F1  [ITALIC] Agglomerative Clustering ||\n|| [EMPTY] | Subtask A: w2v[c+w] | [BOLD] 62.10 | [BOLD] 49.49 ||\n|| [EMPTY] | Subtask B.2: ID | [BOLD] 62.10 | [BOLD] 49.49 ||\n|| [ITALIC] Logistic Regression | [ITALIC] Logistic Regression | [ITALIC] Logistic Regression | [ITALIC] Logistic Regression ||\n|| [EMPTY] | Subtask A: w2v[c+w]norm | 66.81 | 55.61 ||\n|| [EMPTY] | Subtask B.2: ELMo[c+w+v]+ID+B+123 | 66.81 | 55.61 ||\n|| [EMPTY] | Subtask A: ELMo[c+w]norm | 68.22 | 58.61 ||\n|| [EMPTY] | Subtask B.2: w2v[c+w+v]+ID+B+123 | 68.22 | 58.61 ||\n|| [EMPTY] | Cluster Per Dependency Role | 57.99 | 45.79 ||\n|| [EMPTY] | Winner | 62.10 | 49.49 ||",
    "claim": "The logistic regression model demonstrated worse performance in Subtask B.2 than the agglomerative clustering technique, including the final submission.",
    "label": "refute"
  },
  {
    "id": "training_1217_refute",
    "table_caption": "Table: Results of the smoothed model fRS with different K on IMDB using text CNN. “Clean” represents the accuracy on the clean data without adversarial attacking and “Certified” the certified accuracy.",
    "table_content": "|| [ITALIC] K | 20 | 50 | 100 | 250 | 1000 ||\n|| Clean (%) | 88.47 | 88.48 | 88.09 | 84.83 | 67.54 ||\n|| Certified (%) | 65.58 | 77.32 | 81.16 | 79.98 | 65.13 ||",
    "claim": "Clean accuracy decreases as K increases, and the best certified accuracy is achieved at K=250.",
    "label": "refute"
  },
  {
    "id": "training_1048_refute",
    "table_caption": "Table: E2E NER from speech: Micro-Average scores, with and without LM.",
    "table_content": "|| [BOLD] E2E NER | [BOLD] Precision | [BOLD] Recall | [BOLD] F1 ||\n|| without LM | 0.38 | 00.21 | 0.27 ||\n|| with LM | [BOLD] 0.96 | [BOLD] 0.85 | [BOLD] 0.90 ||",
    "claim": "E2E NER, Recall, and F1 scores are higher with a language model, but Precision is higher without it.",
    "label": "refute"
  },
  {
    "id": "training_1201_refute",
    "table_caption": "Table: Accuracies of the proposed method and the state-of-the-art methods for the factoid QA task.",
    "table_content": "|| Name | History | Literature ||\n|| NTEE | [BOLD] 94.7 | [BOLD] 95.1 ||\n|| Fixed NTEE | 90.0 | 93.5 ||\n|| SG-proj | 86.5 | 87.9 ||\n|| SG-proj-dbp | 86.5 | 87.3 ||\n|| BOW | 50.8 | 46.2 ||\n|| BOW-DT | 60.9 | 57.4 ||\n|| QANTA | 65.8 | 63.0 ||\n|| QANTA-full | 73.7 | 69.1 ||\n|| FTS-BRNN | 88.1 | 93.1 ||",
    "claim": "NTEE achieves the highest performance in History but not in Literature.",
    "label": "refute"
  },
  {
    "id": "training_1176_refute",
    "table_caption": "Table: Effect of number of permutations P on S-RAILS+NAWE performance on the development set, for signature length b=1024 and beamwidth B=2,000.",
    "table_content": "|| [ITALIC] b | [BOLD] Median Example  [BOLD] FOM | [BOLD] Median Example  [BOLD] OTWV | [BOLD] Median Example  [BOLD] P@10 | [BOLD] Best Example  [BOLD] FOM | [BOLD] Best Example  [BOLD] OTWV | [BOLD] Best Example  [BOLD] P@10 ||\n|| 128 | 62.1 | 37.4 | 42.1 | 81.7 | 60.8 | 83.8 ||\n|| 256 | 67.2 | 42.6 | 48.6 | 83.0 | 65.4 | 84.9 ||\n|| 512 | 68.2 | 44.8 | 52.6 | 83.6 | 65.9 | 84.9 ||\n|| 1024 | 69.1 | 46.5 | 54.5 | 84.1 | 66.7 | 84.8 ||\n|| 2048 | 70.4 | 48.3 | 54.5 | 85.0 | 66.8 | 86.0 ||",
    "claim": "As the beamwidth increases, the FOM score decreases.",
    "label": "refute"
  },
  {
    "id": "training_1144_refute",
    "table_caption": "Table: Performance comparison with state-of-the-art entity phrase extraction algorithms for the weakly-supervised entity phrase extraction task.",
    "table_content": "|| [BOLD] Methods | [BOLD] NYT ( riedel2013relation,  ) F1 | [BOLD] NYT ( riedel2013relation,  ) Prec | [BOLD] NYT ( riedel2013relation,  ) Rec | [BOLD] Wiki-KBP ( ling2012fine,  ) F1 | [BOLD] Wiki-KBP ( ling2012fine,  ) Prec | [BOLD] Wiki-KBP ( ling2012fine,  ) Rec ||\n|| AutoPhrase ( shang2017automated,  ) | 0.531 | 0.543 | 0.519 | 0.416 | 0.529 | 0.343 ||\n|| Ma & Hovy ( ma2016end,  ) | 0.664 | 0.704 | 0.629 | 0.324 | 0.629 | 0.218 ||\n|| Liu.  [ITALIC] et al. ( 2017arXiv170904109L,  ) | [BOLD] 0.676 | [BOLD] 0.704 | 0.650 | 0.337 | 0.629 | 0.230 ||\n|| ReMine | 0.648 | 0.524 | [BOLD] 0.849 | [BOLD] 0.515 | [BOLD] 0.636 | [BOLD] 0.432 ||",
    "claim": "ReMine does not outperform all other baselines on the Wiki-KBP dataset and has a high recall on the NYT dataset, with F1 scores comparable to the two neural network models.",
    "label": "refute"
  },
  {
    "id": "training_1157_refute",
    "table_caption": "Table: Experiment of our proposed recall mechanism on the MSCOCO Karpathy test split with both cross-entropy loss and CIDEr optimization. We implement our proposed methods: semantic guide (SG), recalled-word slot (RWS) and recalled-word reward (WR) on the baseline model Up-Down. Test results show that our proposed methods have obvious improvement over our baseline. B-1 / B-4 / M / R / C / S refers to BLEU1/ BLEU4 / METEOR / ROUGE-L / CIDEr / SPICE scores.",
    "table_content": "|| Models | Cross-entropy loss B-1 | Cross-entropy loss B-4 | Cross-entropy loss M | Cross-entropy loss R | Cross-entropy loss C | Cross-entropy loss S | CIDEr optimization training B-1 | CIDEr optimization training B-4 | CIDEr optimization training M | CIDEr optimization training R | CIDEr optimization training C | CIDEr optimization training S ||\n|| Test-guide [Mun2016TextguidedAM] | 74.9 | 32.6 | 25.7 | - | 102.4 | - | - | - | - | - | - | - ||\n|| SCST [Rennie_2017_CVPR] | - | 30.0 | 25.9 | 53.4 | 99.4 | - | - | 34.2 | 26.7 | 55.7 | 114.0 | - ||\n|| StackCap [gu2018look] | 76.2 | 35.2 | 26.5 | - | 109.1 | - | 78.5 | 36.1 | 27.4 | - | 120.4 | - ||\n|| CAVP [liu2018context] | - | - | - | - | - | - | - | [BOLD] 38.6 | 28.3 | [BOLD] 58.5 | 126.3 | 21.6 ||\n|| Up-Down [anderson2018bottom] | [BOLD] 77.2 | 36.2 | 27.0 | 56.4 | 113.5 | 20.3 | 79.8 | 36.3 | 27.7 | 56.9 | 120.1 | 21.4 ||\n|| Ours:SG | 77.1 | 36.3 | 27.8 | 56.8 | 115.3 | 21.0 | 80.2 | 38.3 | 28.5 | 58.3 | 127.3 | 22.0 ||\n|| Ours:SG+RWS | 77.1 | 36.6 | 28.0 | 56.9 | 116.9 | 21.3 | 80.3 | 38.3 | 28.5 | 58.3 | 128.3 | 22.2 ||\n|| Ours:SG+RWS+WR | 77.1 | [BOLD] 36.6 | [BOLD] 28.0 | [BOLD] 56.9 | [BOLD] 116.9 | [BOLD] 21.3 | [BOLD] 80.3 | 38.5 | [BOLD] 28.7 | 58.4 | [BOLD] 129.1 | [BOLD] 22.4 ||",
    "claim": "The SG+RWS+WR model achieves the highest scores in BLEU-1, CIDEr, and SPICE metrics with both cross-entropy loss and CIDEr optimization compared to other models.",
    "label": "refute"
  },
  {
    "id": "training_1216_refute",
    "table_caption": "Table: Results of AlgoLisp dataset",
    "table_content": "|| [BOLD] MODEL (%) | [BOLD] Full Testing Set  [BOLD] Acc | [BOLD] Full Testing Set  [BOLD] 50p-Acc | [BOLD] Full Testing Set  [BOLD] M-Acc | [BOLD] Cleaned Testing Set  [BOLD] Acc | [BOLD] Cleaned Testing Set  [BOLD] 50p-Acc | [BOLD] Cleaned Testing Set  [BOLD] M-Acc ||\n|| Seq2Tree | 61.0 | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| LSTM2LSTM+atten | 67.54 | 70.89 | 75.12 | 76.83 | 78.86 | 75.42 ||\n|| TP2LSTM (ours) | 72.28 | 77.62 | 79.92 | 77.67 | 80.51 | 76.75 ||\n|| LSTM2TPR (ours) | 75.31 | 79.26 | 83.05 | 84.44 | 86.13 | 83.43 ||\n|| SAPSpre-VH-Att-256 | 83.80 | 87.45 | [EMPTY] | 92.98 | 94.15 | [EMPTY] ||\n|| [BOLD] TP-N2F (ours) | [BOLD] 84.02 | [BOLD] 88.01 | [BOLD] 93.06 | [BOLD] 93.48 | [BOLD] 94.64 | [BOLD] 92.78 ||",
    "claim": "SAPSpre-VH-Att-256 achieves the highest performance across all metrics on both the full and cleaned testing sets.",
    "label": "refute"
  },
  {
    "id": "training_1174_refute",
    "table_caption": "Table: Results on the Chinese Treebank 5.1 test set.",
    "table_content": "|| Parser charniak2005rerank (R) | LR 80.8 | LP 83.8 | LF 82.3 | Parser petrov2007unlex | LR 81.9 | LP 84.8 | LF 83.3 ||\n|| zhu2013acl (S) | 84.4 | 86.8 | 85.6 | zhang2009tran | 78.6 | 78.0 | 78.3 ||\n|| wang2015feature (S) | [EMPTY] | [EMPTY] | 86.6 | watanabe2015transition | [EMPTY] | [EMPTY] | 84.3 ||\n|| huang2009selftraining (ST) | [EMPTY] | [EMPTY] | 85.2 | dyer2016rnng | [EMPTY] | [EMPTY] | 84.6 ||\n|| dyer2016rnng (R) | [EMPTY] | [EMPTY] | 86.9 | [BOLD] BinarySpan | 85.9 | 87.1 | 86.5 ||\n|| liu2016lookahead | 85.2 | 85.9 | 85.5 | [BOLD] MultiSpan | 86.6 | 88.0 | [BOLD] 87.3 ||\n|| liu2017inorder | [EMPTY] | [EMPTY] | 86.1 | [BOLD] BiaffineRule | 87.1 | 87.5 | [BOLD] 87.3 ||",
    "claim": "The best model improves the labeled F1 score by 0.8 points compared to the in-order transition-based parser (86.1 to 87.3).",
    "label": "refute"
  },
  {
    "id": "training_1073_refute",
    "table_caption": "Table: TempRelPro performances evaluated on the TimeBank-Dense test set and compared with CAEVO.",
    "table_content": "|| [BOLD] System | [BOLD] T-T  [BOLD] P/R/F1 | [BOLD] E-D  [BOLD] P/R/F1 | [BOLD] E-T  [BOLD] P/R/F1 | [BOLD] E-E  [BOLD] P/R/F1 | [BOLD] Overall  [BOLD] P | [BOLD] Overall  [BOLD] R | [BOLD] Overall  [BOLD] F1 ||\n|| [BOLD] TempRelPro | [BOLD] 0.780 | 0.518 | [BOLD] 0.556 | 0.487 | [BOLD] 0.512 | [BOLD] 0.510 | [BOLD] 0.511 ||\n|| CAEVO | 0.712 | [BOLD] 0.553 | 0.494 | [BOLD] 0.494 | 0.508 | 0.506 | 0.507 ||",
    "claim": "TempRelPro achieves a higher overall F1-score than CAEVO (51.1% vs 50.7%) and performs better in labeling T-T and E-D pairs.",
    "label": "refute"
  },
  {
    "id": "training_1223_refute",
    "table_caption": "Table: Results of retraining Infersent on both SNLI and the Comparisons dataset.",
    "table_content": "|| Epoch | Performance (%) Train(Combined) | Performance (%) Test(Comp) | Performance (%) Test(SNLI) ||\n|| 0 | 33.33 | 33.33 | 33.33 ||\n|| 12 | 90.99 | 100.00 | 84.96 ||",
    "claim": "At epoch 12, the test accuracy on the Comparisons dataset is 100.00%, and the test accuracy on the SNLI dataset is 90.00%.",
    "label": "refute"
  },
  {
    "id": "training_1094_refute",
    "table_caption": "Table: Effect of the ratio value γsrc and γtrg on Chinese-English Translation.",
    "table_content": "|| [13mm] [ITALIC] γsrcγtrg | 0.00 | 0.25 | 0.50 | 0.75 ||\n|| 0.00 | 44.59 | 46.19 | 46.26 | 46.14 ||\n|| 0.25 | 45.23 | 46.72 | [BOLD] 46.95 | 46.52 ||\n|| 0.50 | 44.25 | 45.34 | 45.39 | 45.94 ||\n|| 0.75 | 44.18 | 44.98 | 45.35 | 45.37 ||",
    "claim": "The best performance is achieved at γsrc=0.50 and γtrg=0.75, and non-zero γtrg values improve performance compared to γtrg=0.",
    "label": "refute"
  },
  {
    "id": "training_1145_refute",
    "table_caption": "Table: Performance comparison with state-of-the-art Open IE systems on two datasets from different domains, using Precision@K, Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR).",
    "table_content": "|| [BOLD] Methods | [BOLD] NYT ( riedel2013relation,  ) P@100 | [BOLD] NYT ( riedel2013relation,  ) P@200 | [BOLD] NYT ( riedel2013relation,  ) MAP | [BOLD] NYT ( riedel2013relation,  ) NDCG@100 | [BOLD] NYT ( riedel2013relation,  ) NDCG@200 | [BOLD] NYT ( riedel2013relation,  ) MRR | [BOLD] Twitter ( zhang2016geoburst,  ) P@100 | [BOLD] Twitter ( zhang2016geoburst,  ) P@200 | [BOLD] Twitter ( zhang2016geoburst,  ) MAP | [BOLD] Twitter ( zhang2016geoburst,  ) NDCG@100 | [BOLD] Twitter ( zhang2016geoburst,  ) NDCG@200 | [BOLD] Twitter ( zhang2016geoburst,  ) MRR ||\n|| ClausIE | 0.580 | 0.625 | 0.623 | 0.575 | 0.667 | 0.019 | 0.300 | 0.305 | 0.308 | 0.332 | 0.545 | 0.021 ||\n|| Stanford | 0.680 | 0.625 | 0.665 | 0.689 | 0.654 | 0.023 | 0.390 | 0.410 | 0.415 | 0.413 | 0.557 | 0.023 ||\n|| OLLIE | 0.670 | 0.640 | 0.683 | 0.684 | 0.775 | [BOLD] 0.028 | 0.580 | 0.510 | 0.525 | 0.519 | 0.626 | 0.017 ||\n|| MinIE | 0.680 | 0.645 | 0.687 | 0.724 | 0.723 | 0.027 | 0.350 | 0.340 | 0.361 | 0.362 | 0.541 | [BOLD] 0.025 ||\n|| ReMine-G | 0.730 | 0.695 | 0.734 | 0.751 | 0.783 | 0.027 | 0.510 | 0.580 | 0.561 | 0.522 | 0.610 | 0.021 ||\n|| ReMine | [BOLD] 0.780 | [BOLD] 0.720 | [BOLD] 0.760 | [BOLD] 0.787 | [BOLD] 0.791 | 0.027 | [BOLD] 0.610 | [BOLD] 0.610 | [BOLD] 0.627 | [BOLD] 0.615 | [BOLD] 0.651 | 0.022 ||",
    "claim": "ReMine achieves the highest performance on the NYT dataset across all rank-based measures, but not on the Twitter dataset.",
    "label": "refute"
  },
  {
    "id": "training_1075_refute",
    "table_caption": "Table: Classifier performances (F1-scores) in different experimental settings S1 and S2, compared with using only traditional features. TP: true positives and FP: false positives.",
    "table_content": "|| [EMPTY] | [BOLD] Feature vector Traditional features | [BOLD] Feature vector → [ITALIC] f | [BOLD] Total 5271 | [BOLD] TP 2717 | [BOLD] FP 2554 | [BOLD] F1  [BOLD] 0.5155 ||\n|| [BOLD] S1 | GloVe | (→ [ITALIC] w1⊕→ [ITALIC] w2) | 5271 | 2388 | 2883 | 0.4530 ||\n|| [EMPTY] | [EMPTY] | (→ [ITALIC] w1+→ [ITALIC] w2) | 5271 | 2131 | 3140 | 0.4043 ||\n|| [EMPTY] | [EMPTY] | (→ [ITALIC] w1−→ [ITALIC] w2) | 5271 | 2070 | 3201 | 0.3927 ||\n|| [EMPTY] | Word2Vec | (→ [ITALIC] w1⊕→ [ITALIC] w2) | 5271 | 2609 | 2662 | [BOLD] 0.4950 ||\n|| [EMPTY] | [EMPTY] | (→ [ITALIC] w1+→ [ITALIC] w2) | 5271 | 2266 | 3005 | 0.4299 ||\n|| [EMPTY] | [EMPTY] | (→ [ITALIC] w1−→ [ITALIC] w2) | 5271 | 2258 | 3013 | 0.4284 ||\n|| [BOLD] S2 | Word2Vec | ((→ [ITALIC] w1⊕→ [ITALIC] w2)⊕→ [ITALIC] f) | 5271 | 3036 | 2235 | [BOLD] 0.5760 ||\n|| [EMPTY] | [EMPTY] | ((→ [ITALIC] w1+→ [ITALIC] w2)⊕→ [ITALIC] f) | 5271 | 2901 | 2370 | 0.5504 ||\n|| [EMPTY] | [EMPTY] | ((→ [ITALIC] w1−→ [ITALIC] w2)⊕→ [ITALIC] f) | 5271 | 2887 | 2384 | 0.5477 ||",
    "claim": "The classifier in setting S1 with GloVe (→ w1⊕→ w2) achieves the highest F1 score of 0.5760 compared to other settings and traditional features.",
    "label": "refute"
  },
  {
    "id": "training_1260_refute",
    "table_caption": "Table: Average Web Treebank development UAS at different threshold settings.",
    "table_content": "|| Baseline | [ITALIC]  [BOLD] t [BOLD] 1 | [ITALIC]  [BOLD] t [BOLD] 3 | [ITALIC]  [BOLD] t [BOLD] 5 | [ITALIC]  [BOLD] t [BOLD] ∞ ||\n|| 84.11 | 84.89 | 84.97 | 84.81 | 84.14 ||",
    "claim": "t5 performs best, with t1 and t3 also improving over the baseline.",
    "label": "refute"
  },
  {
    "id": "training_1210_refute",
    "table_caption": "Table: Analysis network parameter for DCN",
    "table_content": "|| [BOLD] Models | VQA1.0 Open-Ended (test-dev)  [BOLD] All | VQA1.0 Open-Ended (test-dev)  [BOLD] Yes/No | VQA1.0 Open-Ended (test-dev)  [BOLD] Number | VQA1.0 Open-Ended (test-dev)  [BOLD] others | HAT val dataset  [BOLD] Rank-correlation ||\n|| LSTM Q+I+ Attention(LQIA) | 56.1 | 80.3 | 37.4 | 40.46 | 0.2142 ||\n|| DCN Add_v1(K=4)+(LQIA) | 60.4 | 81.0 | 37.5 | 47.1 | 0.3202 ||\n|| DCN Add_v2(K=4)+(LQIA) | 60.4 | 81.2 | 37.2 | 47.3 | 0.3215 ||\n|| DCN Mul_v1(K=4)+(LQIA) | 60.6 | 80.9 | [BOLD] 37.8 | 47.9 | 0.3229 ||\n|| DCN Mul_v2(K=4)+(LQIA) | [BOLD] 60.9 | [BOLD] 81.3 | 37.5 | [BOLD] 48.2 | [BOLD] 0.3242 ||\n|| DCN Add_v1(K=4)+MCB | 65.1 | 83.1 | 38.5 | 54.5 | 0.3359 ||\n|| DCN Add_v2(K=4)+MCB | 65.2 | 83.4 | 39.0 | 54.6 | 0.3376 ||\n|| DCN Mul_v1(K=4)+MCB | 65.2 | [BOLD] 83.9 | 38.7 | 54.9 | 0.3365 ||\n|| DCN Mul_v2(K=4)+MCB | [BOLD] 65.4 | 83.8 | [BOLD] 39.1 | [BOLD] 55.2 | [BOLD] 0.3389 ||",
    "claim": "DCN Mul_v2(K=4)+MCB achieves the highest accuracy on the VQA1.0 Open-Ended (test-dev) Yes/No category and the highest rank-correlation on the HAT val dataset.",
    "label": "refute"
  },
  {
    "id": "training_1240_refute",
    "table_caption": "Table: Accuracy on CMRC-2017 dataset. Results marked with † are from the latest official CMRC-2017 Leaderboard 777http://www.hfl-tek.com/cmrc2017/leaderboard.html. The best results are in bold face.",
    "table_content": "|| Model | CMRC-2017 Valid | CMRC-2017 Test ||\n|| Random Guess † | 1.65 | 1.67 ||\n|| Top Frequency † | 14.85 | 14.07 ||\n|| AS Reader † | 69.75 | 71.23 ||\n|| GA Reader | 72.90 | 74.10 ||\n|| SJTU BCMI-NLP † | 76.15 | 77.73 ||\n|| 6ESTATES PTE LTD † | 75.85 | 74.73 ||\n|| Xinktech † | 77.15 | 77.53 ||\n|| Ludong University † | 74.75 | 75.07 ||\n|| ECNU † | 77.95 | 77.40 ||\n|| WHU † | 78.20 | 76.53 ||\n|| SAW Reader | [BOLD] 78.95 | [BOLD] 78.80 ||",
    "claim": "SAW Reader achieves higher scores than WHU on the CMRC-2017 Valid set, but WHU scores higher on the Test set.",
    "label": "refute"
  },
  {
    "id": "training_1228_refute",
    "table_caption": "Table: Sentiment analysis results on RNN model pre-trained with aligned Uzbek word embeddings. Best results are highlighted in bold. *FastText scores shown in the table are directly taken from the original paper.",
    "table_content": "|| [BOLD] Dataset | [BOLD] FastText Score* | [BOLD] Our Score ||\n|| Manual Dataset | 0.8782 | [BOLD] 0.8825 ||\n|| Translated Dataset | 0.8832 | [BOLD] 0.8876 ||",
    "claim": "The RNN model with aligned word embeddings outperforms the model with fastText word embeddings on the Manual dataset, while the fastText model performs better on the Translated dataset.",
    "label": "refute"
  },
  {
    "id": "training_1164_refute",
    "table_caption": "Table: Comparisons on the Amazon electronics dataset. Scores are as reported by riejohnson.",
    "table_content": "|| [BOLD] Method | [BOLD] Acc. ||\n|| SVM-unigrams | 88.62 ||\n|| SVM-{1,2}-grams | 90.70 ||\n|| SVM-{1,2,3}-grams | 90.68 ||\n|| NN-unigrams | 88.94 ||\n|| NN-{1,2}-grams | 91.10 ||\n|| NN-{1,2,3}-grams | 91.24 ||\n|| [BOLD] LR (this work) | 91.56 ||\n|| Bag of words CNN | 91.58 ||\n|| Sequential CNN | 92.22 ||",
    "claim": "The LR method outperforms all reported methods, including the Sequential CNN and SVM variants.",
    "label": "refute"
  },
  {
    "id": "training_1254_refute",
    "table_caption": "Table: Results of various coreference resolvers on successfully identifying inter-sentence points of correspondence (PoC) and recall scores of these resolvers split by PoC correspondence type.",
    "table_content": "|| [BOLD] Coref Resolver | [BOLD] P(%) | [BOLD] R(%) | [BOLD] F(%) | [BOLD] Pronominal | [BOLD] Nominal | [BOLD] Comm.-Noun | [BOLD] Repetition | [BOLD] Event Trig. ||\n|| SpaCy | [BOLD] 59.2 | 20.1 | 30.0 | 30.8 | 23.3 | 10.4 | 39.9 | 2.6 ||\n|| AllenNLP | 49.0 | 24.5 | 32.7 | 36.5 | 28.1 | 14.7 | 47.1 | 3.1 ||\n|| Stanford CoreNLP | 54.2 | [BOLD] 26.2 | [BOLD] 35.3 | 40.0 | 27.3 | 17.4 | 55.1 | 2.3 ||",
    "claim": "The AllenNLP resolver has the highest precision (59.2%), and Stanford CoreNLP achieves the highest F1-score (35.3%).",
    "label": "refute"
  },
  {
    "id": "training_1172_refute",
    "table_caption": "Table: Span representation methods.",
    "table_content": "|| Model | English LP | English LR | English LF | Chinese LP | Chinese LR | Chinese LF ||\n|| BinarySpan | 92.16 | 92.19 | 92.17 | 91.31 | 90.48 | 90.89 ||\n|| MultiSpan | 92.47 | [BOLD] 92.41 | [BOLD] 92.44 | [BOLD] 91.69 | 90.91 | [BOLD] 91.30 ||\n|| LinearRule | 92.03 | 92.03 | 92.03 | 91.03 | 89.19 | 90.10 ||\n|| BiaffineRule | [BOLD] 92.49 | 92.23 | 92.36 | 91.31 | [BOLD] 91.28 | 91.29 ||",
    "claim": "BinarySpan achieves higher LF scores than MultiSpan for the English dataset, and BiaffineRule and MultiSpan have similar LF scores for both languages.",
    "label": "refute"
  },
  {
    "id": "training_1095_refute",
    "table_caption": "Table: Comparison with the state-of-the-art systems proposed by [16] on emotion dataset. The metrics P, R and F stand for Precision, Recall and F1-Score.",
    "table_content": "|| [BOLD] Models | Metric | [BOLD] Emotion Anger | [BOLD] Emotion Anticipation | [BOLD] Emotion Disgust | [BOLD] Emotion Fear | [BOLD] Emotion Joy | [BOLD] Emotion Sadness | [BOLD] Emotion Surprise | [BOLD] Emotion Trust | [BOLD] Emotion Micro-Avg ||\n|| MaxEnt | P | 76 | 72 | 62 | 57 | 55 | 65 | 62 | 62 | 66 ||\n|| MaxEnt | R | 72 | 61 | 47 | 31 | 50 | 65 | 15 | 38 | 52 ||\n|| MaxEnt | F | 74 | 66 | 54 | 40 | 52 | 65 | 24 | 47 | 58 ||\n|| SVM | P | 76 | 70 | 59 | 55 | 52 | 64 | 46 | 57 | 63 ||\n|| SVM | R | 69 | 60 | 53 | 40 | 52 | 60 | 22 | 45 | 53 ||\n|| SVM | F | 72 | 64 | 56 | 46 | 52 | 62 | 30 | 50 | 58 ||\n|| LSTM | P | 76 | 68 | 64 | 51 | 56 | 60 | 40 | 57 | 62 ||\n|| LSTM | R | 77 | 68 | 68 | 48 | 41 | 77 | 17 | 49 | 60 ||\n|| LSTM | F | 76 | 67 | 65 | 49 | 46 | 67 | 21 | 51 | 61 ||\n|| BiLSTM | P | 77 | 70 | 61 | 58 | 54 | 62 | 42 | 59 | 64 ||\n|| BiLSTM | R | 77 | 66 | 64 | 43 | 59 | 72 | 20 | 44 | 60 ||\n|| BiLSTM | F | 77 | [BOLD] 68 | 63 | 49 | 56 | 67 | 27 | 50 | 62 ||\n|| CNN | P | 77 | 68 | 62 | 53 | 54 | 63 | 36 | 53 | 62 ||\n|| CNN | R | 77 | 60 | 61 | 46 | 56 | 72 | 24 | 49 | 59 ||\n|| CNN | F | 77 | 64 | 62 | 49 | 55 | 67 | [BOLD] 28 | 50 | 60 ||\n|| E2 (proposed) | P | 81 | 74 | 70 | 66 | 64 | 67 | 68 | 68 | 71 ||\n|| E2 (proposed) | R | 83 | 62 | 74 | 42 | 59 | 81 | 13 | 49 | 63 ||\n|| E2 (proposed) | F | [BOLD] 82 | [BOLD] 68 | [BOLD] 72 | [BOLD] 51 | [BOLD] 62 | [BOLD] 73 | 22 | [BOLD] 57 | [BOLD] 67 ||",
    "claim": "The proposed system (E2) achieves higher F-scores for all emotions except anticipation compared to other models.",
    "label": "refute"
  },
  {
    "id": "training_1064_refute",
    "table_caption": "Table: Model comparison between our methods and baseline methods. ACC denotes accuracy (all ACC metrics in the table are percentage numbers with % omitted). Specially for MNLI, we average the results of matched and mismatched validation set.",
    "table_content": "|| [BOLD] Model | [BOLD] Model | [BOLD] Performance (ACC)  [BOLD] DeepQA | [BOLD] Performance (ACC)  [BOLD] MNLI | [BOLD] Performance (ACC)  [BOLD] SNLI | [BOLD] Performance (ACC)  [BOLD] QNLI | [BOLD] Performance (ACC)  [BOLD] RTE | [BOLD] Inference Speed(QPS) | [BOLD] Parameters (M) ||\n|| [BOLD] Original Model | [BOLD] BERT-3 | 75.78 | 70.77 | 77.75 | 78.51 | 57.42 | 207 | 50.44 ||\n|| [BOLD] Original Model | [BOLD] BERTlarge | 81.47 | 79.10 | 80.90 | 90.30 | 68.23 | 16 | 333.58 ||\n|| [BOLD] Original Model | [BOLD] BERTlarge ensemble | 81.66 | 79.57 | 81.39 | 90.91 | 70.75 | 16/3 | 333.58*3 ||\n|| [BOLD] Traditional Distillation Model | [BOLD] Bi-LSTM (1-o-1) | 71.69 | 59.39 | 69.59 | 69.12 | 56.31 | 207 | 50.44 ||\n|| [BOLD] Traditional Distillation Model | [BOLD] Bi-LSTM (1avg-o-1) | 71.93 | 59.60 | 70.04 | 69.53 | 57.35 | 207 | 50.44 ||\n|| [BOLD] Traditional Distillation Model | [BOLD] Bi-LSTM (m-o-m) | 72.04 | 61.71 | 72.89 | 69.89 | 58.12 | 207/3 | 50.44*3 ||\n|| [BOLD] Traditional Distillation Model | [BOLD] BERT-3 (1-o-1) | 77.35 | 71.07 | 78.62 | 77.65 | 55.23 | 217 | 45.69 ||\n|| [BOLD] Traditional Distillation Model | [BOLD] BERT-3 (1avg-o-1) | 77.63 | 70.63 | 78.64 | 78.20 | 58.12 | 217 | 45.69 ||\n|| [BOLD] Traditional Distillation Model | [BOLD] BERT-3 (m-o-m) | 77.44 | 71.28 | 78.71 | 77.90 | 57.40 | 217/3 | 45.69*3 ||\n|| [BOLD] Our Distillation Model | [BOLD] Bi-LSTM (TMKDbase) | 74.73 | 61.68 | 71.71 | 69.99 | 62.74 | 207 | 50.45 ||\n|| [BOLD] Our Distillation Model | ∗ [BOLD] TMKDbase | 79.93 | 71.29 | 78.35 | 83.53 | 66.64 | 217 | 45.70 ||\n|| [BOLD] Our Distillation Model | ∗ [BOLD] TMKDlarge | [BOLD] 80.43 | [BOLD] 73.93 | [BOLD] 79.48 | [BOLD] 86.44 | [BOLD] 67.50 | [BOLD] 217 | [BOLD] 45.70 ||",
    "claim": "TMKDlarge achieves higher accuracy than BERT-3 on only some tasks.",
    "label": "refute"
  },
  {
    "id": "training_1224_refute",
    "table_caption": "Table: Percentage of entailments split by overlap rate of words in SNLI.",
    "table_content": "|| Top | Entailment | Neutral | Contradiction ||\n|| All | 33.4 | 33.3 | 33.3 ||\n|| 10000 | 39.5 | 35.7 | 24.8 ||\n|| 1000 | 50.8 | 40.7 | 8.5 ||",
    "claim": "As the word overlap in the sentences increases, the percentage of contradictions rises.",
    "label": "refute"
  },
  {
    "id": "training_1171_refute",
    "table_caption": "Table: Results for the attribution span labeler.",
    "table_content": "|| Partial | GS + no EP | Precision 79.40 | Recall 79.96 | [ITALIC] F1 79.68 ||\n|| Partial | GS + EP | 65.93 | 79.96 | 72.27 ||\n|| Partial | Auto + EP | 64.40 | 51.68 | 57.34 ||\n|| Exact | GS + no EP | 65.72 | 66.19 | 65.95 ||\n|| Exact | GS + EP | 54.57 | 66.19 | 59.82 ||\n|| Exact | Auto + EP | 47.83 | 38.39 | 42.59 ||",
    "claim": "When error propagation is introduced, the F1 score degradation is largely due to a drop in precision, while recall changes significantly. When full automation is added, the degradation is largely due to a drop in recall.",
    "label": "refute"
  },
  {
    "id": "training_1213_refute",
    "table_caption": "Table: Combination of 3 tags of each category for hadamard mixture model namely addition, concatenation, multiplication and 1d-convolution",
    "table_content": "|| [BOLD] Context | [BOLD] BLEU-1 | [BOLD] Meteor | [BOLD] Rouge | [BOLD] CIDer ||\n|| Tag-n3-add | 22.4 | 9.1 | 22.2 | 26.7 ||\n|| Tag-n3-con | [BOLD] 24.8 | 10.6 | 24.4 | [BOLD] 53.2 ||\n|| Tag-n3-joint | 22.1 | 8.9 | 21.7 | 24.6 ||\n|| Tag-n3-conv | 24.1 | 10.3 | 24.0 | 47.9 ||\n|| Tag-v3-add | 24.1 | 10.2 | 23.9 | 46.7 ||\n|| Tag-v3-con | 24.5 | 10.7 | 24.2 | [BOLD] 52.3 ||\n|| Tag-v3-joint | 22.5 | 9.1 | 22.1 | 25.6 ||\n|| Tag-v3-conv | 23.2 | 9.0 | 24.2 | 38.0 ||\n|| Tag-q3-add | 24.5 | 10.5 | 24.4 | 51.4 ||\n|| Tag-q3-con | 24.6 | [BOLD] 10.8 | 24.3 | [BOLD] 55.0 ||\n|| Tag-q3-joint | 22.1 | 9.0 | 22.0 | 25.9 ||\n|| Tag-q3-conv | 24.3 | 10.4 | 24.0 | 48.6 ||",
    "claim": "The Tag-q3-con model achieves the highest BLEU-1 and METEOR scores, while the Tag-n3-con model achieves the highest CIDEr score among all models.",
    "label": "refute"
  },
  {
    "id": "training_1236_refute",
    "table_caption": "Table: The means (over all 25 STS competition datasets) of the absolute differences in Pearson’s r between each pair of models.",
    "table_content": "|| Models | Mean Pearson Abs. Diff. ||\n|| Word / Trigram | 2.75 ||\n|| Word / LSTM | 2.17 ||\n|| Trigram / LSTM | 2.89 ||",
    "claim": "The Word/Trigram pair shows the largest mean Pearson absolute difference compared to the other model pairs.",
    "label": "refute"
  },
  {
    "id": "training_1156_refute",
    "table_caption": "Table: MAP performance on the three BMASS relations with ≥100 unigram analogies. Uni is using unigram embeddings on unigram data, UniM is using MWE embeddings on unigram data, and MWE is performance with MWE embeddings over the full MWE data.",
    "table_content": "|| Rel | PM-2 Uni | PM-2 Uni [ITALIC] M | PM-2 MWE | CBOW Uni | CBOW Uni [ITALIC] M | CBOW MWE ||\n|| L2 | 0.07 | 0.10 | 0.07 | 0.11 | 0.14 | 0.06 ||\n|| L3 | 0.14 | 0.19 | 0.06 | 0.12 | 0.16 | 0.06 ||\n|| L4 | 0.01 | 0.00 | 0.02 | 0.04 | 0.05 | 0.07 ||",
    "claim": "Unigram analogies are worse captured than MWE data for has-lab-number (L2) and better for has-tradename (L3), but perform worse on tradename-of (L4).",
    "label": "refute"
  },
  {
    "id": "training_1184_refute",
    "table_caption": "Table: Performance of Ensemble system on Hinglish and Spanglish test datasets",
    "table_content": "|| [EMPTY] | [BOLD] F1  [BOLD] o | [BOLD] F1  [BOLD] + | [BOLD] F1  [BOLD] - | [BOLD] F1  [BOLD] Macro | [BOLD] Macro  [BOLD] Precision | [BOLD] Macro  [BOLD] Recall ||\n|| [BOLD] Hinglish | 0.640 | 0.762 | 0.729 | 0.707 | 0.712 | 0.705 ||\n|| [BOLD] Spanglish | 0.135 | 0.825 | 0.375 | 0.725 | 0.763 | 0.696 ||",
    "claim": "The ensemble achieves an F1-score of 0.707 and a recall of 0.705 on the Hinglish dataset, and an F1-score of 0.725 and a recall of 0.705 on the Spanglish dataset.",
    "label": "refute"
  },
  {
    "id": "training_1241_refute",
    "table_caption": "Table: Case study on CMRC-2017.",
    "table_content": "|| Model | Operation | CMRC-2017 Valid | CMRC-2017 Test ||\n|| [EMPTY] | concat | 74.80 | 75.13 ||\n|| Word + Char | sum | 75.40 | 75.53 ||\n|| [EMPTY] | mul | 77.80 | 77.93 ||\n|| [EMPTY] | concat | 75.95 | 76.43 ||\n|| Word + BPE | sum | 76.20 | 75.83 ||\n|| [EMPTY] | mul | [BOLD] 78.95 | [BOLD] 78.80 ||",
    "claim": "The \"concat\" operation outperforms \"mul\" and \"sum\" operations on the CMRC-2017 Valid and Test sets.",
    "label": "refute"
  },
  {
    "id": "training_1230_refute",
    "table_caption": "Table: Performance of systems on FEVER. “F1” indicates the sentence-level evidence F1 score. “LA” indicates Label Acc. without considering the evidence prediction. “FS”=FEVER Score Thorne et al. (2018)",
    "table_content": "|| Model | F1 | LA | FS ||\n|| Hanselowski Hanselowski et al. ( 2018 ) | - | 68.49 | 64.74 ||\n|| Yoneda Yoneda et al. ( 2018 ) | 35.84 | 69.66 | 65.41 ||\n|| Nie Nie et al. ( 2019 ) | 51.37 | 69.64 | 66.15 ||\n|| Full system (single) | [BOLD] 76.87 | [BOLD] 75.12 | [BOLD] 70.18 ||\n|| [ITALIC]  [BOLD] Dev set | [ITALIC]  [BOLD] Dev set | [ITALIC]  [BOLD] Dev set | [ITALIC]  [BOLD] Dev set ||\n|| Hanselowski Hanselowski et al. ( 2018 ) | 37.33 | 65.22 | 61.32 ||\n|| Yoneda Yoneda et al. ( 2018 ) | 35.21 | 67.44 | 62.34 ||\n|| Nie Nie et al. ( 2019 ) | 52.81 | 68.16 | 64.23 ||\n|| Full system (single) | [BOLD] 74.62 | [BOLD] 72.56 | [BOLD] 67.26 ||\n|| [ITALIC]  [BOLD] Test set | [ITALIC]  [BOLD] Test set | [ITALIC]  [BOLD] Test set | [ITALIC]  [BOLD] Test set ||",
    "claim": "The Full system achieves the highest scores on Label Accuracy and FEVER Score, but not on F1, compared to other models on both the Dev and Test sets.",
    "label": "refute"
  },
  {
    "id": "training_1271_refute",
    "table_caption": "Table: The state of the art performance on different datasets. We also compare with existing word segmentation toolkits Jieba and CKIP Hsieh et al. (2012).",
    "table_content": "|| [EMPTY] | AS | CITYU | MSR | PKU | UD ||\n|| CKIP | 97.7 | 94.3 | 92.0 | 93.9 | 91.2 ||\n|| Jieba | 87.1 | 86.8 | 86.5 | 87.6 | 87.6 ||\n|| DBLP:journals/corr/LiuCGQL16 | — | — | 97.3 | 96.8 | [EMPTY] ||\n|| yang2017neural | 95.7 | 96.9 | 97.5 | 96.3 | — ||\n|| zhou2017word | — | — | 97.8 | 96.0 | — ||\n|| cai2017fast | — | 95.6 | 97.1 | 95.8 | — ||\n|| chen2017adversarial | 94.6 | 95.6 | 96.0 | 94.3 | — ||\n|| K17-3015 | — | — | — | — | 94.6 ||\n|| DBLP:journals/corr/abs-1711-04411 | — | — | 98.0 | 96.5 | — ||\n|| ma2018 | 96.2 | 97.2 | 98.1 | 96.1 | 96.9 ||\n|| Ours (baseline model) | 97.5 | 97.1 | 98.4 | 96.5 | 95.7 ||\n|| Ours (generally pretrained ELMo) | 97.5 | 97.8 | 98.1 | 96.8 | 97.0 ||\n|| Ours (character level ELMo) | [BOLD] 98.0 | [BOLD] 98.6 | [BOLD] 98.7 | [BOLD] 97.7 | [BOLD] 98.3 ||",
    "claim": "We achieve the best results on all datasets except for the AS dataset.",
    "label": "refute"
  },
  {
    "id": "training_1204_refute",
    "table_caption": "Table: Performance of P2T-Tran",
    "table_content": "|| Direction | Category | Basic Units | Text Space Dev | Text Space Test ||\n|| Zh-En | P2T-Tran | Zh: Pinyin, En: subword | 41.93 | 42.51 ||\n|| Zh-En | P2T-Tran | Zh: subword, En: subword | 42.59 | 42.75 ||\n|| Zh-En | T2T-Tran | Zh: subword, En: subword | 43.16 | \\mathbf{43.46} ||\n|| En-Zh | P2T-Tran | En: syllable, Zh: subword | 35.12 | 36.12 ||\n|| En-Zh | P2T-Tran | En: subword, Zh: subword | 35.25 | 36.04 ||\n|| En-Zh | T2T-Tran | En: subword, Zh: subword | 36.26 | \\mathbf{37.13} ||",
    "claim": "The performance difference between P2T-Tran-Syllable and P2T-Tran-Subword on the test set is quite large.",
    "label": "refute"
  },
  {
    "id": "training_1225_refute",
    "table_caption": "Table: Zero-shot reasoning: Performance on previously unobserved Xs and Zs.",
    "table_content": "|| Test set | InferSent (%) | augmented-InferSent (%) ||\n|| Held-out nouns | 47.9 | 82.0 ||\n|| Made up words | 48.0 | 83.2 ||\n|| Long noun phrases | 49.1 | 84.9 ||",
    "claim": "Augmented-InferSent achieves higher accuracy than InferSent across most test sets, but InferSent performs better on long noun phrases.",
    "label": "refute"
  },
  {
    "id": "training_1266_refute",
    "table_caption": "Table: Illustrative sentence pairs from the STS datasets showing errors made by LSTMavg and avg. The last three columns show the gold similarity score, the similarity score of LSTMavg, and the similarity score of avg. Boldface indicates smaller error compared to gold scores.",
    "table_content": "|| # | Sentence 1 | Sentence 2 | Lavg | avg | Gold ||\n|| 1 | the lamb is looking at the camera. | a cat looking at the camera. | [BOLD] 3.42 | 4.13 | 0.8 ||\n|| 2 | he also said shockey is “living the dream life of a new york athlete. | “jeremy’s a good guy,” barber said, adding:“jeremy is living the dream life of the new york athlete. | [BOLD] 3.55 | 4.22 | 2.75 ||\n|| 3 | bloomberg chips in a billion | bloomberg gives $1.1 b to university | [BOLD] 3.99 | 3.04 | 4.0 ||\n|| 4 | in other regions, the sharia is imposed. | in other areas, sharia law is being introduced by force. | [BOLD] 4.44 | 3.72 | 4.75 ||\n|| 5 | three men in suits sitting at a table. | two women in the kitchen looking at a object. | 3.33 | [BOLD] 2.79 | 0.0 ||\n|| 6 | we never got out of it in the first place! | where does the money come from in the first place? | 4.00 | [BOLD] 3.33 | 0.8 ||\n|| 7 | two birds interacting in the grass. | two dogs play with each other outdoors. | 3.44 | [BOLD] 2.81 | 0.2 ||",
    "claim": "Avg overestimates similarity in examples 3 and 4 due to one-word-to-multiword paraphrases.",
    "label": "refute"
  },
  {
    "id": "training_1175_refute",
    "table_caption": "Table: Comparison of QbE system performance on the evaluation set.",
    "table_content": "|| System | [BOLD] Median Example  [BOLD] FOM | [BOLD] Median Example  [BOLD] OTWV | [BOLD] Median Example  [BOLD] P@10 | [BOLD] Best Example  [BOLD] FOM | [BOLD] Best Example  [BOLD] OTWV | [BOLD] Best Example  [BOLD] P@10 | [BOLD] Query time (s) ||\n|| RAILS  | 6.7 | 2.7 | 44.0 | 20.7 | 10.4 | 84.4 | 24.7 ||\n|| S-RAILS (baseline) | 24.5 | 14.4 | 34.5 | 46.2 | 26.6 | 87.4 | 0.078 ||\n|| S-RAILS+NAWE (ours) | 43.3 | 22.4 | 60.2 | 65.4 | 43.3 | 95.1 | 0.38 ||",
    "claim": "S-RAILS+NAWE shows improvements over S-RAILS in all performance metrics, with a relative improvement of more than 70% in Median Example results and over 50% in Best Example FOM and OTWV.",
    "label": "refute"
  },
  {
    "id": "training_1242_refute",
    "table_caption": "Table: Accuracy on PD and CFT datasets. Results of AS Reader and CAS Reader are from [Cui et al.2016].",
    "table_content": "|| Model | PD Valid | PD Test | CFT Test-human ||\n|| AS Reader | 64.1 | 67.2 | 33.1 ||\n|| GA Reader | 67.2 | 69.0 | 36.9 ||\n|| CAS Reader | 65.2 | 68.1 | 35.0 ||\n|| SAW Reader | [BOLD] 72.8 | [BOLD] 75.1 | [BOLD] 43.8 ||",
    "claim": "SAW Reader outperforms CAS Reader in PD Valid and PD Test, but CAS Reader outperforms SAW Reader in CFT Test-human.",
    "label": "refute"
  },
  {
    "id": "training_1180_refute",
    "table_caption": "Table: Results of the monolingual experiments (in percentage) in terms of accuracy (A), precision (P), recall (R), and macro F-score (F).",
    "table_content": "|| Arabic | Arabic A | Arabic P | Arabic R | Arabic F | French A | French P | French R | French F | English A | English P | English R | English F ||\n|| RF | 68.0 | 67.0 | 82.0 | 68.0 | 68.5 | 71.7 | 87.3 | 61.0 | 61.2 | 60.0 | 70.0 | 61.0 ||\n|| CNN | [BOLD] 80.5 | 79.1 | 84.9 | [BOLD] 80.4 | [BOLD] 77.6 | 68.2 | 59.6 | [BOLD] 73.5 | [BOLD] 77.9 | 74.6 | 84.7 | [BOLD] 77.8 ||",
    "claim": "CNN achieves the highest F-score for French and English, and the highest accuracy for French.",
    "label": "refute"
  },
  {
    "id": "training_1268_refute",
    "table_caption": "Table: Results on SemEval textual similarity datasets (Pearson’s r×100). The highest score in each row is in boldface.",
    "table_content": "|| Dataset | LSTMavg | avg | GRAN ||\n|| MSRpar | [BOLD] 49.0 | 45.9 | 47.7 ||\n|| MSRvid | 84.3 | 85.1 | [BOLD] 85.2 ||\n|| SMT-eur | [BOLD] 51.2 | 47.5 | 49.3 ||\n|| OnWN | [BOLD] 71.5 | 71.2 | 71.5 ||\n|| SMT-news | [BOLD] 68.0 | 58.2 | 58.7 ||\n|| STS 2012 Average | [BOLD] 64.8 | 61.6 | 62.5 ||\n|| headline | [BOLD] 77.3 | 76.9 | 76.1 ||\n|| OnWN | 81.2 | 72.8 | [BOLD] 81.4 ||\n|| FNWN | 53.2 | 50.2 | [BOLD] 55.6 ||\n|| SMT | [BOLD] 40.7 | 38.0 | 40.3 ||\n|| STS 2013 Average | 63.1 | 59.4 | [BOLD] 63.4 ||\n|| deft forum | [BOLD] 56.6 | 55.6 | 55.7 ||\n|| deft news | 78.0 | [BOLD] 78.5 | 77.1 ||\n|| headline | 74.5 | [BOLD] 75.1 | 72.8 ||\n|| images | 84.7 | 85.6 | [BOLD] 85.8 ||\n|| OnWN | 84.9 | 81.4 | [BOLD] 85.1 ||\n|| tweet news | 76.3 | [BOLD] 78.7 | 78.7 ||\n|| STS 2014 Average | 75.8 | 75.8 | [BOLD] 75.9 ||\n|| answers-forums | 71.8 | 70.6 | [BOLD] 73.1 ||\n|| answers-students | 71.1 | [BOLD] 75.8 | 72.9 ||\n|| belief | 75.3 | 76.8 | [BOLD] 78.0 ||\n|| headline | 79.5 | [BOLD] 80.3 | 78.6 ||\n|| images | 85.8 | [BOLD] 86.0 | 85.8 ||\n|| STS 2015 Average | 76.7 | [BOLD] 77.9 | 77.7 ||\n|| 2014 SICK | 71.3 | 72.4 | [BOLD] 72.9 ||\n|| 2015 Twitter | [BOLD] 52.1 | [BOLD] 52.1 | 50.2 ||",
    "claim": "avg outperforms LSTMavg and GRAN in datasets focused on comparing glosses like OnWN and FNWN.",
    "label": "refute"
  },
  {
    "id": "training_1234_refute",
    "table_caption": "Table: Manual evaluation of 100-pair data samples drawn from five ranges of the automatic paraphrase score (first column). Second column shows total count of pairs in that range in ParaNMT-50M. Paraphrase strength and fluency were judged on a 1-3 scale and the table shows counts of each score designation.",
    "table_content": "|| Para. Score Range | # (M) | Tri. Overlap Mean (Std.) | Paraphrase 1 | Paraphrase 2 | Paraphrase 3 | Fluency 1 | Fluency 2 | Fluency 3 ||\n|| (-0.1, 0.2] | 4.0 | 0.00±0.0 | 92 | 6 | 2 | 1 | 5 | 94 ||\n|| (0.2, 0.4] | 3.8 | 0.02±0.1 | 53 | 32 | 15 | 1 | 12 | 87 ||\n|| (0.4, 0.6] | 6.9 | 0.07±0.1 | 22 | 45 | 33 | 2 | 9 | 89 ||\n|| (0.6, 0.8] | 14.4 | 0.17±0.2 | 1 | 43 | 56 | 11 | 0 | 89 ||\n|| (0.8, 1.0] | 18.0 | 0.35±0.2 | 1 | 13 | 86 | 3 | 0 | 97 ||",
    "claim": "In the highest paraphrase score range, 56% of the pairs possess a strong paraphrase relationship.",
    "label": "refute"
  },
  {
    "id": "training_1243_refute",
    "table_caption": "Table: Performances and training efficiency of GramCTC with different model strides",
    "table_content": "|| Loss Stride | WER 2 | WER 4 | Epoch Time (hours) 2 | Epoch Time (hours) 4 ||\n|| GramCTC | 21.46 | 18.27 | 18.3 | 9.6 ||",
    "claim": "From stride 2 to stride 4, the WER decreases from 21.46 to 18.27, but the epoch time increases from 18.3 hours to 9.6 hours.",
    "label": "refute"
  },
  {
    "id": "training_1264_refute",
    "table_caption": "Table: Number of relation mentions (RM), relation mentions annotated as None, relation mentions with conflicting annotations and conflicts involving None",
    "table_content": "|| [BOLD] Dataset | [BOLD] Wiki-KBP | [BOLD] NYT ||\n|| Total Number of RM | 225977 | 530767 ||\n|| RM annotated as None | 100521 | 356497 ||\n|| RM with conflicts | 32008 | 58198 ||\n|| Conflicts involving None | 30559 | 38756 ||",
    "claim": "A large portion of instances is annotated as None, and most conflicts do not involve the None type in both the Wiki-KBP and NYT datasets.",
    "label": "refute"
  },
  {
    "id": "training_1306_refute",
    "table_caption": "Table: Exact match scores of Sparc in different search strategies. SFS: Sparse First Search. DFS: Dense First Search. Hybrid: Combination of SFS + DFS. Exact match scores are reported.",
    "table_content": "|| [BOLD] Model | SQuAD-Open DenSPI | SQuAD-Open + Sparc | CuratedTREC DenSPI | CuratedTREC + Sparc ||\n|| SFS | 33.3 | 36.9 (+3.6) | 28.8 | 30.0 (+1.2) ||\n|| DFS | 28.5 | 34.4 (+5.9) | 29.5 | 34.3 (+4.8) ||\n|| Hybrid | 36.2 | 40.7 (+4.5) | 31.6 | 35.7 (+4.1) ||",
    "claim": "SFS outperforms DFS on the CuratedTREC dataset.",
    "label": "refute"
  },
  {
    "id": "training_1239_refute",
    "table_caption": "Table: Accuracy on CBT dataset. Results marked with ‡ are of previously published works [Dhingra et al.2017, Cui et al.2016, Yang et al.2017].",
    "table_content": "|| Model | CBT-NE Valid | CBT-NE Test | CBT-CN Valid | CBT-CN Test ||\n|| Human ‡ | - | 81.6 | - | 81.6 ||\n|| LSTMs ‡ | 51.2 | 41.8 | 62.6 | 56.0 ||\n|| MemNets ‡ | 70.4 | 66.6 | 64.2 | 63.0 ||\n|| AS Reader ‡ | 73.8 | 68.6 | 68.8 | 63.4 ||\n|| Iterative Attentive Reader ‡ | 75.2 | 68.2 | 72.1 | 69.2 ||\n|| EpiReader ‡ | 75.3 | 69.7 | 71.5 | 67.4 ||\n|| AoA Reader ‡ | 77.8 | 72.0 | 72.2 | 69.4 ||\n|| NSE ‡ | 78.2 | 73.2 | 74.3 | 71.9 ||\n|| FG Reader ‡ | [BOLD] 79.1 | [BOLD] 75.0 | [BOLD] 75.3 | [BOLD] 72.0 ||\n|| GA Reader ‡ | 76.8 | 72.5 | 73.1 | 69.6 ||\n|| SAW Reader | 78.5 | 74.9 | 75.0 | 71.6 ||",
    "claim": "SAW Reader outperforms FG Reader on the CBT-NE and CBT-CN test sets.",
    "label": "refute"
  },
  {
    "id": "training_1255_refute",
    "table_caption": "Table: Our results on Subtask A: Grouping Verbs to Frame Type Clusters. Purity F1-score is denoted as Pu F1, B-Cubed F1-score is denoted as B3 F1. denotes our final submission (# 536426), denotes our post-competition result, denotes a baseline, and denotes the submission of the winning team.",
    "table_content": "|| [BOLD] Method | [BOLD] Method w2v[c+w]norm | [BOLD] Pu F1  [BOLD] 76.68 | [BOLD] B3 F1  [BOLD] 68.10 ||\n|| [EMPTY] | ELMo[c+w]norm | 77.03 | 69.50 ||\n|| [EMPTY] | Cluster Per Verb | 73.78 | 65.35 ||\n|| [EMPTY] | Winner | 78.15 | 70.70 ||",
    "claim": "The \"ELMo[c+w]norm\" method achieves the highest Pu F1 and B3 F1 scores among the listed methods.",
    "label": "refute"
  },
  {
    "id": "training_1251_refute",
    "table_caption": "Table: Performance when adding a new agent for the L11 domain to a multi-domain dialogue manager using SCALE training with two partially trained agents for the SFR and SFH domains.",
    "table_content": "|| Performance in L11 domain Training data | Performance in L11 domain Reward | Performance in L11 domain Success | Performance in L11 domain Turns ||\n|| 250 SFR+ 250 SFH | −10.89±0.40 | 39.89±0.96 | 16.65±0.21 ||\n|| +250 L11 | 4.18±0.28 | 62.18±0.95 | 7.89±0.11 ||\n|| +250 SFR+250 SFH+250 L11 | 7.26±0.22 | 70.47±0.94 | 6.79±0.08 ||\n|| Performance in SFR domain | Performance in SFR domain | Performance in SFR domain | Performance in SFR domain ||\n|| 250 SFR+ 250 SFH | 6.12±0.22 | 70.22±0.91 | 7.90±0.08 ||\n|| +250 L11 | 6.75±0.21 | 73.54±0.86 | 7.93±0.08 ||\n|| +250 SFR+250 SFH+250 L11 | 8.05±0.20 | 79.38±0.83 | 7.79±0.08 ||",
    "claim": "Performance in both the L11 and SFR domains improves as more training data is added, except when the L11 domain is included in the training.",
    "label": "refute"
  },
  {
    "id": "training_1205_refute",
    "table_caption": "Table: P2P-Tran with different basic units. The numbers in bracket in first column are the number of basic units.",
    "table_content": "|| Basic Units | Direction | Dev | Test ||\n|| Zh: Pinyin (1485) En: phoneme (39) | Zh-En | 51.17 | 51.75 ||\n|| Zh: Pinyin (1485) En: phoneme (39) | En-Zh | 46.39 | 46.75 ||\n|| Zh: Pinyin (1485) En: syllable(10004) | Zh-En | 70.15 | 70.34 ||\n|| Zh: Pinyin (1485) En: syllable(10004) | En-Zh | 49.41 | 49.98 ||\n|| Zh: subword (16000) En: subword (10000) | Zh-En | 71.06 | \\mathbf{71.23} ||\n|| Zh: subword (16000) En: subword (10000) | En-Zh | 50.57 | \\mathbf{50.90} ||",
    "claim": "The subword approach achieves the best performance in both Zh-En and En-Zh directions, while the phoneme approach performs better, especially in the Zh-En direction.",
    "label": "refute"
  },
  {
    "id": "training_1297_refute",
    "table_caption": "Table: Accuracy in prepositional attachment disambiguation.",
    "table_content": "|| Classifier Embedding method | HPCD (enriching) GloVe | LRFR Word2vec | OntoLSTM Glove- extended | FNN Word2vec | FNN GloVe | FNN Our method (ALS) | FNN Our method (WD) ||\n|| Resources | POS tag, WordNet, VerbNet | POS tag, WordNet, VerbNet | POS tag, WordNet | POS tag | POS tag | POS tag | POS tag ||\n|| Accuracy | 0.887 | [BOLD] 0.903 | 0.897 | 0.866 | 0.858 | 0.883 | 0.892 ||",
    "claim": "The OntoLSTM Glove-extended method achieves the highest accuracy of 0.903 among the compared methods.",
    "label": "refute"
  },
  {
    "id": "training_1263_refute",
    "table_caption": "Table: Number of labeling functions and the relation types they can annotated w.r.t. two kinds of information",
    "table_content": "|| Kind | Wiki-KBP #Types | Wiki-KBP #LF | NYT #Types | NYT #LF ||\n|| Pattern | 13 | 147 | 16 | 115 ||\n|| KB | 7 | 7 | 25 | 26 ||",
    "claim": "Knowledge bases identify more relation types than heuristic patterns for the Wiki-KBP dataset, while knowledge bases identify more relation types than heuristic patterns for the NYT dataset.",
    "label": "refute"
  },
  {
    "id": "training_1198_refute",
    "table_caption": "Table: Personalization task 1 (Issuing API calls). The model solves the task successfully by directing its attention towards the memories containing the values of the fields of the API call. The signal grows stronger on the second hop and switches almost exclusively to the price range in the third. This indicates that the model does not need multiple hops for successfully completing PT1. (Zero attention weight signifies a very small value.)",
    "table_content": "|| [BOLD] Time | [BOLD] Locutor | [BOLD] Dialog History | [BOLD] Hop #1 | [BOLD] Hop #2 | [BOLD] Hop #3 ||\n|| 1 | [EMPTY] | female elderly | 0.006 | 0 | 0 ||\n|| 2 | User | hi | 0.012 | 0 | 0 ||\n|| 3 | Bot | good day madam how could i assist you today | 0 | 0 | 0 ||\n|| 4 | User | may i have a table | 0.006 | 0.248 | 0 ||\n|| 5 | Bot | thank you madam i shall start the reservation now | 0 | 0 | 0 ||\n|| 6 | User |  | 0.064 | 0 | 0 ||\n|| 7 | Bot | could you tell me your preference on the type of cuisine | 0.001 | 0 | 0 ||\n|| 8 | User | i love french food | 0.307 | 0.103 | 0 ||\n|| 9 | Bot | could you tell me where the restaurant should be located | 0 | 0 | 0 ||\n|| 10 | User | madrid please | 0.265 | 0.282 | 0 ||\n|| 11 | Bot | would you mind telling me how many guests shall be at your table | 0 | 0 | 0 ||\n|| 12 | User | for four please | 0.225 | 0.076 | 0 ||\n|| 13 | Bot | would you mind telling me your price range | 0 | 0 | 0 ||\n|| 14 | User | in a cheap price range please | 0.11 | 0.29 | 0.999 ||\n|| 15 | Bot | thank you madam i shall provide you with options shortly | 0 | 0 | 0 ||\n|| [BOLD] User input | [BOLD] User input |  |  |  |  ||\n|| [BOLD] Correct answer | [BOLD] Correct answer | api_call french madrid four cheap | api_call french madrid four cheap | api_call french madrid four cheap | api_call french madrid four cheap ||\n|| [BOLD] Predicted answer | [BOLD] Predicted answer | api_call french madrid four cheap | api_call french madrid four cheap | api_call french madrid four cheap | api_call french madrid four cheap ||",
    "claim": "The model struggled with accuracy in issuing and updating API calls, as the predicted answers do not match the correct answers in several cases.",
    "label": "refute"
  },
  {
    "id": "training_1200_refute",
    "table_caption": "Table: Overall comparison of model precisions (in %). Confidence intervals (±95%) are shown next to the average performance.",
    "table_content": "|| Approach | Model | Predicting assistant responses R@1 | Predicting assistant responses R@2 | Predicting assistant responses R@5 | Predicting customer responses R@1 | Predicting customer responses R@2 | Predicting customer responses R@5 ||\n|| Beam Search | [ITALIC] HREDL | [BOLD] 31.2± [BOLD] 0.9 | [BOLD] 45.8± [BOLD] 0.8 | [BOLD] 73.4± [BOLD] 0.9 | 28.5±0.7 | [BOLD] 39.8± [BOLD] 0.9 | [BOLD] 66.1± [BOLD] 0.8 ||\n|| [EMPTY] | [ITALIC] HREDG | 29.9±0.7 | 44.3±0.9 | 71.1±0.6 | 27.1±0.7 | 37.7±0.9 | 63.1±0.9 ||\n|| [EMPTY] | [ITALIC] HREDLG | 30.3±1.0 | 44.2±0.9 | 70.6±0.7 | [BOLD] 28.9± [BOLD] 1.1 | [BOLD] 39.8± [BOLD] 1.1 | 64.6±1.2 ||\n|| Context Relevance | [ITALIC] HREDL- [ITALIC] CR | 33.4±0.7 | 48.0±0.8 | 75.4±0.6 | 32.8±1.0 | 47.5±1.0 | 73.5±0.9 ||\n|| [EMPTY] | [ITALIC] HREDG- [ITALIC] CR | 34.6±1.1 | 50.0±1.0 | 76.2±0.7 | 32.9±0.8 | 47.3±0.7 | 74.2±0.8 ||\n|| [EMPTY] | [ITALIC] HREDLG- [ITALIC] CR | 33.9±0.9 | 48.8±0.8 | 74.8±0.8 | 32.5±0.9 | 48.1±0.7 | 74.8±0.6 ||\n|| Answer Relevance | [ITALIC] HREDL- [ITALIC] AR | 39.6±0.7 | 55.7±0.9 | 81.1±0.7 | 40.0±1.1 | 55.8±1.0 | 80.4±0.8 ||\n|| [EMPTY] | [ITALIC] HREDG- [ITALIC] AR | 42.7±0.9 | 58.5±0.8 | 82.5±0.7 | [BOLD] 41.0± [BOLD] 0.9 | [BOLD] 56.9± [BOLD] 0.7 | [BOLD] 81.5± [BOLD] 0.6 ||\n|| [EMPTY] | [ITALIC] HREDLG- [ITALIC] AR | 43.0±0.8 | 59.4±0.8 | [BOLD] 82.7±0.8 | 40.1±0.9 | 55.7±0.9 | 80.0±0.9 ||\n|| Context and Answer Relevance | [ITALIC] HREDL- [ITALIC] CAR | 41.3±0.8 | 57.2±0.8 | 81.2±0.5 | 39.9±1.0 | 55.3±1.0 | 79.6±0.6 ||\n|| [EMPTY] | [ITALIC] HREDG- [ITALIC] CAR | [BOLD] 44.0± [BOLD] 0.7 | [BOLD] 59.8± [BOLD] 0.9 | 82.6±0.7 | 40.9±0.7 | 56.8±0.7 | 80.6±0.7 ||\n|| [EMPTY] | [ITALIC] HREDLG- [ITALIC] CAR | 43.8±0.7 | 59.5±0.8 | 82.6±0.7 | 39.4±0.7 | 55.1±0.9 | 79.6±0.6 ||",
    "claim": "HREDL-CAR achieves the highest score in predicting assistant responses R@1 and R@2.",
    "label": "refute"
  },
  {
    "id": "training_1244_refute",
    "table_caption": "Table: Comparison of CTC and GramCTC.",
    "table_content": "|| Loss | Train CER | Train WER | Train Holdout CER | Train Holdout WER | Dev CER | Dev WER ||\n|| CTC | 4.38 | 12.41 | 4.60 | 12.89 | 11.64 | 28.68 ||\n|| GramCTC | 4.33 | 10.42 | 4.66 | 11.37 | 12.03 | 27.1 ||",
    "claim": "The WERs of GramCTC are better than those of CTC across the Train and Train Holdout datasets, but CTC has better WER in the Dev dataset.",
    "label": "refute"
  },
  {
    "id": "training_1293_refute",
    "table_caption": "Table: Development F1 scores on section 22 of the PTB when using various models to produce candidates and to score them. ∪ denotes taking the union of candidates from each of two models; + denotes using a weighted average of the models’ log-probabilities.",
    "table_content": "|| Candidates | Scoring models RD | Scoring models LM | Scoring models RD + LM ||\n|| RD | 92.22 | 93.66 | 93.99 ||\n|| LM | 92.57 | 92.20 | 93.07 ||\n|| RD ∪ LM | 92.24 | 93.47 | 94.15 ||",
    "claim": "Using either model alone results in higher performance than combining the scores of both models (RD + LM).",
    "label": "refute"
  },
  {
    "id": "training_1248_refute",
    "table_caption": "Table: Results on RACE and COIN dev set with cosine and bilinear score in PSS. We use BERTbase as encoder here.",
    "table_content": "|| Top K | 1 | 2 | 3 | 4 | 5 | 6 ||\n|| RACE-cos | 58.4 | 60.1 | 63.3 | 65.8 | [BOLD] 66.5 | 66 ||\n|| RACE-bi | 59.5 | 60.5 | 63.4 | [BOLD] 66.8 | 66.4 | 66.2 ||\n|| COIN-cos | 81.0 | 82.0 | [BOLD] 83.5 | 83.0 | 82.5 | 82.4 ||\n|| COIN-bi | 81.7 | 82.0 | 82.6 | [BOLD] 82.8 | 82.4 | 82.2 ||",
    "claim": "Cosine score method works better on the COIN dataset (83.5% vs. 82.8%), and cosine score works better on the RACE dataset (66.5% vs. 66.8%).",
    "label": "refute"
  },
  {
    "id": "training_1285_refute",
    "table_caption": "Table: The results of two weighting (WT) strategies based on the soft regularization.",
    "table_content": "|| Methods | EN→DE BLEU | EN→DE METEOR | EN→FR BLEU | EN→FR METEOR ||\n|| OnlyText | 27.1 | 49.3 | 51.3 | 69.1 ||\n|| Frozen-WT | 28.8 | 50.7 | 52.1 | 69.4 ||\n|| Adaptive-WT | [BOLD] 29.2 | [BOLD] 50.9 | [BOLD] 52.4 | [BOLD] 69.5 ||",
    "claim": "Frozen-WT achieves the highest BLEU and METEOR scores for both EN→DE and EN→FR translation tasks.",
    "label": "refute"
  },
  {
    "id": "training_1250_refute",
    "table_caption": "Table: Selection of committee members for multi-policy Bayesian committee machine for SFR domain. The committee policy is trained on 7500 dialogues equally spread across three domains.",
    "table_content": "|| MBCM – SFR Committee | MBCM – SFR Reward | MBCM – SFR Success | MBCM – SFR #Turns ||\n|| members | [EMPTY] | [EMPTY] | [EMPTY] ||\n|| SFR | 7.32±0.22 | 79.97±0.82 | 8.51±0.10 ||\n|| SFR+SFH | 9.20±0.18 | 86.51±0.70 | 8.05±0.09 ||\n|| SFR+L11 | 8.73±0.19 | 84.56±0.73 | 8.12±0.09 ||\n|| SFR+SFH+L11 | 9.67±0.17 | 88.28±0.66 | 7.96±0.08 ||",
    "claim": "For good performance on the SFR domain, the L11 committee member is more useful than the SFH committee member.",
    "label": "refute"
  },
  {
    "id": "training_1148_refute",
    "table_caption": "Table: Accuracies for POS tagging, using a variety of normalization strategies.",
    "table_content": "|| [EMPTY] | LAI | Multilingual | Language-aware | Gold ||\n|| MarMoT–POS | 60.47 | 63.96 | 63.93 | 67.52 ||\n|| Bilty–POS | 63.77 | 66.41 | 66.68 | 70.37 ||\n|| MaChAmp–POS | 64.18 | 66.71 | 66.57 | 69.63 ||",
    "claim": "MarMoT outperforms in the Language-aware setting.",
    "label": "refute"
  },
  {
    "id": "training_1246_refute",
    "table_caption": "Table: Results on the test set of SemEval Task 11, ROCStories, MCTest and the development set of COIN Task 1. The test set of COIN is not public. DCMN_BERT: BERT + DCMN + PSS + AOI. Previous SOTA: previous state-of-the-art model. All the results are from single models.",
    "table_content": "|| Task | Previous STOA | [EMPTY] | BERT | DCMN_BERT | XLNet | DCMN_XLNet ||\n|| SemEval Task 11 |  | 89.5 | 90.5 | 91.8 (+1.3) | 92.0 | 93.4 (+1.4) ||\n|| ROCStories |  | 91.8 | 90.8 | 92.4 (+1.6) | 93.8 | 95.8 (+2.0) ||\n|| MCTest-MC160 |  | 81.7 | 73.8 | 85.0 (+11.2) | 80.6 | 86.2 (+5.6) ||\n|| MCTest-MC500 |  | 82.0 | 80.4 | 86.5 (+6.1) | 83.4 | 86.6 (+3.2) ||\n|| COIN Task 1 |  | 84.2 | 84.3 | 88.8 (+4.5) | 89.1 | 91.1 (+2.0) ||\n|| [BOLD] Average | [EMPTY] | 85.8 | 84.0 | [BOLD] 88.9 (+4.9) | 87.8 | [BOLD] 90.6 (+2.8) ||",
    "claim": "The proposed model achieves a 4.9% improvement in average accuracy over directly fine-tuned BERT (88.9% vs. 84.0%) and a 3.5% improvement over directly fine-tuned XLNet (90.6% vs. 87.8%).",
    "label": "refute"
  },
  {
    "id": "training_1277_refute",
    "table_caption": "Table: Average inference speed of BERT on the MNLI-matched validation set in examples per second (± standard deviation). The speedup relative to the original model is indicated in parentheses.",
    "table_content": "|| Batch size | 1 | 4 | 16 | 64 ||\n|| Original | 17.0±0.3 | 67.3±1.3 | 114.0±3.6 | 124.7±2.9 ||\n|| Pruned (50%) | 17.3±0.6 | 69.1±1.3 | 134.0±3.6 | 146.6±3.4 ||\n|| [EMPTY] | (+1.9%) | (+2.7%) | (+17.5%) | (+17.5%) ||",
    "claim": "Pruning half of the model’s heads speeds up inference by up to 2.7% for higher batch sizes.",
    "label": "refute"
  },
  {
    "id": "training_1245_refute",
    "table_caption": "Table: A deeper look into the dev set, measuring WER with language model decoding of each model on different slices of the dev set. Serving latency (milliseconds) is the 98th percentile latency on the last packet as described in Sec. 4. Training time is in hours per epoch with the data and infrastructure the same. *This model has twice the number of parameters as the Baseline, and suffers from prohibitively large serving latency.",
    "table_content": "|| Devsets | Baseline | 2×Baseline* | Mix-3 ||\n|| Clean casual speech | 5.90 | [BOLD] 5.00 | 5.80 ||\n|| Farfield | 35.05 | 30.60 | [BOLD] 26.49 ||\n|| Names | 19.73 | 19.30 | [BOLD] 17.40 ||\n|| Overall | 18.46 | 17.46 | [BOLD] 15.74 ||\n|| Serving latency | [BOLD] 112 | 25933 | 153 ||\n|| Training time | [BOLD] 17 | 29 | 25 ||",
    "claim": "Mix-3 achieves the lowest error rates for \"Farfield,\" \"Names,\" and \"Overall\" devsets, and has a serving latency of 153, which is lower than the baseline.",
    "label": "refute"
  },
  {
    "id": "training_1037_refute",
    "table_caption": "Table: Span-level precision (P), recall (R) and F1-scores (F1) on four distinct baseline NER systems. All scores are computed as average over five-fold cross validation.",
    "table_content": "|| [EMPTY] | CRF P | CRF R | CRF F1 | BiLSTM CRF P | BiLSTM CRF R | BiLSTM CRF F1 | MTL P | MTL R | MTL F1 | BioBERT P | BioBERT R | BioBERT F1 ||\n|| case | 0.59 | 0.76 | [BOLD] 0.66 | 0.40 | 0.22 | 0.28 | 0.55 | 0.38 | 0.44 | 0.43 | 0.64 | 0.51 ||\n|| condition | 0.45 | 0.18 | 0.26 | 0.00 | 0.00 | 0.00 | 0.62 | 0.62 | [BOLD] 0.62 | 0.33 | 0.37 | 0.34 ||\n|| factor | 0.40 | 0.05 | 0.09 | 0.23 | 0.04 | 0.06 | 0.6 | 0.53 | [BOLD] 0.56 | 0.17 | 0.10 | 0.12 ||\n|| finding | 0.50 | 0.33 | 0.40 | 0.39 | 0.26 | 0.31 | 0.62 | 0.61 | [BOLD] 0.61 | 0.41 | 0.53 | 0.46 ||\n|| modifier | 0.74 | 0.32 | 0.45 | 0.60 | 0.42 | 0.47 | 0.66 | 0.63 | [BOLD] 0.65 | 0.51 | 0.52 | 0.50 ||\n|| micro avg. | 0.52 | 0.31 | 0.39 | 0.41 | 0.23 | 0.30 | 0.52 | 0.44 | [BOLD] 0.47 | 0.39 | 0.49 | 0.43 ||\n|| macro avg. | 0.51 | 0.31 | 0.38 | 0.37 | 0.23 | 0.28 | 0.61 | 0.58 | [BOLD] 0.59 | 0.40 | 0.49 | 0.44 ||",
    "claim": "MTL achieves the highest F1 scores across all categories except for \"case,\" and both micro and macro averages.",
    "label": "refute"
  },
  {
    "id": "training_1299_refute",
    "table_caption": "Table: Comparative Results",
    "table_content": "|| [EMPTY] | Accuracy | Precision | Recall | F1 ||\n|| NBSVM | 0.8223 | 0.8180 | 0.7007 | 0.7548 ||\n|| First Seed | 0.8461 | 0.7869 | 0.8309 | 0.8083 ||\n|| Best Seed | 0.8458 | 0.7806 | 0.8416 | [BOLD] 0.8099 ||",
    "claim": "The Best Seed model has the highest precision of 0.8099 among the models listed.",
    "label": "refute"
  },
  {
    "id": "training_1155_refute",
    "table_caption": "Table: Comparisons with baseline models on the Restaurant dataset and Laptop dataset. The results of baseline models are retrieved from published papers. The best results in GloVe-based models and BERT-based models are all in bold separately. -A means that the model is based on adjacent-relation graph, and -G means the model is based on global-relation graph.",
    "table_content": "|| Word Embedding | Models | Restaurant Acc | Restaurant Macro-F1 | Laptop Acc | Laptop Macro-F1 ||\n|| GloVe | TD-LSTM | 75.63 | - | 68.13 | - ||\n|| GloVe | ATAE-LSTM | 77.20 | - | 68.70 | - ||\n|| GloVe | MenNet | 78.16 | 65.83 | 70.33 | 64.09 ||\n|| GloVe | IAN | 78.60 | - | 72.10 | - ||\n|| GloVe | RAN | 80.23 | 70.80 | 74.49 | [BOLD] 71.35 ||\n|| GloVe | PBAN | 81.16 | - | 74.12 | - ||\n|| GloVe | TSN | 80.1 | - | 73.1 | - ||\n|| GloVe | AEN | 80.98 | 72.14 | 73.51 | 69.04 ||\n|| GloVe | SDGCN-A w/o p | 81.61 | 72.22 | 73.20 | 68.54 ||\n|| GloVe | SDGCN-G w/o p | 81.61 | 72.93 | 73.67 | 68.70 ||\n|| GloVe | SDGCN-A | 82.14 | 73.47 | 75.39 | 70.04 ||\n|| GloVe | SDGCN-G | [BOLD] 82.95 | [BOLD] 75.79 | [BOLD] 75.55 | [BOLD] 71.35 ||\n|| BERT | AEN-BERT | 83.12 | 73.76 | 79.93 | 76.31 ||\n|| BERT | SDGCN-BERT | [BOLD] 83.57 | [BOLD] 76.47 | [BOLD] 81.35 | [BOLD] 78.34 ||",
    "claim": "SDGCN-A achieves the highest performance among GloVe-based models, while SDGCN-BERT achieves the highest performance among BERT-based models.",
    "label": "refute"
  },
  {
    "id": "training_1247_refute",
    "table_caption": "Table: Experiment results on RACE test set. All the results are from single models. PSS: Passage Sentence Selection; AOI: Answer Option Interaction. ∗ indicates our implementation.",
    "table_content": "|| [BOLD] Model | RACE-M/H | RACE ||\n|| HAF  | 45.0/46.4 | 46.0 ||\n|| MRU  | 57.7/47.4 | 50.4 ||\n|| HCM  | 55.8/48.2 | 50.4 ||\n|| MMN  | 61.1/52.2 | 54.7 ||\n|| GPT  | 62.9/57.4 | 59.0 ||\n|| RSM  | 69.2/61.5 | 63.8 ||\n|| OCN  | 76.7/69.6 | 71.7 ||\n|| XLNet  | 85.5/80.2 | 81.8 ||\n|| BERTbase∗ | 71.1/62.3 | 65.0 ||\n|| BERTlarge∗ | 76.6/70.1 | 72.0 ||\n|| XLNetlarge∗ | 83.7/78.6 | 80.1 ||\n|| Our Models | [EMPTY] | [EMPTY] ||\n|| BERTbase∗ + DCMN | 73.2/64.2 | 67.0 ||\n|| BERTlarge∗ + DCMN | 79.2/72.1 | 74.1 ||\n|| BERTlarge∗ + DCMN + PSS + AOI | 79.3/74.4 | [BOLD] 75.8 ||\n|| XLNetlarge∗ + DCMN + PSS + AOI | 86.5/81.3 | [BOLD] 82.8 ||\n|| Human Performance | [EMPTY] | [EMPTY] ||\n|| Turkers | 85.1/69.4 | 73.3 ||\n|| Ceiling | 95.4/94.2 | 94.5 ||",
    "claim": "Directly fine-tuned BERTbase, BERTlarge, and XLNetlarge achieve accuracies of 66.0%, 73.0%, and 81.0% on the RACE dataset, respectively.",
    "label": "refute"
  },
  {
    "id": "training_1274_refute",
    "table_caption": "Table: Comparison of runtime for difference inference procedures in the noise-free constraint setting: Viterbi, A*[He et al.2017] and GBI. For SRL-100 refer Table 1 and SRL-NW is a model trained on NW genre.",
    "table_content": "|| Network | Genre(s) | No. of examples | Failure rate (%) | Inference time (approx. mins) Viterbi | Inference time (approx. mins) GBI | Inference time (approx. mins) A* ||\n|| SRL-100 | All | 25.6k | 9.82 | 109 | 288 | 377 ||\n|| SRL-NW | BC | 4.9k | 26.86 | 23 | 110 | 117 ||\n|| SRL-NW | BN | 3.9K | 18.51 | 18 | 64 | 100 ||\n|| SRL-NW | PT | 2.8k | 10.01 | 8 | 19 | 15 ||\n|| SRL-NW | TC | 2.2k | 19.01 | 5 | 23 | 20 ||\n|| SRL-NW | WB | 2.3k | 20.32 | 12 | 49 | 69 ||",
    "claim": "A* tends to be faster than GBI across most datasets, with a slight exception in the BC domain.",
    "label": "refute"
  },
  {
    "id": "training_1257_refute",
    "table_caption": "Table: Ablation study on the English development set.",
    "table_content": "|| System | P | R | F1 ||\n|| Ours (local) | 87.7 | 85.5 | 86.6 ||\n|| w/o POS tags | 87.3 | 84.5 | 85.9 ||\n|| w/o predicate-specific encoding | 80.9 | 79.8 | 80.4 ||\n|| with basic classifier | 86.7 | 84.5 | 85.6 ||",
    "claim": "The model with a basic classifier achieves the highest F1 score compared to the versions with all features included (Ours), without POS tags, and without predicate-specific encoding.",
    "label": "refute"
  },
  {
    "id": "training_1292_refute",
    "table_caption": "Table: Development F1 scores on section 22 of the PTB when using various models to produce candidates and to score them. ∪ denotes taking the union of candidates from each of two models; + denotes using a weighted average of the models’ log-probabilities.",
    "table_content": "|| Candidates | Scoring models RD | Scoring models RG | Scoring models RD + RG ||\n|| RD | 92.22 | 93.45 | 93.87 ||\n|| RG | 90.24 | 89.55 | 90.53 ||\n|| RD ∪ RG | 92.22 | 92.78 | 93.92 ||",
    "claim": "Combining the scores of both models (RD + RG) does not improve performance compared to using the RD model alone.",
    "label": "refute"
  },
  {
    "id": "training_1231_refute",
    "table_caption": "Table: System performance on different answer types. “PN”= Proper Noun",
    "table_content": "|| Answer Type | Total | Correct | Acc. (%) ||\n|| Person | 50 | 28 | 56.0 ||\n|| Location | 31 | 14 | 45.2 ||\n|| Date | 26 | 13 | 50.0 ||\n|| Number | 14 | 4 | 28.6 ||\n|| Artwork | 19 | 7 | 36.8 ||\n|| Yes/No | 17 | 12 | [BOLD] 70.6 ||\n|| Event | 5 | 2 | 40.0 ||\n|| Common noun | 11 | 3 | 27.3 ||\n|| Group/Org | 17 | 6 | 35.3 ||\n|| Other PN | 20 | 9 | 45.0 ||\n|| Total | 200 | 98 | 49.0 ||",
    "claim": "The model performs the best on the Person answer type with an accuracy of 56.0%.",
    "label": "refute"
  },
  {
    "id": "training_1296_refute",
    "table_caption": "Table: Performance on preposition selection.",
    "table_content": "|| Dataset | Method | Precision | Recall | F1 score ||\n|| CoNLL | State-of-the-art | 0.2592 | 0.3611 | 0.3017 ||\n|| CoNLL | Word2vec | 0.1558 | 0.1579 | 0.1569 ||\n|| CoNLL | GloVe | 0.1538 | 0.1578 | 0.1558 ||\n|| CoNLL | Our method (ALS) | 0.3355 | 0.3355 | 0.3355 ||\n|| CoNLL | Our method (WD) | 0.3590 | 0.3684 | [BOLD] 0.3636 ||\n|| SE | State-of-the-art | 0.2704 | 0.2961 | 0.2824 ||\n|| SE | Word2vec | 0.2450 | 0.2585 | 0.2516 ||\n|| SE | GloVe | 0.2454 | 0.2589 | 0.2520 ||\n|| SE | Our method (ALS) | 0.2958 | 0.3146 | [BOLD] 0.3049 ||\n|| SE | Our method (WD) | 0.2899 | 0.3055 | 0.2975 ||",
    "claim": "The tensor with ALS decomposition achieves the highest F1 score on the CoNLL dataset, while the tensor with weighted decomposition achieves the highest F1 score on the SE dataset.",
    "label": "refute"
  },
  {
    "id": "training_1272_refute",
    "table_caption": "Table: Testing set OOV rate, together with the accuracy achieved on OOV set by our baseline model and our main model, respectively.",
    "table_content": "|| [EMPTY] | AS | CITYU | MSR | PKU | UD ||\n|| OOV % | 4.2 | 7.3 | 2.6 | 3.5 | 11.9 ||\n|| Accuracy % (baseline model) | 90.3 | 92.7 | 93.4 | 89.2 | 91.0 ||\n|| Accuracy % (char-level ELMo) | [BOLD] 91.3 | [BOLD] 95.6 | [BOLD] 93.7 | [BOLD] 93.2 | [BOLD] 95.5 ||",
    "claim": "The char-level ELMo model outperforms the baseline model in accuracy across most datasets, but not in the MSR dataset.",
    "label": "refute"
  },
  {
    "id": "training_1305_refute",
    "table_caption": "Table: Results on the SQuAD development set. LSTM+SA+ELMo is a query-agnostic baseline from Seo et al. (2018).",
    "table_content": "|| [EMPTY] | [BOLD] Model | EM | F1 ||\n|| Original | DrQA (Chen et al.,  2017 ) | 69.5 | 78.8 ||\n|| Original | BERT (Devlin et al.,  2019 ) | 84.1 | 90.9 ||\n|| Query-Agnostic | LSTM + SA + ELMo | 52.7 | 62.7 ||\n|| Query-Agnostic | DenSPI | 73.6 | 81.7 ||\n|| Query-Agnostic | DenSPI + Sparc | 76.4 | 84.8 ||",
    "claim": "The F1 score difference between BERT and DenSPI + Sparc in a query-agnostic setting is 9.2.",
    "label": "refute"
  },
  {
    "id": "training_1238_refute",
    "table_caption": "Table: Results for Human Queries (GenX) Our model outperforms LSTM and semantic parsing models on complex human-generated queries, showing it is robust to work on natural language. Better performance than Tree-LSTM (Unsup.) shows the efficacy in representing sub-phrases using explicit denotations. Our model also performs better without an external parser, showing the advantages of latent syntax.",
    "table_content": "|| [BOLD] Model | [BOLD] Accuracy ||\n|| LSTM (No KG) | 0.0 ||\n|| LSTM | 64.9 ||\n|| Bi-LSTM | 64.6 ||\n|| Tree-LSTM | 43.5 ||\n|| Tree-LSTM (Unsup.) | 67.7 ||\n|| Sempre | 48.1 ||\n|| Our Model (Pre-parsed) | 67.1 ||\n|| Our Model | 73.7 ||",
    "claim": "Tree-LSTM (Unsup.) achieves the highest accuracy compared to other models listed in the table.",
    "label": "refute"
  }
]